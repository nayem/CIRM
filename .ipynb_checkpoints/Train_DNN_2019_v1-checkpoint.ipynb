{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DNN(working)\n",
    "\n",
    "** - Generalize DNN structure  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import pixiedust** is for debugging jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras Version: 2.2.4 \n",
      "Tensorflow Version: 1.12.0 \n",
      "Python Version: 3.6.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
    "\n",
    "import keras\n",
    "print(\"keras Version:\", keras.__version__ , \"\\nTensorflow Version:\", tf.VERSION, \"\\nPython Version:\", platform.python_version()) \n",
    "from keras.utils.data_utils import Sequence\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import pixiedust #jupyter notebook debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To control GPU usage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read parameters from .mat files\n",
    "some read-only version names for ease use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'_e12v1' -> 2 lstm layers, batch size 25, 256 nodes\n",
    "'_e12v2' -> 2 lstm layers with a dropout layer, batch size 25, 256 nodes\n",
    "\n",
    "'_e12v3' -> 2 BLSTM layers, batch size 25, 256 nodes\n",
    "'_e12v4' -> 2 lstm layers with a dropout layer, batch size 25, 256 nodes\n",
    "\n",
    "'_e13v1' -> 2 Blstm layers, batch size 25, 256 nodes, signal approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Constants and File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code_VERSION** is the version of this notebook. This is used for naming weights files\n",
    "\n",
    "***Note: This will be different in each notebook.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code_VERSION = \"_e19v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary constants/file names that are used through out the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_FILE = \"./dnn_models/DNN_params.mat\"\n",
    "\n",
    "TrainData_FILE = \"./dnn_models/Train_datas.mat\"\n",
    "DevData_FILE = \"./dnn_models/CrossValidation_datas.mat\"\n",
    "TestData_FILE = \"./dnn_models/Test_datas.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following filenames are used to save weights, \n",
    "\n",
    "**MODEL_FILE: ** to save best model/weights after an epoch cycle (if weights need to be updated)\n",
    "\n",
    "**SAVE_MODEL_FILE: ** to save final/best model/weights after a training.\n",
    "\n",
    "**LOG_FILE: ** to print log\n",
    "\n",
    "\n",
    "***Note: This will be different in each notebook.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model, after a training cycle\n",
    "MODEL_FILE = \"./dnn_models/dnn_weights\"+ Code_VERSION+\"_{epoch:02d}.h5\"\n",
    "\n",
    "# Final saved file\n",
    "SAVE_MODEL_FILE = \"./dnn_models/dnn_model_final\"+ Code_VERSION+\".h5\"\n",
    "\n",
    "LOG_FILE = \"./dnn_models/dnn_log\"+Code_VERSION+\".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following filenames are used as final output, \n",
    "\n",
    "**OUTPUT_FILE: ** Output of test. After testing on all test files, we save the output mask for each testing file.\n",
    "\n",
    "***Note: This will be different in each notebook.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated real+imag for test dataset\n",
    "OUTPUT_FILE = \"./dnn_models/dnn_Real_Imag\"+Code_VERSION+\".mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Class defination to read Matlab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "#     opts_dict = dict()\n",
    "\n",
    "    def __init__(self, FILE_PARA, FILE_DATA=\"\", FILE_TEST=\"\"):\n",
    "        \n",
    "        # Basic parameters\n",
    "        with h5py.File(FILE_PARA, 'r') as f:\n",
    "            key_list = list(f.keys())\n",
    "            print('Opt keys:')\n",
    "\n",
    "            for e,(k, v) in enumerate(f['opts'].items()):\n",
    "\n",
    "                print(\"{0}->{1},\".format(e,k), end=\"\")\n",
    "\n",
    "                if k == 'ARMA_order':\n",
    "                    self.ARMA_order = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.ARMA_order\n",
    "                elif k == 'ada_grad_eps':\n",
    "                    self.ada_grad_eps = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.ada_grad_eps\n",
    "                elif k == 'ada_sgd_scale':\n",
    "                    self.ada_sgd_scale = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.ada_sgd_scale\n",
    "                elif k == 'amra_order':\n",
    "                    self.amra_order = int(np.array(v)[0][0])\n",
    "                elif k == 'change_momentum_point':\n",
    "                    self.change_momentum_point = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.change_momentum_point\n",
    "                elif k == 'clip_level':\n",
    "                    self.clip_level = int(np.array(v)[0][0])\n",
    "                elif k == 'cost_function':\n",
    "                    self.cost_function = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.cost_function += chr(c[0])\n",
    "\n",
    "#                     self.opts_dict[k] = self.cost_function\n",
    "\n",
    "                elif k == 'cv_interval':\n",
    "                    self.cv_interval = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.cv_interval\n",
    "                elif k == 'dim_input':\n",
    "                    self.dim_input = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.dim_input\n",
    "                    print(\"(\",k,\"=\",self.dim_input,\")\",end=\" \")\n",
    "                elif k == 'dim_output':\n",
    "                    self.dim_output = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.dim_output\n",
    "                    print(\"(\",k,\"=\",self.dim_output,\")\",end=\" \")\n",
    "                elif k == 'drop_ratio':\n",
    "                    self.drop_ratio = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.drop_ratio\n",
    "                elif k == 'eval_on_gpu':\n",
    "                    self.eval_on_gpu = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.\n",
    "                elif k == 'feawin':\n",
    "                    self.feawin = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.feawin\n",
    "                elif k == 'final_momentum':\n",
    "                    self.final_momentum = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.final_momentum\n",
    "                elif k == 'Fs':\n",
    "                    self.Fs = int(np.array(v)[0][0])\n",
    "                elif k == 'fRange':\n",
    "                    self.fRange = int(np.array(v)[0][0])\n",
    "                elif k == 'hid_struct':\n",
    "                    self.hid_struct = np.array(v)\n",
    "#                     self.opts_dict[k] = self.hid_struct\n",
    "                elif k == 'hopsize':\n",
    "                    self.hopsize = int(np.array(v)[0][0])\n",
    "                elif k == 'initial_momentum':\n",
    "                    self.initial_momentum = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.initial_momentum\n",
    "                elif k == 'isDropout':\n",
    "                    self.isDropout = 0\n",
    "#                     self.opts_dict[k] = self.isDropout\n",
    "                elif k == 'isDropoutInput':\n",
    "                    self.isDropoutInput = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isDropoutInput\n",
    "                elif k == 'isGPU':\n",
    "                    self.isGPU = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isGPU\n",
    "                elif k == 'isNormalize':\n",
    "                    self.isNormalize = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isNormalize\n",
    "                elif k == 'isPretrain':\n",
    "                    self.isPretrain = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isPretrain\n",
    "                elif k == 'labwin':\n",
    "                    self.labwin = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.labwin\n",
    "                elif k == 'labeltype':\n",
    "                    self.labeltype = int(np.array(v)[0][0])\n",
    "                elif k == 'labcompress':\n",
    "                    self.labcompress = int(np.array(v)[0][0])\n",
    "                elif k == 'learner':\n",
    "                    self.learner = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.learner += chr(c[0])\n",
    "#                     self.opts_dict[k] = self.learner\n",
    "                elif k == 'logistic_max':\n",
    "                    self.logistic_max = int(np.array(v)[0][0])\n",
    "                elif k == 'logistic_steep':\n",
    "                    self.logistic_max = int(np.array(v)[0][0])\n",
    "                elif k == 'net_struct':\n",
    "                    self.net_struct = np.array(v)\n",
    "#                     for n_s in np.array(v):\n",
    "#                         print('Opts Net Stuct:',n_s[0])\n",
    "#                     self.opts_dict[k] = self.net_struct\n",
    "                elif k == 'noise':\n",
    "                    self.noise = int(np.array(v)[0][0])\n",
    "                elif k == 'nfft':\n",
    "                    self.nfft = int(np.array(v)[0][0])\n",
    "                elif k == 'numGammatoneChans':\n",
    "                    self.numGammatoneChans = int(np.array(v)[0][0])\n",
    "                elif k == 'overlap':\n",
    "                    self.overlap = int(np.array(v)[0][0])\n",
    "                elif k == 'overlap_len':\n",
    "                    self.overlap_len = int(np.array(v)[0][0])\n",
    "                elif k == 'rbm_batch_size':\n",
    "                    self.rbm_batch_size = int(np.array(v)[0][0])\n",
    "                    print(\"(self.rbm_batch_size=\",self.rbm_batch_size,\")\",end=\" \")\n",
    "#                     self.opts_dict[k] = self.rbm_batch_size\n",
    "                elif k == 'rbm_learn_rate_binary':\n",
    "                    self.rbm_learn_rate_binary = np.array(v)\n",
    "#                     self.opts_dict[k] = self.rbm_learn_rate_binary\n",
    "                elif k == 'rbm_learn_rate_real':\n",
    "                    self.rbm_learn_rate_real = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.rbm_learn_rate_real\n",
    "                elif k == 'rbm_max_epoch':\n",
    "                    self.rbm_max_epoch = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.rbm_max_epoch\n",
    "                elif k == 'save_on_fly':\n",
    "                    self.save_on_fly = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.save_on_fly\n",
    "                elif k == 'sgd_batch_size':\n",
    "                    self.sgd_batch_size = int(np.array(v)[0][0]) # BATCH_SIZE for training net\n",
    "                    print(\"(self.sgd_batch_size:\",self.sgd_batch_size,\")\",end=\" \")\n",
    "#                     self.opts_dict[k] = self.sgd_batch_size\n",
    "                elif k == 'sgd_learn_rate':\n",
    "                    self.sgd_learn_rate = np.array(v)\n",
    "#                     self.opts_dict[k] = self.sgd_learn_rate\n",
    "                elif k == 'sgd_max_epoch':\n",
    "                    self.sgd_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.sgd_max_epoch\n",
    "                elif k == 'split_tanh1_c1':\n",
    "                    self.split_tanh1_c1 = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.split_tanh1_c1\n",
    "                elif k == 'split_tanh1_c2':\n",
    "                    self.split_tanh1_c2 = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.split_tanh1_c2\n",
    "                elif k == 'unit_type_hidden':\n",
    "                    self.unit_type_hidden = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_hidden += chr(c[0])\n",
    "\n",
    "                elif k == 'unit_type_output':\n",
    "                    self.unit_type_output = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_output += chr(c[0])\n",
    "                elif k == 'winlen':\n",
    "                    self.winlen = int(np.array(v)[0][0])\n",
    "                elif k == 'feawin':\n",
    "                    self.feawin = int(np.array(v)[0][0])\n",
    "\n",
    "                        \n",
    "\n",
    "    # Read different data files (Train, Dev, Test)\n",
    "    def read_data(self, FILE_NAME, DATA_TYPE):\n",
    "        print(FILE_NAME, os.path.isfile(FILE_NAME))\n",
    "\n",
    "        with h5py.File(FILE_NAME, 'r') as f:\n",
    "            # print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_TEST,len(f.keys())) )\n",
    "\n",
    "            for k, v in f.items():\n",
    "                if DATA_TYPE.lower() == 'train':\n",
    "                    # Features (input)\n",
    "                    if k == 'trData':\n",
    "                        self.trData = np.transpose(np.array(v))\n",
    "                    elif k == 'trLabel_r':\n",
    "                        self.trLabel_r = np.transpose(np.array(v))\n",
    "                    elif k == 'trLabel_i':\n",
    "                        self.trLabel_i = np.transpose(np.array(v))\n",
    "                    elif k == 'numframes':\n",
    "                        self.trNumframes = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_r':\n",
    "                        self.trCleanSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_i':\n",
    "                        self.trCleanSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_r':\n",
    "                        self.trMixtureSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_i':\n",
    "                        self.trMixtureSpec_i = np.transpose(np.array(v))\n",
    "                \n",
    "                elif DATA_TYPE.lower() == 'dev':\n",
    "                    # Features (input)\n",
    "                    if k == 'cvData':\n",
    "                        self.cvData = np.transpose(np.array(v))\n",
    "                    elif k == 'cvLabel_r':\n",
    "                        self.cvLabel_r = np.transpose(np.array(v))\n",
    "                    elif k == 'cvLabel_i':\n",
    "                        self.cvLabel_i = np.transpose(np.array(v))\n",
    "                    elif k == 'numframes':\n",
    "                        self.cvNumframes = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_r':\n",
    "                        self.cvCleanSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_i':\n",
    "                        self.cvCleanSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_r':\n",
    "                        self.cvMixtureSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_i':\n",
    "                        self.cvMixtureSpec_i = np.transpose(np.array(v))\n",
    "            \n",
    "                elif DATA_TYPE.lower() == 'test':\n",
    "                    # Features (input)\n",
    "                    if k == 'teData':\n",
    "                        self.teData = np.transpose(np.array(v))\n",
    "                    elif k == 'teLabel_r':\n",
    "                        self.teLabel_r = np.transpose(np.array(v))\n",
    "                    elif k == 'teLabel_i':\n",
    "                        self.teLabel_i = np.transpose(np.array(v))\n",
    "                    elif k == 'numframes':\n",
    "                        self.teNumframes = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_r':\n",
    "                        self.teCleanSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_i':\n",
    "                        self.teCleanSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_r':\n",
    "                        self.teMixtureSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_i':\n",
    "                        self.teMixtureSpec_i = np.transpose(np.array(v))\n",
    "            \n",
    "            # Display statistics\n",
    "            if DATA_TYPE.lower() == 'train':\n",
    "                self.trLabel = np.concatenate((self.trLabel_r, self.trLabel_i), axis=1)\n",
    "                self.display_stat(DATA_TYPE=\"train\")\n",
    "                \n",
    "            elif DATA_TYPE.lower() == 'dev':\n",
    "                self.cvLabel = np.concatenate((self.cvLabel_r, self.cvLabel_i), axis=1)\n",
    "                self.display_stat(DATA_TYPE=\"dev\")\n",
    "                \n",
    "            elif DATA_TYPE.lower() == 'test':\n",
    "                self.teLabel = np.concatenate((self.teLabel_r, self.teLabel_i), axis=1)\n",
    "                self.display_stat(DATA_TYPE=\"test\")\n",
    "                \n",
    "                \n",
    "                \n",
    "    # Display the simple statistics of Data (train, dev, test)\n",
    "    def display_stat(self, DATA_TYPE):\n",
    "        if DATA_TYPE.lower() == 'train':\n",
    "            print(\"\\nSummary->[TRAIN DATA]\")\n",
    "            print(\"trNumframes.shape=\", self.trNumframes.shape)\n",
    "            \n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trData)\n",
    "            print(\"trData.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trLabel_r)\n",
    "            print(\"trLabel_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trLabel_i)\n",
    "            print(\"trLabel_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trLabel)\n",
    "            print(\"trLabel.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trCleanSpec_r)\n",
    "            print(\"trCleanSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trCleanSpec_i)\n",
    "            print(\"trCleanSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trMixtureSpec_r)\n",
    "            print(\"trMixtureSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trMixtureSpec_i)\n",
    "            print(\"trMixtureSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            \n",
    "        elif DATA_TYPE.lower() == 'dev':\n",
    "            print(\"\\nSummary->[DEV DATA]\")\n",
    "            print(\"cvNumframes.shape=\", self.cvNumframes.shape)\n",
    "            \n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvData)\n",
    "            print(\"cvData.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvLabel_r)\n",
    "            print(\"cvLabel_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvLabel_i)\n",
    "            print(\"cvLabel_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvLabel)\n",
    "            print(\"cvLabel.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvCleanSpec_r)\n",
    "            print(\"cvCleanSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvCleanSpec_i)\n",
    "            print(\"cvCleanSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvMixtureSpec_r)\n",
    "            print(\"cvMixtureSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvMixtureSpec_i)\n",
    "            print(\"cvMixtureSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "\n",
    "            \n",
    "        elif DATA_TYPE.lower() == 'test':\n",
    "            print(\"\\nSummary->[TEST DATA]\")\n",
    "            print(\"teNumframes.shape=\", self.teNumframes.shape)\n",
    "            \n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teData)\n",
    "            print(\"teData.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teLabel_r)\n",
    "            print(\"teLabel_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teLabel_i)\n",
    "            print(\"teLabel_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teLabel)\n",
    "            print(\"teLabel.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teCleanSpec_r)\n",
    "            print(\"teCleanSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teCleanSpec_i)\n",
    "            print(\"teCleanSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teMixtureSpec_r)\n",
    "            print(\"teMixtureSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teMixtureSpec_i)\n",
    "            print(\"teMixtureSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "                        \n",
    "    # Helper function for display, returns actual values\n",
    "    def data_stat(self, data):\n",
    "        return data.shape,np.mean(data),np.var(data),np.std(data),np.amin(data),np.amax(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an object which does the parameter (Matlab *Opts* object) reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt keys:\n",
      "0->ARMA_order,1->Fs,2->ada_grad_eps,3->ada_sgd_scale,4->arma_order,5->change_momentum_point,6->clip_level,7->cost_function,8->cv_interval,9->dim_input,( dim_input = 1230 ) 10->dim_output,( dim_output = 963 ) 11->drop_ratio,12->eval_on_gpu,13->fRange,14->feawin,15->final_momentum,16->hid_struct,17->hop_size,18->hopsize,19->initial_momentum,20->isDropout,21->isDropoutInput,22->isGPU,23->isNormalize,24->isPretrain,25->labcompress,26->labeltype,27->labwin,28->learner,29->logistic_max,30->logistic_steep,31->net_struct,32->nfft,33->noise,34->numGammatoneChans,35->overlap,36->overlap_len,37->rbm_batch_size,(self.rbm_batch_size= 1024 ) 38->rbm_learn_rate_binary,39->rbm_learn_rate_real,40->rbm_max_epoch,41->save_on_fly,42->sgd_batch_size,(self.sgd_batch_size: 1024 ) 43->sgd_learn_rate,44->sgd_max_epoch,45->split_tanh1_c1,46->split_tanh1_c2,47->tr_mu,48->tr_std,49->unit_type_hidden,50->unit_type_output,51->win_len,52->winlen,"
     ]
    }
   ],
   "source": [
    "opts = Opts(PARAM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opts.labwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dnn_models/Train_datas.mat True\n",
      "\n",
      "Summary->[TRAIN DATA]\n",
      "trNumframes.shape= (15000, 1)\n",
      "trData.shape=(1951920, 1230), mean=-0.0008, variance=0.6125, std=0.7826, range=[-11.5572,12.4862]\n",
      "trLabel_r.shape=(1951920, 963), mean=0.0295, variance=0.0217, std=0.1473, range=[-10.0000,10.0000]\n",
      "trLabel_i.shape=(1951920, 963), mean=0.0000, variance=0.0133, std=0.1154, range=[-10.0000,10.0000]\n",
      "trLabel.shape=(1951920, 1926), mean=0.0148, variance=0.0177, std=0.1331, range=[-10.0000,10.0000]\n",
      "trCleanSpec_r.shape=(1951920, 321), mean=0.0021, variance=0.2999, std=0.5477, range=[-46.6837,39.0942]\n",
      "trCleanSpec_i.shape=(1951920, 321), mean=-0.0000, variance=0.2914, std=0.5398, range=[-50.2884,37.3594]\n",
      "trMixtureSpec_r.shape=(1951920, 321), mean=0.0046, variance=3.2842, std=1.8122, range=[-133.2251,127.9604]\n",
      "trMixtureSpec_i.shape=(1951920, 321), mean=-0.0000, variance=3.1975, std=1.7882, range=[-133.8369,109.0255]\n"
     ]
    }
   ],
   "source": [
    "opts.read_data(TrainData_FILE, DATA_TYPE=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dev (validation) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dnn_models/CrossValidation_datas.mat True\n",
      "\n",
      "Summary->[DEV DATA]\n",
      "cvNumframes.shape= (3300, 1)\n",
      "cvData.shape=(449610, 1230), mean=-0.0000, variance=0.6076, std=0.7795, range=[-9.6298,10.4941]\n",
      "cvLabel_r.shape=(449610, 963), mean=0.0286, variance=0.0213, std=0.1458, range=[-10.0000,10.0000]\n",
      "cvLabel_i.shape=(449610, 963), mean=-0.0000, variance=0.0130, std=0.1138, range=[-10.0000,10.0000]\n",
      "cvLabel.shape=(449610, 1926), mean=0.0143, variance=0.0173, std=0.1316, range=[-10.0000,10.0000]\n",
      "cvCleanSpec_r.shape=(449610, 321), mean=0.0020, variance=0.3002, std=0.5479, range=[-40.8223,41.5895]\n",
      "cvCleanSpec_i.shape=(449610, 321), mean=-0.0000, variance=0.2895, std=0.5381, range=[-39.2169,43.0322]\n",
      "cvMixtureSpec_r.shape=(449610, 321), mean=0.0044, variance=3.2486, std=1.8024, range=[-121.7945,125.0182]\n",
      "cvMixtureSpec_i.shape=(449610, 321), mean=-0.0001, variance=3.1470, std=1.7740, range=[-101.6771,93.7446]\n"
     ]
    }
   ],
   "source": [
    "opts.read_data(DevData_FILE, DATA_TYPE=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Test data\n",
    "\n",
    "You can run this portion later, when you need this data. It will help with ram storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dnn_models/Test_datas.mat True\n",
      "\n",
      "Summary->[TEST DATA]\n",
      "teNumframes.shape= (545, 1)\n",
      "teData.shape=(72440, 1230), mean=-0.0000, variance=0.6272, std=0.7919, range=[-9.4365,10.2858]\n",
      "teLabel_r.shape=(72440, 963), mean=0.0308, variance=0.0225, std=0.1500, range=[-10.0000,10.0000]\n",
      "teLabel_i.shape=(72440, 963), mean=-0.0000, variance=0.0138, std=0.1175, range=[-10.0000,10.0000]\n",
      "teLabel.shape=(72440, 1926), mean=0.0154, variance=0.0184, std=0.1356, range=[-10.0000,10.0000]\n",
      "teCleanSpec_r.shape=(72440, 321), mean=0.0020, variance=0.2997, std=0.5475, range=[-42.8964,38.0338]\n",
      "teCleanSpec_i.shape=(72440, 321), mean=-0.0000, variance=0.2920, std=0.5403, range=[-37.0904,45.6445]\n",
      "teMixtureSpec_r.shape=(72440, 321), mean=0.0041, variance=3.4055, std=1.8454, range=[-104.7786,93.6277]\n",
      "teMixtureSpec_i.shape=(72440, 321), mean=-0.0000, variance=3.3203, std=1.8222, range=[-96.0003,123.9898]\n"
     ]
    }
   ],
   "source": [
    "opts.read_data(TestData_FILE, DATA_TYPE=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Next Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_batch(opts, batch_size, CYCLE):\n",
    "    '''\n",
    "    Return the total number of batch depending on CYCLE {Train, Dev, Test}\n",
    "    \n",
    "    parameters: \n",
    "        opts <object> = all the parametric data\n",
    "        batch_size <int>        \n",
    "        CYCLE <string>  = {train, dev, test}\n",
    "        \n",
    "    returns:\n",
    "        num_batch <int> = total number of batches\n",
    "        total_num_samples <int> = total number of frame, shape[0] of data\n",
    "    '''\n",
    "    \n",
    "    if CYCLE.lower()=='train':\n",
    "        total_num_samples = opts.trData.shape[0]\n",
    "        \n",
    "    elif CYCLE.lower()=='dev':\n",
    "        total_num_samples = opts.cvData.shape[0]\n",
    "        \n",
    "    elif CYCLE.lower()=='test':\n",
    "        total_num_samples = opts.teData.shape[0]\n",
    "    \n",
    "    \n",
    "    BATCH_TYPE = 'actual' # {actual,floor}\n",
    "    # 'actual' uses all frames, so last batch can be smaller than batch_size.\n",
    "    # 'floor' may not use all the frames, but all the batches will be of equal size.\n",
    "    \n",
    "    if BATCH_TYPE.lower() == 'actual':\n",
    "        num_batch = math.ceil(total_num_samples/batch_size) \n",
    "    elif BATCH_TYPE.lower() == 'floor':\n",
    "        num_batch = total_num_samples//batch_size \n",
    "        \n",
    "        \n",
    "    return num_batch, total_num_samples\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ready_batchID(opts, batch_size, CYCLE):\n",
    "    '''\n",
    "    Create a 2D array with (start,end) frame number of each batch.\n",
    "    \n",
    "    parameters: \n",
    "        opts <object> = all the parametric data\n",
    "        batch_size <int>        \n",
    "        CYCLE <string>  = {train, dev, test}  \n",
    "        \n",
    "    returns:\n",
    "        batchIDs <nparray.2D> = shape(number_of_batches, 2); \n",
    "                                each row (s,e) is the inclusive start(s) and end(e) frame number\n",
    "        total_num_samples <int> = total number of frame, shape[0] of data\n",
    "    '''\n",
    "\n",
    "    num_batch, total_num_samples = number_of_batch(opts, batch_size, CYCLE)\n",
    "        \n",
    "    batchIDs = np.zeros((num_batch,2))\n",
    "    # (s,e); s = the begining frame number and e = the ending frame number of a batch\n",
    "    \n",
    "    s = np.where( np.arange(total_num_samples)%batch_size == 0 )\n",
    "    e = np.where( np.arange(total_num_samples)%batch_size == batch_size-1 )\n",
    "        \n",
    "    batchIDs[:, 0] = s[0]\n",
    "    \n",
    "    # when all batches are equal of batch_size   \n",
    "    if e[0].shape[0] >= num_batch:\n",
    "        batchIDs[:, 1] = e[0][0:num_batch]\n",
    "\n",
    "    # when last batch is smaller than batch_size\n",
    "    elif e[0].shape[0] < num_batch:\n",
    "        batchIDs[:num_batch-1, 1] = e[0][0:num_batch]\n",
    "        batchIDs[-1,1] = total_num_samples-1\n",
    "        \n",
    "    return batchIDs, total_num_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. DNN Batch+ID Generator\n",
    "\n",
    "Every time it returns a batch `(x, y, ids)`. **x** is the features, **y** is the labels and **ids** is the data frame numbers which are randomly selected. \n",
    "\n",
    "***NOTE:*** Use this generator when you need **ids** explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn_batch_id_gen(opts, batch_size, CYCLE):\n",
    "    '''\n",
    "    Return a batch with the selected data frame ids.\n",
    "\n",
    "    parameters: \n",
    "        opts <object> = all the parametric data\n",
    "        total_num_samples <int> = Shape[1] of data\n",
    "        batch_size <int>        \n",
    "        cycle <string>  = train, dev, test\n",
    "        \n",
    "    returns:\n",
    "        x <nparray.2D> = shape(number_of_batches, 1230); data/features/input\n",
    "        y <nparray.2D> = shape(number_of_batches, 963); labels\n",
    "        ids <list> = randomly selected data frame ids\n",
    "    '''\n",
    "\n",
    "    batchIDs, total_num_samples = ready_batchID(opts, batch_size, CYCLE) \n",
    "    seq = np.random.permutation(total_num_samples)\n",
    "    # randomly shuffled frames\n",
    "    \n",
    "    for batch in batchIDs:\n",
    "        ids = seq[ int( batch[0]): int( batch[1]) ]\n",
    "        # selected frame ids\n",
    "        \n",
    "        if CYCLE.lower()=='train':\n",
    "            x = opts.trData[ ids ]\n",
    "            y = opts.trLabel[ ids ]\n",
    "\n",
    "        elif CYCLE.lower()=='dev':\n",
    "            x = opts.cvData[ ids ]\n",
    "            y = opts.cvLabel[ ids ]\n",
    "\n",
    "        elif CYCLE.lower()=='test':\n",
    "            x = opts.teData[ ids ]\n",
    "            y = opts.teLabel[ids ]\n",
    "\n",
    "        yield [x, y, ids]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. DNN Batch Generator\n",
    "\n",
    "Every time it returns a batch `(x, y)`. **x** is the features and **y** is the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_DataGenerator_MA(keras.utils.Sequence):\n",
    "    '''\n",
    "    Generates data for Keras\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, opts, batch_size=32, CYCLE='Train', shuffle=True):\n",
    "        '''\n",
    "        Initialization\n",
    "        '''\n",
    "        self.opts = opts\n",
    "        self.batch_size = batch_size\n",
    "        self.CYCLE = CYCLE\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.BATCH_TYPE = 'floor' # {actual,floor}\n",
    "        # 'actual' uses all frames, so last batch can be smaller than batch_size.\n",
    "        # 'floor' may not use all the frames, but all the batches will be of equal size.\n",
    "        \n",
    "        self.__len__()    \n",
    "        self.batchIDs = self.ready_batchID() \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Denotes the number of batches per epoch.\n",
    "\n",
    "        returns:\n",
    "            num_batch <int> = total number of batches\n",
    "            \n",
    "        variable:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test}\n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            self.total_num_samples = self.opts.trData.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            self.total_num_samples = self.opts.cvData.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            self.total_num_samples = self.opts.teData.shape[0]\n",
    "\n",
    "\n",
    "        if self.BATCH_TYPE.lower() == 'actual':\n",
    "            self.num_batch = math.ceil(self.total_num_samples/self.batch_size) \n",
    "            \n",
    "        elif self.BATCH_TYPE.lower() == 'floor':\n",
    "            self.num_batch = self.total_num_samples//self.batch_size \n",
    "\n",
    "\n",
    "        return int(self.num_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ready_batchID(self):\n",
    "        '''\n",
    "        Create a 2D array with (start,end) frame number of each batch.\n",
    "\n",
    "        returns:\n",
    "            batchIDs <nparray.2D> = shape(number_of_batches, 2); \n",
    "                                    each row (s,e) is the inclusive start(s) and end(e) frame number\n",
    "        variables:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test} \n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        batchIDs = np.zeros((self.num_batch,2))\n",
    "        # (s,e); s = the begining frame number and e = the ending frame number of a batch\n",
    "\n",
    "        s = np.where( np.arange(self.total_num_samples)%self.batch_size == 0 )\n",
    "        e = np.where( np.arange(self.total_num_samples)%self.batch_size == self.batch_size-1 )\n",
    "\n",
    "        batchIDs[:, 0] = s[0]\n",
    "\n",
    "        # when all batches are equal of batch_size   \n",
    "        if e[0].shape[0] >= self.num_batch:\n",
    "            batchIDs[:, 1] = e[0][0:self.num_batch]\n",
    "\n",
    "        # when last batch is smaller than batch_size\n",
    "        elif e[0].shape[0] < self.num_batch:\n",
    "            batchIDs[:self.num_batch-1, 1] = e[0][0:self.num_batch]\n",
    "            batchIDs[-1,1] = self.total_num_samples-1\n",
    "\n",
    "        return batchIDs\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Generate one batch of data\n",
    "        '''\n",
    "        # Get the correcponding batch\n",
    "        batch = self.batchIDs[index]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = self.indexes[ int( batch[0]): int( batch[1]) ]\n",
    "        \n",
    "        # Generate data\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.total_num_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        '''\n",
    "        Generates data containing batch_size samples.\n",
    "        '''\n",
    "        if CYCLE.lower()=='train':\n",
    "            x = opts.trData[ list_IDs_temp ]\n",
    "            y = opts.trLabel[ list_IDs_temp ]\n",
    "\n",
    "        elif CYCLE.lower()=='dev':\n",
    "            x = opts.cvData[ list_IDs_temp ]\n",
    "            y = opts.cvLabel[ list_IDs_temp ]\n",
    "\n",
    "        elif CYCLE.lower()=='test':\n",
    "            x = opts.teData[ list_IDs_temp ]\n",
    "            y = opts.teLabel[list_IDs_temp ]\n",
    "\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_DataGenerator_SA(keras.utils.Sequence):\n",
    "    '''\n",
    "    Generates data for Keras\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, CYCLE='Train', opts=None, input_list=['features','mix_spec_r','mix_spec_i'], \n",
    "                 batch_size=32, shuffle=True):\n",
    "        '''\n",
    "        Initialization\n",
    "        '''\n",
    "        self.opts = opts\n",
    "        # key of the dictionary used as input\n",
    "        self.input_list = input_list\n",
    "        self.batch_size = batch_size\n",
    "        self.CYCLE = CYCLE\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.BATCH_TYPE = 'floor' # {actual,floor}\n",
    "        # 'actual' uses all frames, so last batch can be smaller than batch_size.\n",
    "        # 'floor' may not use all the frames, but all the batches will be of equal size.\n",
    "        \n",
    "        self.__len__()    \n",
    "        self.batchIDs = self.ready_batchID() \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Denotes the number of batches per epoch.\n",
    "\n",
    "        returns:\n",
    "            num_batch <int> = total number of batches\n",
    "            \n",
    "        variable:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test}\n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            self.total_num_samples = self.opts.trData.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            self.total_num_samples = self.opts.cvData.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            self.total_num_samples = self.opts.teData.shape[0]\n",
    "\n",
    "\n",
    "        if self.BATCH_TYPE.lower() == 'actual':\n",
    "            self.num_batch = math.ceil(self.total_num_samples/self.batch_size) \n",
    "            \n",
    "        elif self.BATCH_TYPE.lower() == 'floor':\n",
    "            self.num_batch = self.total_num_samples//self.batch_size \n",
    "\n",
    "\n",
    "        return int(self.num_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ready_batchID(self):\n",
    "        '''\n",
    "        Create a 2D array with (start,end) frame number of each batch.\n",
    "\n",
    "        returns:\n",
    "            batchIDs <nparray.2D> = shape(number_of_batches, 2); \n",
    "                                    each row (s,e) is the inclusive start(s) and end(e) frame number\n",
    "        variables:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test} \n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        batchIDs = np.zeros((self.num_batch,2))\n",
    "        # (s,e); s = the begining frame number and e = the ending frame number of a batch\n",
    "\n",
    "        s = np.where( np.arange(self.total_num_samples)%self.batch_size == 0 )\n",
    "        e = np.where( np.arange(self.total_num_samples)%self.batch_size == self.batch_size-1 )\n",
    "\n",
    "        batchIDs[:, 0] = s[0]\n",
    "\n",
    "        # when all batches are equal of batch_size   \n",
    "        if e[0].shape[0] >= self.num_batch:\n",
    "            batchIDs[:, 1] = e[0][0:self.num_batch]\n",
    "\n",
    "        # when last batch is smaller than batch_size\n",
    "        elif e[0].shape[0] < self.num_batch:\n",
    "            batchIDs[:self.num_batch-1, 1] = e[0][0:self.num_batch]\n",
    "            batchIDs[-1,1] = self.total_num_samples-1\n",
    "\n",
    "        return batchIDs\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Generate one batch of data\n",
    "        '''\n",
    "        # Get the correcponding batch\n",
    "        batch = self.batchIDs[index]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = self.indexes[ int( batch[0]): int( batch[1]) ]\n",
    "        \n",
    "        # Generate data\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.total_num_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        '''\n",
    "        Generates data containing batch_size samples.\n",
    "        '''\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            x = self.opts.trData[ list_IDs_temp ]\n",
    "            y = self.opts.trLabel[ list_IDs_temp ]\n",
    "            mix_stft_r = self.opts.trMixtureSpec_r[list_IDs_temp]\n",
    "            mix_stft_i = self.opts.trMixtureSpec_i[list_IDs_temp]\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            x = self.opts.cvData[ list_IDs_temp ]\n",
    "            y = self.opts.cvLabel[ list_IDs_temp ]\n",
    "            mix_stft_r = self.opts.cvMixtureSpec_r[list_IDs_temp]\n",
    "            mix_stft_i = self.opts.cvMixtureSpec_i[list_IDs_temp]\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            x = self.opts.teData[ list_IDs_temp ]\n",
    "            y = self.opts.teLabel[list_IDs_temp ]\n",
    "            mix_stft_r = self.opts.teMixtureSpec_r[list_IDs_temp]\n",
    "            mix_stft_i = self.opts.teMixtureSpec_i[list_IDs_temp]\n",
    "            \n",
    "\n",
    "        return {self.input_list[0]:x, self.input_list[1]:mix_stft_r, self.input_list[2]:mix_stft_i}, y\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def dnn_batch_gen(opts, batch_size, CYCLE):\n",
    "    '''\n",
    "    Return a batch. \n",
    "\n",
    "    parameters: \n",
    "        opts <object> = all the parametric data\n",
    "        total_num_samples <int> = Shape[1] of data\n",
    "        batch_size <int>        \n",
    "        cycle <string>  = train, dev, test\n",
    "        \n",
    "    returns:\n",
    "        x <nparray.2D> = shape(number_of_batches, 1230); data/features/input\n",
    "        y <nparray.2D> = shape(number_of_batches, 963); labels\n",
    "    '''\n",
    "\n",
    "    batchIDs, total_num_samples = ready_batchID(opts, batch_size, CYCLE) \n",
    "    seq = np.random.permutation(total_num_samples)\n",
    "    # randomly shuffled frames\n",
    "\n",
    "    for e, batch in enumerate( range(batchID.shape[0]) ):\n",
    "        ids = seq[ batchID[batch][0]:batchID[batch][1] ]\n",
    "        # selected frame ids\n",
    "        \n",
    "        if cycle.lower()=='train':\n",
    "            x = opts.trData[ ids ]\n",
    "            y = opts.trLabel[ ids ]\n",
    "\n",
    "        elif cycle.lower()=='dev':\n",
    "            x = opts.cvData[ ids ]\n",
    "            y = opts.cvLabel[ ids ]\n",
    "\n",
    "        elif cycle.lower()=='test':\n",
    "            x = opts.teData[ ids ]\n",
    "            y = opts.teLabel[ids ]\n",
    "\n",
    "        yield [x, y]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_freqs = 963\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Unwrapping\n",
    "\n",
    "Need to signal approximation to unwrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrapAugmentedTF_wAvg(impt_mag, num_per_side, wrap_freqs, unwrap_mag=None):\n",
    "    '''\n",
    "    Description: Unwrap the augmented time-frequency (T-F) representation and compute the average.\n",
    "\n",
    "    Input:\n",
    "        impt_mag: wrapped T-F representation with dimensions (2*T+1)*d x m\n",
    "        num_per_side(T): number of frames to left and right of each frame used to augmented T-F representation\n",
    "    \n",
    "    Output:\n",
    "        unwrap_avgmag: unwrapped and averaged T-F representation with dimensions d x m\n",
    "        unwrap_mag: unwrapped T-F representation with dimensions d x (2*T+1) x m\n",
    "    '''\n",
    "\n",
    "    sliding_window_len       = 2*num_per_side + 1\n",
    "\n",
    "    numWrapFreqs = wrap_freqs\n",
    "    numFrames = K.shape(impt_mag)[1] #tensor\n",
    "    numFreqs = numWrapFreqs//sliding_window_len\n",
    "    \n",
    "    print(\"<numFrames>\",numFrames)\n",
    "    print(\"<numFreqs>\",numFreqs)\n",
    "    \n",
    "    unwrap_avgmag = K.zeros( (numFreqs,numFrames) ) #tensor\n",
    "    print(\"<unwrap_avgmag>\",unwrap_avgmag)\n",
    "    unwrap_avgmagTemp = []\n",
    "\n",
    "    \n",
    "    if num_per_side > 0:\n",
    "        if unwrap_mag is None:\n",
    "\n",
    "            unwrap_mag = K.zeros( (numFreqs,sliding_window_len,numFrames) ) #tensor\n",
    "            curr_ind_location = K.zeros ( (numFrames), dtype='int32') #tensor\n",
    "    \n",
    "            #for frameNum = 1:numFrames\n",
    "            def f_numFrames(frameNum):\n",
    "                # Get the indices for the frames used in this augmented matrix\n",
    "                frame_inds=K.arange(frameNum-num_per_side, frameNum+num_per_side+1) #tensor\n",
    "                print('1.<frame_inds>',frame_inds ) \n",
    "                frame_inds = tf.clip_by_value(frame_inds, K.variable(0,dtype='int32'), (numFrames-1))\n",
    "                print('2.<frame_inds>',frame_inds)\n",
    "              \n",
    "                # Unwrap the data for this frame, Size d x (2*T + 1)\n",
    "                slid_win_data = K.reshape( impt_mag[...,frameNum], (numFreqs,sliding_window_len) ) \n",
    "              \n",
    "                #for ind_num = 1:length(frame_inds)\n",
    "                def f_frame_inds(ind_num):        \n",
    "                    slid = slid_win_data[:,ind_num]\n",
    "                    fi = frame_inds[ind_num]\n",
    "\n",
    "                    '''\n",
    "                    Do stack() kind of think here,\n",
    "                    check dimensons with matlab code\n",
    "                    correct the loss_SA() \n",
    "                    \n",
    "                    unwrap_mag(:,curr_ind_location(fi), fi) = slid;\n",
    "                    '''\n",
    "                    \n",
    "#                     tf.assign( unwrap_mag[:, K.gather(curr_ind_location,fi), fi], slid)\n",
    "#                     \n",
    "\n",
    "#                     same = K.equal(curr_ind_location, fi)\n",
    "#                     tf.assign(curr_ind_location, curr_ind_location+ K.cast(same, dtype='int32'))\n",
    "                    return slid\n",
    "                \n",
    "                \n",
    "                slid = K.map_fn(f_frame_inds, K.arange(K.shape(frame_inds)[0]) , dtype='float32')\n",
    "                return slid\n",
    "            \n",
    "            slid = K.map_fn(f_numFrames, K.arange(numFrames), dtype='float32')\n",
    "    \n",
    "    \n",
    "#         for frameNum in range(numFrames):\n",
    "#             tf.assign( unwrap_avgmag[:,frameNum] , K.mean(unwrap_mag[:,:,frameNum],axis=1) )\n",
    "        \n",
    "        def f_frameNum(frameNum):\n",
    "            print(\"<[[frameNum]]>\", K.expand_dims(K.expand_dims(frameNum,axis=-1), axis=-1) )\n",
    "            uw_mag = tf.gather_nd(unwrap_mag, [[frameNum]] )\n",
    "            print(\"1.<uw_mag>\",uw_mag)\n",
    "            uw_mag = K.mean(uw_mag,axis=1)\n",
    "            print(\"2.<uw_mag>\",uw_mag)  \n",
    "#             tf.assign( unwrap_avgmag[:,frameNum] , K.mean(uw_mag,axis=1) )\n",
    "#             return unwrap_avgmag\n",
    "            return uw_mag\n",
    "        \n",
    "        unwrap_avgmagTemp.append( K.map_fn(f_frameNum, K.arange(numFrames) , dtype='float32') )\n",
    "        unwrap_avgmag = tf.stack(unwrap_avgmagTemp, axis=1)\n",
    "        print(\"<unwrap_avgmag>\",unwrap_avgmag)\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        unwrap_avgmag = impt_mag;\n",
    "        unwrap_mag = 0;\n",
    "\n",
    "    return unwrap_avgmag,unwrap_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Mask Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction_MA(yTrue,yPred):\n",
    "    \n",
    "    r, c = K.shape(yTrue)[0], K.shape(yTrue)[1]\n",
    "    \n",
    "    samples = K.cast(r,dtype='float32') \n",
    "    # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    cost_real = K.sum(K.square(yTrue[:, :c//2]- yPred[:, :c//2]))\n",
    "    cost_imag = K.sum(K.square(yTrue[:, c//2:]- yPred[:, c//2:]))\n",
    "    \n",
    "    return 0.5*(cost_real+cost_imag)/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Signal Approximation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def lossFunction_SA(opts):\n",
    "    '''\n",
    "    variables:\n",
    "        opts.batch_ids <list> = id of randomly selected frames\n",
    "    '''\n",
    "    \n",
    "    def customLoss_train_SA(yTrue,yPred):\n",
    "\n",
    "        r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "        samples = K.cast(r,dtype='float32') \n",
    "        # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "\n",
    "        pred_real = yPred[:, :(c//2)]\n",
    "        pred_imag = yPred[:, (c//2):]\n",
    "\n",
    "        true_real = yTrue[:, :(c//2)]\n",
    "        true_imag = yTrue[:, (c//2):]\n",
    "\n",
    "\n",
    "        mix_stft = K.cast(K.transpose(opts.mix_spec[opts.batch_ids, :321]),dtype='complex64') + \\\n",
    "                1j*K.cast(K.transpose(opts.mix_spec[opts.batch_ids, 321:]),dtype='complex64')\n",
    "\n",
    "        complex_irmmask = K.cast(K.transpose(pred_real),dtype='complex64') + \\\n",
    "                        1j*K.cast(K.transpose(pred_imag),dtype='complex64') \n",
    "        # estimated mask\n",
    "\n",
    "\n",
    "        if opts.labwin==0:\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        else:\n",
    "            real_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_real), opts.labwin)\n",
    "            imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_imag), opts.labwin)\n",
    "            print(real_output_unwrap.shape)\n",
    "\n",
    "            clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_real), opts.labwin)\n",
    "            clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_imag), opts.labwin)\n",
    "            print(clean_real_unwrap.shape)\n",
    "\n",
    "            complex_irmmask    = K.cast(K.transpose(real_output_unwrap),dtype='complex64') + \\\n",
    "                                1j*K.cast(K.transpose(imag_output_unwrap),dtype='complex64')\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "            true_irmmask       = K.cast(K.transpose(clean_real_unwrap),dtype='complex64') + \\\n",
    "                                1j*K.cast(K.transpose(clean_imag_unwrap),dtype='complex64')\n",
    "            y                  = true_irmmask*mix_stft\n",
    "\n",
    "            \n",
    "        diff = y-estimate\n",
    "        cost_real = K.sum(K.square(np.real(diff)))\n",
    "        cost_imag = K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "        return  0.5*(cost_real+cost_imag)/samples\n",
    "    \n",
    "    \n",
    "    return customLoss_train_SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction_SA(yTrue,yPred):\n",
    "\n",
    "    X_r, X_i = mix_spec_r, mix_spec_i\n",
    "\n",
    "    labwin = opts.labwin\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "\n",
    "    samples = K.cast(r,dtype='float32') \n",
    "    # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "\n",
    "    pred_real = yPred[:, :(c//2)] \n",
    "    pred_imag = yPred[:, (c//2):]\n",
    "\n",
    "    true_real = yTrue[:, :(c//2)]\n",
    "    true_imag = yTrue[:, (c//2):]\n",
    "\n",
    "    mix_stft = K.cast(K.transpose(X_r),dtype='complex64') + \\\n",
    "            1j*K.cast(K.transpose(X_i),dtype='complex64')\n",
    "    print(\"<mix_stft>\",mix_stft)\n",
    "\n",
    "    complex_irmmask = K.cast(K.transpose(pred_real),dtype='complex64') + \\\n",
    "                    1j*K.cast(K.transpose(pred_imag),dtype='complex64') \n",
    "    # estimated mask\n",
    "    \n",
    "    print(\"<complex_irmmask>\",complex_irmmask)\n",
    "    print(\"<28.complex_irmmask*mix_stft>\",complex_irmmask*mix_stft)\n",
    "\n",
    "\n",
    "    if labwin==0:\n",
    "        estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "    else:\n",
    "        real_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_real), labwin, wrap_freqs)\n",
    "        imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_imag), labwin, wrap_freqs)\n",
    "        print(\"<real_output_unwrap>\",real_output_unwrap.shape)\n",
    "        print(\"<imag_output_unwrap>\",imag_output_unwrap.shape)\n",
    "\n",
    "        clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_real), labwin, wrap_freqs)\n",
    "        clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_imag), labwin, wrap_freqs)\n",
    "        print(\"<clean_real_unwrap>\",clean_real_unwrap.shape)\n",
    "        print(\"<clean_imag_unwrap>\",clean_imag_unwrap.shape)\n",
    "\n",
    "        complex_irmmask    = K.cast(K.transpose(real_output_unwrap),dtype='complex64') + \\\n",
    "                            1j*K.cast(K.transpose(imag_output_unwrap),dtype='complex64')\n",
    "        print(\"<complex_irmmask>\",complex_irmmask)\n",
    "        estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        true_irmmask       = K.cast(K.transpose(clean_real_unwrap),dtype='complex64') + \\\n",
    "                            1j*K.cast(K.transpose(clean_imag_unwrap),dtype='complex64')\n",
    "        y                  = true_irmmask*mix_stft\n",
    "\n",
    "\n",
    "    diff = y-estimate\n",
    "    cost_real = K.sum(K.square(np.real(diff)))\n",
    "    cost_imag = K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "    return  0.5*(cost_real+cost_imag)/samples\n",
    "\n",
    "\n",
    "# return customLoss_train_SA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opts.sgd_batch_size\n",
    "# batch_size = 5\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "n_input = 1230  # data input\n",
    "n_hidden_1 = 1024  # 1st layer number of neurons\n",
    "n_hidden_2 = 1024  # 2nd layer number of neurons\n",
    "n_hidden_3 = 1024  # 3rd layer number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_train**, **n_train_dim** = ```<number of samples for training, input dimentions>```, final will be (1951920, 1230)\n",
    "\n",
    "**n_dev**, **n_dev_dim** = ```<number of samples for development, input dimentions>```, final will be (449610, 1230)\n",
    "\n",
    "**n_test**, **n_test_dim** =```<number of samples for testing, input dimentions>```, final will be (72440, 1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_train_dim = (195192, 1230) # opts.trData.shape = (1951920, 1230)\n",
    "n_dev, n_dev_dim = (44961, 1230) # opts.cvData.shape =(449610, 1230)\n",
    "n_test, n_test_dim = (72440, 1230) # opts.teData.shape =(72440, 1230)\n",
    "\n",
    "n_train_files = 15000 #15000\n",
    "n_dev_files = 3300 # 3300\n",
    "n_test_files = 545\n",
    "\n",
    "n_classes = (963 + 963)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_VERSION = \"_e14v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST_TRAINMODEL_FILE = \"./dnn_models/lstm_weights\"+ PRETRAIN_VERSION+\"_05.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(BEST_TRAINMODEL_FILE, custom_objects={'customLoss_train':customLoss_train,\n",
    "                                                          'customLoss_val':customLoss_val,\n",
    "                                                          'customLoss_test':customLoss_test,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify and retrain"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "BatchNormalization()\n",
    "model.add( Dense(n_hidden_1, activation='relu', input_shape=(n_input,)) )\n",
    "BatchNormalization()\n",
    "model.add( Dense(n_hidden_2, activation='relu') )\n",
    "BatchNormalization()\n",
    "model.add( Dense(n_hidden_3, activation='relu') )\n",
    "BatchNormalization()\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "# callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "#              ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = lossFunction_SA(opts), optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "#                     validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "#                     validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1, callbacks=callbacks)\n",
    "\n",
    "# history = model.fit_generator(opts.next_batch(opts.cvData.shape[0], batch_size),  \n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    print('EPOCHS*****************')\n",
    "    for x,y,ids in dnn_batch_id_gen(opts, batch_size, 'Train'):\n",
    "        opts.batch_ids = ids\n",
    "#         training_loss = model.train_on_batch(x, y)\n",
    "        history = model.fit(x, y, batch_size, callbacks=None, shuffle=False)\n",
    "        break\n",
    "        \n",
    "    for x,y,ids in dnn_batch_id_gen(opts, batch_size, 'Dev'):\n",
    "        opts.batch_ids = ids\n",
    "        scores = model.test_on_batch(x, y)\n",
    "        print(scores)\n",
    "        break\n",
    "\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "main_input = Input(shape=(n_input,), name='features')\n",
    "\n",
    "# n = K.variable(np.empty((321,batch_size)))\n",
    "# mix_spec_r = Input(tensor=n, name='mix_spec_r')\n",
    "\n",
    "# n1 = K.variable(np.empty((321,batch_size)))\n",
    "# mix_spec_i = Input(tensor=n1, name='mix_spec_i')\n",
    "\n",
    "mix_spec_r = Input(shape=(321,), name='mix_spec_r')\n",
    "mix_spec_i = Input(shape=(321,), name='mix_spec_i')\n",
    "\n",
    "bn1 = BatchNormalization()(main_input)\n",
    "x1 = Dense(n_hidden_1, activation='relu')(bn1)\n",
    "\n",
    "bn2 = BatchNormalization()(x1)\n",
    "x2 = Dense(n_hidden_2, activation='relu')(bn2)\n",
    "\n",
    "bn3 = BatchNormalization()(x2)\n",
    "x3 = Dense(n_hidden_3, activation='relu')(bn3)\n",
    "\n",
    "bn4 = BatchNormalization()(x3)\n",
    "main_output = Dense(n_classes, activation='linear')(bn4)\n",
    "\n",
    "model = Model(inputs=[main_input, mix_spec_r, mix_spec_i], outputs=main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mix_stft> Tensor(\"loss/dense_4_loss/add:0\", shape=(321, ?), dtype=complex64)\n",
      "<complex_irmmask> Tensor(\"loss/dense_4_loss/add_1:0\", shape=(?, ?), dtype=complex64)\n",
      "<28.complex_irmmask*mix_stft> Tensor(\"loss/dense_4_loss/mul_2:0\", shape=(321, ?), dtype=complex64)\n",
      "<numFrames> Tensor(\"loss/dense_4_loss/strided_slice_6:0\", shape=(), dtype=int32)\n",
      "<numFreqs> 321\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/zeros:0\", shape=(321, ?), dtype=float32)\n",
      "1.<frame_inds> Tensor(\"loss/dense_4_loss/map/while/arange:0\", shape=(?,), dtype=int32)\n",
      "2.<frame_inds> Tensor(\"loss/dense_4_loss/map/while/clip_by_value:0\", shape=(?,), dtype=int32)\n",
      "<[[frameNum]]> Tensor(\"loss/dense_4_loss/map_1/while/ExpandDims_1:0\", shape=(1, 1), dtype=int32)\n",
      "1.<uw_mag> Tensor(\"loss/dense_4_loss/map_1/while/GatherNd:0\", shape=(1, 3, ?), dtype=float32)\n",
      "2.<uw_mag> Tensor(\"loss/dense_4_loss/map_1/while/Mean:0\", shape=(1, ?), dtype=float32)\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/stack:0\", shape=(?, 1, 1, ?), dtype=float32)\n",
      "<numFrames> Tensor(\"loss/dense_4_loss/strided_slice_7:0\", shape=(), dtype=int32)\n",
      "<numFreqs> 321\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/zeros_3:0\", shape=(321, ?), dtype=float32)\n",
      "1.<frame_inds> Tensor(\"loss/dense_4_loss/map_2/while/arange:0\", shape=(?,), dtype=int32)\n",
      "2.<frame_inds> Tensor(\"loss/dense_4_loss/map_2/while/clip_by_value:0\", shape=(?,), dtype=int32)\n",
      "<[[frameNum]]> Tensor(\"loss/dense_4_loss/map_3/while/ExpandDims_1:0\", shape=(1, 1), dtype=int32)\n",
      "1.<uw_mag> Tensor(\"loss/dense_4_loss/map_3/while/GatherNd:0\", shape=(1, 3, ?), dtype=float32)\n",
      "2.<uw_mag> Tensor(\"loss/dense_4_loss/map_3/while/Mean:0\", shape=(1, ?), dtype=float32)\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/stack_1:0\", shape=(?, 1, 1, ?), dtype=float32)\n",
      "<real_output_unwrap> (?, 1, 1, ?)\n",
      "<imag_output_unwrap> (?, 1, 1, ?)\n",
      "<numFrames> Tensor(\"loss/dense_4_loss/strided_slice_8:0\", shape=(), dtype=int32)\n",
      "<numFreqs> 321\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/zeros_6:0\", shape=(321, ?), dtype=float32)\n",
      "1.<frame_inds> Tensor(\"loss/dense_4_loss/map_4/while/arange:0\", shape=(?,), dtype=int32)\n",
      "2.<frame_inds> Tensor(\"loss/dense_4_loss/map_4/while/clip_by_value:0\", shape=(?,), dtype=int32)\n",
      "<[[frameNum]]> Tensor(\"loss/dense_4_loss/map_5/while/ExpandDims_1:0\", shape=(1, 1), dtype=int32)\n",
      "1.<uw_mag> Tensor(\"loss/dense_4_loss/map_5/while/GatherNd:0\", shape=(1, 3, ?), dtype=float32)\n",
      "2.<uw_mag> Tensor(\"loss/dense_4_loss/map_5/while/Mean:0\", shape=(1, ?), dtype=float32)\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/stack_2:0\", shape=(?, 1, 1, ?), dtype=float32)\n",
      "<numFrames> Tensor(\"loss/dense_4_loss/strided_slice_9:0\", shape=(), dtype=int32)\n",
      "<numFreqs> 321\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/zeros_9:0\", shape=(321, ?), dtype=float32)\n",
      "1.<frame_inds> Tensor(\"loss/dense_4_loss/map_6/while/arange:0\", shape=(?,), dtype=int32)\n",
      "2.<frame_inds> Tensor(\"loss/dense_4_loss/map_6/while/clip_by_value:0\", shape=(?,), dtype=int32)\n",
      "<[[frameNum]]> Tensor(\"loss/dense_4_loss/map_7/while/ExpandDims_1:0\", shape=(1, 1), dtype=int32)\n",
      "1.<uw_mag> Tensor(\"loss/dense_4_loss/map_7/while/GatherNd:0\", shape=(1, 3, ?), dtype=float32)\n",
      "2.<uw_mag> Tensor(\"loss/dense_4_loss/map_7/while/Mean:0\", shape=(1, ?), dtype=float32)\n",
      "<unwrap_avgmag> Tensor(\"loss/dense_4_loss/stack_3:0\", shape=(?, 1, 1, ?), dtype=float32)\n",
      "<clean_real_unwrap> (?, 1, 1, ?)\n",
      "<clean_imag_unwrap> (?, 1, 1, ?)\n",
      "<complex_irmmask> Tensor(\"loss/dense_4_loss/add_2:0\", shape=(?, 1, 1, ?), dtype=complex64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got <tf.Tensor 'loss/dense_4_loss/sub:0' shape=(?, 1, 321, ?) dtype=complex64>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-0b4f9a8c0ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#               optimizer = 'adam', metrics = ['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.compile(loss = lossFunction_SA, \n\u001b[0;32m----> 4\u001b[0;31m               optimizer = 'adam', metrics = ['accuracy'])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 342\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-2caa02ff0131>\u001b[0m in \u001b[0;36mlossFunction_SA\u001b[0;34m(yTrue, yPred)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mcost_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mcost_imag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msquare\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m-> 1470\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msquare\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m    340\u001b[0m           indices=x.indices, values=x_square, dense_shape=x.dense_shape)\n\u001b[1;32m    341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msquare\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   8195\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8196\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 8197\u001b[0;31m         \"Square\", x=x, name=name)\n\u001b[0m\u001b[1;32m   8198\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8199\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m               raise TypeError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    540\u001b[0m     raise TypeError(\n\u001b[1;32m    541\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <tf.Tensor 'loss/dense_4_loss/sub:0' shape=(?, 1, 321, ?) dtype=complex64>"
     ]
    }
   ],
   "source": [
    "# model.compile(loss = lossFunction_SA(mix_spec_r,mix_spec_i), \n",
    "#               optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.compile(loss = lossFunction_SA, \n",
    "              optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "print(model.metrics_names)\n",
    "\n",
    "params = {'opts': opts,\n",
    "          'input_list': ['features','mix_spec_r','mix_spec_i'],\n",
    "          'batch_size': batch_size, \n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DNN_DataGenerator_SA('Train', **params)\n",
    "validation_generator = DNN_DataGenerator_SA('Dev', **params)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   424/390384 [..............................] - ETA: 6:34:24 - loss: 32.2653 - acc: 5.8962e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-35:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-36:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-33:\n",
      "Process ForkPoolWorker-28:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f8f5c682532c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_MODEL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-50-1fae5f3b22ff>\", line 108, in __getitem__\n",
      "    x, y = self.__data_generation(list_IDs_temp)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-50-1fae5f3b22ff>\", line 126, in __data_generation\n",
      "    y = self.opts.trLabel[ list_IDs_temp ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/knayem/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6, \n",
    "                    callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_per_epochs = np.zeros(epochs)\n",
    "\n",
    "scores = []\n",
    "# scores.append(score)\n",
    "scores_per_epochs[epochs]=np.average(scores)\n",
    "\n",
    "if EarlyStoping(scores_per_epochs):\n",
    "    return\n",
    "\n",
    "    \n",
    "    \n",
    "def EarlyStoping(scores_per_epochs, patience=5, min_delta=1e-6):\n",
    "    \n",
    "    if epochs>=patience:\n",
    "        diff = np.diff(scores_per_epochs[-patience:])\n",
    "        abs_diff = np.abs(diff)\n",
    "        EarlyStop = np.all(abs_diff <= min_delta)\n",
    "        \n",
    "        return EarlyStop\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Custom loss function SA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def customLoss_train_SA(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.trNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_r = K.variable(value=0)\n",
    "    cost_i = K.variable(value=0)\n",
    "    \n",
    "    for e, n,ids in enumerate(zip(numframes,opts.batch_ids)):\n",
    "        real_output = yPred[e, :n, :(c//2)].reshape(n,(c//2))\n",
    "        imag_output = yPred[e, :n, (c//2):].reshape(n,(c//2))\n",
    "        \n",
    "        clean_real = yTrue[e, :n, :(c//2)].reshape(n,(c//2))\n",
    "        clean_imag = yTrue[e, :n, (c//2):].reshape(n,(c//2))\n",
    "        \n",
    "        \n",
    "        mix_stft = opts.mix_spec[e, :n, :321].T + 1j*opts.mix_spec[e, :n, 321:].T\n",
    "        mix_stft = mix_stft.reshape(321,n)\n",
    "        \n",
    "        \n",
    "        complex_irmmask = real_output.T + 1j*imag_output.T # estimate mask\n",
    "        \n",
    "        if opts.labwin==0:\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        else:\n",
    "            real_output_unwrap,_ = unwrapAugmentedTF_wAvg(real_output.T, opts.labwin)\n",
    "            imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(imag_output.T, opts.labwin)\n",
    "            print(real_output_unwrap.shape)\n",
    "            \n",
    "            clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(clean_real.T, opts.labwin)\n",
    "            clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(clean_imag.T, opts.labwin)\n",
    "            print(clean_real_unwrap.shape)\n",
    "            \n",
    "            complex_irmmask    = real_output_unwrap + 1j*imag_output_unwrap\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "            \n",
    "            true_irmmask       = clean_real_unwrap + 1j*clean_imag_unwrap\n",
    "            y                  = true_irmmask*mix_stft\n",
    "        \n",
    "        diff = y-estimate\n",
    "        cost_r += K.sum(K.square(np.real(diff)))\n",
    "        cost_i += K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "    \n",
    "    cost = 0.5*(cost_r+cost_i)/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss_val_SA(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.cvNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_r = K.variable(value=0)\n",
    "    cost_i = K.variable(value=0)\n",
    "    \n",
    "    for e, (n,ids) in enumerate(zip(numframes,opts.batch_ids)):\n",
    "        real_output = yPred[e, :n, :(c//2)]\n",
    "        real_output = K.reshape(real_output, (n,c//2))\n",
    "        \n",
    "        imag_output = yPred[e, :n, (c//2):]\n",
    "        imag_output = K.reshape(imag_output, (n,c//2))\n",
    "        \n",
    "        clean_real = yTrue[e, :n, :(c//2)]\n",
    "        clean_real = K.reshape(clean_real, (n,c//2))\n",
    "        \n",
    "        clean_imag = yTrue[e, :n, (c//2):]\n",
    "        clean_imag = K.reshape(clean_imag, (n,c//2))\n",
    "        \n",
    "        \n",
    "        mix_stft = K.cast(K.transpose(opts.mix_spec[e, :n, :321]),dtype='complex64') + 1j*K.cast(K.transpose(opts.mix_spec[e, :n, 321:]),dtype='complex64')\n",
    "        mix_stft = K.reshape(mix_stft, (321,n))\n",
    "        \n",
    "        \n",
    "        complex_irmmask = K.cast(K.transpose(real_output),dtype='complex64') + 1j*K.cast(K.transpose(imag_output),dtype='complex64') # estimate mask\n",
    "        \n",
    "        if opts.labwin==0:\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        else:\n",
    "            real_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(real_output), opts.labwin)\n",
    "            imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(imag_output), opts.labwin)\n",
    "            print(real_output_unwrap.shape)\n",
    "            \n",
    "            clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(clean_real), opts.labwin)\n",
    "            clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(clean_imag), opts.labwin)\n",
    "            print(clean_real_unwrap.shape)\n",
    "            \n",
    "            complex_irmmask    = K.cast(K.transpose(real_output_unwrap),dtype='complex64') + 1j*K.cast(K.transpose(imag_output_unwrap),dtype='complex64')\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "            \n",
    "            true_irmmask       = K.cast(K.transpose(clean_real_unwrap),dtype='complex64') + 1j*K.cast(K.transpose(clean_imag_unwrap),dtype='complex64')\n",
    "            y                  = true_irmmask*mix_stft\n",
    "        \n",
    "        diff = y-estimate\n",
    "        cost_r += K.sum(K.square(np.real(diff)))\n",
    "        cost_i += K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "    \n",
    "    cost = 0.5*(cost_r+cost_i)/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss_test_SA(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.teNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_r = K.variable(value=0)\n",
    "    cost_i = K.variable(value=0)\n",
    "    \n",
    "    for e, (n,ids) in enumerate(zip(numframes,opts.batch_ids)):\n",
    "        real_output = yPred[e, :n, :(c//2)]\n",
    "        real_output = K.reshape(real_output, (n,c//2))\n",
    "        \n",
    "        imag_output = yPred[e, :n, (c//2):].reshape(n,(c//2))\n",
    "        imag_output = K.reshape(imag_output, (n,c//2))\n",
    "        \n",
    "        clean_real = yTrue[e, :n, :(c//2)].reshape(n,(c//2))\n",
    "        clean_real = K.reshape(clean_real, (n,c//2))\n",
    "        \n",
    "        clean_imag = yTrue[e, :n, (c//2):].reshape(n,(c//2))\n",
    "        clean_imag = K.reshape(clean_imag, (n,c//2))\n",
    "        \n",
    "        \n",
    "        mix_stft = opts.mix_spec[e, :n, :321].T + 1j*opts.mix_spec[e, :n, 321:].T\n",
    "        mix_stft = mix_stft.reshape(321,n)\n",
    "        \n",
    "        \n",
    "        complex_irmmask = real_output.T + 1j*imag_output.T # estimate mask\n",
    "        \n",
    "        if opts.labwin==0:\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        else:\n",
    "            real_output_unwrap,_ = unwrapAugmentedTF_wAvg(real_output.T, opts.labwin)\n",
    "            imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(imag_output.T, opts.labwin)\n",
    "            print(real_output_unwrap.shape)\n",
    "            \n",
    "            clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(clean_real.T, opts.labwin)\n",
    "            clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(clean_imag.T, opts.labwin)\n",
    "            print(clean_real_unwrap.shape)\n",
    "            \n",
    "            complex_irmmask    = real_output_unwrap + 1j*imag_output_unwrap\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "            \n",
    "            true_irmmask       = clean_real_unwrap + 1j*clean_imag_unwrap\n",
    "            y                  = true_irmmask*mix_stft\n",
    "        \n",
    "        diff = y-estimate\n",
    "        cost_r += K.sum(K.square(np.real(diff)))\n",
    "        cost_i += K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "    \n",
    "    cost = 0.5*(cost_r+cost_i)/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossFunctionCallback_SA(Callback):\n",
    "    def __init__(self, model, opts):\n",
    "        self.model = model\n",
    "        self.opts = opts\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.model.loss = customLoss_train_SA\n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.model.loss = customLoss_val_SA\n",
    "        pass\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        # print('-> on_train_end=',self.params)\n",
    "        self.model.loss = customLoss_test_SA\n",
    "        pass\n",
    "        \n",
    " \n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss_train(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r = K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.trNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_mag = K.variable(value=0)\n",
    "    for e, n in enumerate(numframes):\n",
    "        cost_mag += K.sum(K.square(yTrue[e,:n]- yPred[e,:n]))\n",
    "\n",
    "    cost = cost_mag/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss_val(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.cvNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_mag = K.variable(value=0)\n",
    "    for e,n in enumerate(numframes):\n",
    "        cost_mag += K.sum(K.square(yTrue[e,:n]- yPred[e,:n]))\n",
    "\n",
    "    cost = cost_mag/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss_test(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.teNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_mag = K.variable(value=0)\n",
    "    for e,n in enumerate(numframes):\n",
    "        cost_mag += K.sum(K.square(yTrue[e,:n]- yPred[e,:n]))\n",
    "\n",
    "    cost = cost_mag/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossFunctionCallback(Callback):\n",
    "    def __init__(self, model, opts):\n",
    "        self.model = model\n",
    "        self.opts = opts\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.model.loss = customLoss_train\n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.model.loss = customLoss_val\n",
    "        pass\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        # print('-> on_train_end=',self.params)\n",
    "        self.model.loss = customLoss_test\n",
    "        pass\n",
    "        \n",
    " \n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RNN variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Max_RNN = 256\n",
    "\n",
    "# feat_vec_len = 1230\n",
    "# out_vec_len = 963\n",
    "\n",
    "# epochs = 50\n",
    "# train_size = 15000\n",
    "# dev_size = 3300\n",
    "# batch_size = 256\n",
    "\n",
    "\n",
    "batch_size = 25 #opts.sgd_batch_size\n",
    "epochs = 50\n",
    "\n",
    "Max_RNN = 256\n",
    "Max_Frame = 185\n",
    "\n",
    "'''\n",
    "n_train, n_train_dim = <number of samples for training, input dimentions>, final will be (1951920, 1230)\n",
    "n_dev, n_dev_dim = <number of samples for development, input dimentions>, final will be (449610, 1230)\n",
    "n_test, n_test_dim =<number of samples for testing, input dimentions>, final will be (72440, 1230)\n",
    "\n",
    "'''\n",
    "n_train, n_train_dim = (1951920, 1230) # opts.trData.shape = (1951920, 1230)\n",
    "n_dev, n_dev_dim = (449610, 1230) # opts.cvData.shape =(449610, 1230)\n",
    "n_test, n_test_dim = (72440, 1230) # opts.teData.shape =(72440, 1230)\n",
    "\n",
    "n_train_files = 15000 #15000\n",
    "n_dev_files = 3300 # 3300\n",
    "n_test_files = 545\n",
    "\n",
    "n_classes = (963 + 963)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Mag weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRETRAIN_VERSION = \"_e14v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST_TRAINMODEL_FILE = \"./dnn_models/lstm_weights\"+ PRETRAIN_VERSION+\"_05.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(BEST_TRAINMODEL_FILE, custom_objects={'customLoss_train':customLoss_train,\n",
    "                                                          'customLoss_val':customLoss_val,\n",
    "                                                          'customLoss_test':customLoss_test,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_26 (Bidirectio (None, 185, 512)          3045376   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 185, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 185, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 185, 1926)         988038    \n",
      "=================================================================\n",
      "Total params: 5,608,326\n",
      "Trainable params: 5,608,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "600/600 [==============================] - 931s - loss: 0.0135 - acc: 0.0387 - val_loss: 0.0125 - val_acc: 0.1571\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 919s - loss: 0.0121 - acc: 0.1205 - val_loss: 0.0121 - val_acc: 0.0734\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 938s - loss: 0.0118 - acc: 0.0618 - val_loss: 0.0120 - val_acc: 0.0885\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 974s - loss: 0.0117 - acc: 0.0569 - val_loss: 0.0120 - val_acc: 0.0516\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 984s - loss: 0.0117 - acc: 0.0458 - val_loss: 0.0119 - val_acc: 0.0830\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 965s - loss: 0.0116 - acc: 0.0450 - val_loss: 0.0119 - val_acc: 0.0536\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 929s - loss: 0.0116 - acc: 0.0446 - val_loss: 0.0119 - val_acc: 0.0545\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.pop()\n",
    "model.add(TimeDistributed(Dense(units=n_classes, activation='linear')))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-4, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True),\n",
    "             LossFunctionCallback_SA(model,opts)]\n",
    "\n",
    "adam = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='train'), \n",
    "                    validation_data=next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train_files//batch_size, \n",
    "                    validation_steps=n_dev_files//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 1\n",
    "single bidirectional GRU layer\n",
    "\n",
    "real+img (963+963)=1926-d output vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "model = Sequential()\n",
    "\n",
    "BatchNormalization()\n",
    "model.add(LSTM(Max_RNN, return_sequences=True, input_shape=(Max_Frame,opts.dim_input)))\n",
    "BatchNormalization()\n",
    "model.add(LSTM(Max_RNN, return_sequences=True))\n",
    "# model.add(Bidirectional(LSTM(Max_RNN, return_sequences=True), input_shape=(Max_RNN,feat_vec_len)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Bidirectional(GRU(Max_RNN, return_sequences=True, stateful=True)))\n",
    "BatchNormalization()\n",
    "model.add(TimeDistributed(Dense(units=n_classes, activation='linear')))\n",
    "#model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "# callbacks = [EarlyStopping(monitor='val_acc', patience=2, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "#              ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-4, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True),\n",
    "             LossFunctionCallback(model,opts)]\n",
    "\n",
    "\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=0.00001, rho=0.9, epsilon=1e-08)\n",
    "adam = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# model.compile(loss = \"mse\", optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = rmsprop, metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='train'), \n",
    "                    validation_data=next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train_files//batch_size, \n",
    "                    validation_steps=n_dev_files//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# plot metrics                                                       \n",
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle(\"Train result\")\n",
    "\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "# plt.plot(history.history['val_mean_absolute_percentage_error'])\n",
    "# plt.plot(history.history['val_cosine_proximity'])\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['acc'])\n",
    "\n",
    "# plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "# plt.plot(history.history['mean_absolute_percentage_error'])\n",
    "# plt.plot(history.history['cosine_proximity'])\n",
    "                                                           \n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"numbers\")\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for x,y in next_batch_rnn(opts,batch_size,maxlen=Max_RNN, CYCLE='test'):\n",
    "    print('OUTSIDE:',x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST_MODEL_FILE = \"./dnn_models/lstm_weights\"+ Code_VERSION+\"_00.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-e513b7962f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                           \u001b[0;34m'customLoss_val_SA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcustomLoss_val_SA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                           \u001b[0;34m'customLoss_test_SA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcustomLoss_test_SA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                            'opts':opts,})\n\u001b[0m",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects)\u001b[0m\n\u001b[1;32m    266\u001b[0m                   \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                   \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                   sample_weight_mode=sample_weight_mode)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# Set optimizer weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                            \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                            \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mloss_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weights_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 910\u001b[0;31m                                         sample_weight, mask)\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/knayem/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \"\"\"\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-3826e4c6aafa>\u001b[0m in \u001b[0;36mcustomLoss_val_SA\u001b[0;34m(yTrue, yPred)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mreal_output_unwrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrapAugmentedTF_wAvg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mimag_output_unwrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrapAugmentedTF_wAvg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimag_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output_unwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-54d4e4df18c8>\u001b[0m in \u001b[0;36munwrapAugmentedTF_wAvg\u001b[0;34m(impt_mag, num_per_side, unwrap_mag)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnumFreqs\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mnumWrapFreqs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0msliding_window_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnumFreqs\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumFreqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0munwrap_avgmag\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumFreqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumFrames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_per_side\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(BEST_MODEL_FILE, custom_objects={'customLoss_train_SA':customLoss_train_SA,\n",
    "                                                          'customLoss_val_SA':customLoss_val_SA,\n",
    "                                                          'customLoss_test_SA':customLoss_test_SA,\n",
    "                                                           'opts':opts,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/545 [==============================] - 5s\n",
      "y_hat shape (545, 185, 1926)\n"
     ]
    }
   ],
   "source": [
    "for x,y in next_batch_rnn(opts,batch_size,maxlen=Max_Frame, CYCLE='test'):\n",
    "    y_hat = model.predict(x, batch_size=x.shape[0], verbose=1)\n",
    "    print('y_hat shape', y_hat.shape)\n",
    "    sio.savemat(OUTPUT_FILE, {'y_hat':y_hat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545, 185, 1926)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
