{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM Series (tf)\n",
    "\n",
    "** - 1 LSTM  **\n",
    "\n",
    "** - Train and Dev mse functions **\n",
    "\n",
    "** - Adam Optimizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.11</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read parameters from .mat files\n",
    "save in Opt object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_VERSION = \"_e10v5\"\n",
    "Test_Data_VERSION = '_e04v2_nf'\n",
    "Spectrogram_VERSION = '_e04v2_nf'\n",
    "\n",
    "Code_VERSION = \"_tf_e01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNN_DATA_FILE = \"./dnn_models/DNN_datas\"+ Data_VERSION+\".mat\"\n",
    "DNN_TEST_FILE = \"./dnn_models/Test_datas\"+Test_Data_VERSION+\".mat\"\n",
    "\n",
    "DNN_PARAMS_FILE = \"./dnn_models/DNN_params\"+Data_VERSION+\".mat\"\n",
    "\n",
    "SPECTROGRAM_FILE = './dnn_models/results'+Spectrogram_VERSION+'.mat'\n",
    "\n",
    "\n",
    "# Best model, after adding 1st dense layer\n",
    "MODEL_FILE = \"./dnn_models/lstm_weights\"+ Code_VERSION+\"_{epoch:02d}.h5\"\n",
    "\n",
    "#dummy save file\n",
    "SAVE_MODEL_FILE = \"./dnn_models/lstm_py_model\"+ Code_VERSION+\".h5\"\n",
    "\n",
    "# estimated real+imag for test dataset\n",
    "OUTPUT_FILE = \"./dnn_models/Real_Imag\"+Code_VERSION+\".mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files saved by Matlab reading class. Data, Parameters are read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    opts_dict = dict()\n",
    "\n",
    "    def __init__(self, FILE_PARA, FILE_DATA, FILE_TEST, FILE_SPEC):\n",
    "        \n",
    "        # Basic parameters\n",
    "        with h5py.File(FILE_PARA, 'r') as f:\n",
    "            key_list = list(f.keys())\n",
    "            print('Opt key:',key_list)\n",
    "\n",
    "            for k, v in f['opts'].items():\n",
    "\n",
    "                print('key:', k)\n",
    "\n",
    "                if k == 'ARMA_order':\n",
    "                    self.ARMA_order = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.ARMA_order\n",
    "                elif k == 'ada_grad_eps':\n",
    "                    self.ada_grad_eps = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.ada_grad_eps\n",
    "                elif k == 'ada_sgd_scale':\n",
    "                    self.ada_sgd_scale = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.ada_sgd_scale\n",
    "                elif k == 'change_momentum_point':\n",
    "                    self.change_momentum_point = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.change_momentum_point\n",
    "                elif k == 'cost_function':\n",
    "                    self.cost_function = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.cost_function += chr(c[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.cost_function\n",
    "\n",
    "                elif k == 'cv_interval':\n",
    "                    self.cv_interval = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.cv_interval\n",
    "                elif k == 'dim_input':\n",
    "                    self.dim_input = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.dim_input\n",
    "                    print (k, self.dim_input)\n",
    "                elif k == 'dim_output':\n",
    "                    self.dim_output = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.dim_output\n",
    "                    print (k, self.dim_output)\n",
    "                elif k == 'drop_ratio':\n",
    "                    self.drop_ratio = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.drop_ratio\n",
    "                elif k == 'eval_on_gpu':\n",
    "                    self.eval_on_gpu = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.eval_on_gpu\n",
    "                elif k == 'final_momentum':\n",
    "                    self.final_momentum = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.final_momentum\n",
    "                elif k == 'hid_struct':\n",
    "                    self.hid_struct = np.array(v)\n",
    "                    self.opts_dict[k] = self.hid_struct\n",
    "                elif k == 'initial_momentum':\n",
    "                    self.initial_momentum = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.initial_momentum\n",
    "                elif k == 'isDropout':\n",
    "                    self.isDropout = 0\n",
    "                    self.opts_dict[k] = self.isDropout\n",
    "                elif k == 'isDropoutInput':\n",
    "                    self.isDropoutInput = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isDropoutInput\n",
    "                elif k == 'isGPU':\n",
    "                    self.isGPU = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isGPU\n",
    "                elif k == 'isNormalize':\n",
    "                    self.isNormalize = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isNormalize\n",
    "                elif k == 'isPretrain':\n",
    "                    self.isPretrain = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isPretrain\n",
    "                elif k == 'learner':\n",
    "                    self.learner = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.learner += chr(c[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.learner\n",
    "\n",
    "                elif k == 'net_struct':\n",
    "                    self.net_struct = np.array(v)\n",
    "                    for n_s in np.array(v):\n",
    "                        print('Opts Net Stuct:',n_s[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.net_struct\n",
    "                elif k == 'rbm_batch_size':\n",
    "                    self.rbm_batch_size = int(np.array(v)[0][0])\n",
    "                    print(\"self.rbm_batch_size:\",self.rbm_batch_size)\n",
    "                    # self.opts_dict[k] = self.rbm_batch_size\n",
    "                elif k == 'rbm_learn_rate_binary':\n",
    "                    self.rbm_learn_rate_binary = np.array(v)\n",
    "                    # self.opts_dict[k] = self.rbm_learn_rate_binary\n",
    "                elif k == 'rbm_learn_rate_real':\n",
    "                    self.rbm_learn_rate_real = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_learn_rate_real\n",
    "                elif k == 'rbm_max_epoch':\n",
    "                    self.rbm_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_max_epoch\n",
    "                elif k == 'save_on_fly':\n",
    "                    self.save_on_fly = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.save_on_fly\n",
    "                elif k == 'sgd_batch_size':\n",
    "                    self.sgd_batch_size = int(np.array(v)[0][0])\n",
    "                    print(\"self.sgd_batch_size:\",self.sgd_batch_size)\n",
    "#                     self.sgd_batch_size = 1024//2 # BATCH_SIZE for training net\n",
    "                    # self.opts_dict[k] = self.sgd_batch_size\n",
    "                elif k == 'sgd_learn_rate':\n",
    "                    self.sgd_learn_rate = np.array(v)\n",
    "                    # self.opts_dict[k] = self.sgd_learn_rate\n",
    "                elif k == 'sgd_max_epoch':\n",
    "                    self.sgd_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.sgd_max_epoch\n",
    "                elif k == 'split_tanh1_c1':\n",
    "                    self.split_tanh1_c1 = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.split_tanh1_c1\n",
    "                elif k == 'split_tanh1_c2':\n",
    "                    self.split_tanh1_c2 = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.split_tanh1_c2\n",
    "                elif k == 'unit_type_hidden':\n",
    "                    self.unit_type_hidden = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_hidden += chr(c[0])\n",
    "\n",
    "                elif k == 'unit_type_output':\n",
    "                    self.unit_type_output = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_output += chr(c[0])\n",
    "\n",
    "        # Training and Dev Data \n",
    "        with h5py.File(FILE_DATA, 'r') as f:\n",
    "            \n",
    "            print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_DATA,len(f.keys())) )\n",
    "\n",
    "            for k, v in f.items():\n",
    "                print(k, end=', ')\n",
    "                \n",
    "                # Train Data\n",
    "                if k == 'trData':\n",
    "                    self.trData = np.transpose(np.array(v))\n",
    "                    print(\"trData.shape: \", self.trData.shape)\n",
    "                    print(\"trData-> mean:\", np.mean(self.trData), \", var:\", np.var(self.trData), \", std:\",\n",
    "                          np.std(self.trData), \", range:\", (np.amin(self.trData),np.amax(self.trData)))\n",
    "                elif k == 'trLabel_i':\n",
    "                    self.trLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_i.shape: \", self.trLabel_i.shape)\n",
    "                    print(\"trLabel_i-> mean:\", np.mean(self.trLabel_i), \", var:\", np.var(self.trLabel_i), \", std:\",\n",
    "                          np.std(self.trLabel_i), \", range:\", (np.amin(self.trLabel_i),np.amax(self.trLabel_i)))\n",
    "                elif k == 'trLabel_r':\n",
    "                    self.trLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_r.shape: \", self.trLabel_r.shape)\n",
    "                    print(\"trLabel_r-> mean:\", np.mean(self.trLabel_r), \", var:\", np.var(self.trLabel_r), \", std:\",\n",
    "                          np.std(self.trLabel_r), \", range:\", (np.amin(self.trLabel_r),np.amax(self.trLabel_r)))\n",
    "                elif k == 'trNumframes':\n",
    "                    print(\"trNumframes.shape: \", v.shape)\n",
    "                    self.trNumframes = np.transpose(np.array(v))\n",
    "                    \n",
    "                # Dev Data\n",
    "                elif k == 'cvData':\n",
    "                    self.cvData = np.transpose(np.array(v))\n",
    "                    print(\"cvData.shape: \", self.cvData.shape)\n",
    "                    print(\"cvData-> mean:\", np.mean(self.cvData), \", var:\", np.var(self.cvData), \", std:\",\n",
    "                          np.std(self.cvData), \", range:\", (np.amin(self.cvData),np.amax(self.cvData)))\n",
    "                elif k == 'cvLabel_i':\n",
    "                    self.cvLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_i.shape: \", self.cvLabel_i.shape)\n",
    "                    print(\"cvLabel_i-> mean:\", np.mean(self.cvLabel_i), \", var:\", np.var(self.cvLabel_i), \", std:\",\n",
    "                          np.std(self.cvLabel_i), \", range:\", (np.amin(self.cvLabel_i),np.amax(self.cvLabel_i)))\n",
    "                elif k == 'cvLabel_r':\n",
    "                    self.cvLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_r.shape: \", self.cvLabel_r.shape)\n",
    "                    print(\"cvLabel_r-> mean:\", np.mean(self.cvLabel_r), \", var:\", np.var(self.cvLabel_r), \", std:\",\n",
    "                          np.std(self.cvLabel_r), \", range:\", (np.amin(self.cvLabel_r),np.amax(self.cvLabel_r)))\n",
    "                elif k == 'cvNumframes':\n",
    "                    print(\"cvNumframes.shape: \", v.shape)\n",
    "                    self.cvNumframes = np.transpose(np.array(v))\n",
    "\n",
    "            self.trLabel = np.concatenate((self.trLabel_r, self.trLabel_i), axis=1)\n",
    "            self.cvLabel = np.concatenate((self.cvLabel_r, self.cvLabel_i), axis=1)\n",
    "            \n",
    "        \n",
    "        # Test Data \n",
    "        print(FILE_TEST, os.path.isfile(FILE_TEST))\n",
    "        \n",
    "        with h5py.File(FILE_TEST, 'r') as f:\n",
    "            print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_TEST,len(f.keys())) )\n",
    "\n",
    "            for k, v in f.items():\n",
    "                print(k, end=', ')\n",
    "                \n",
    "                #Test Data\n",
    "                if k == 'teData':\n",
    "                    self.teData = np.transpose(np.array(v))\n",
    "                    print(\"teData.shape: \", self.teData.shape)\n",
    "                    print(\"teData-> mean:\", np.mean(self.teData), \", var:\", np.var(self.teData), \", std:\",\n",
    "                          np.std(self.teData), \", range:\", (np.amin(self.teData),np.amax(self.teData)))\n",
    "                    \n",
    "                elif k == 'teLabel_i':\n",
    "                    self.teLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"teLabel_i.shape: \", self.teLabel_i.shape)\n",
    "                    print(\"teLabel_i-> mean:\", np.mean(self.teLabel_i), \", var:\", np.var(self.teLabel_i), \", std:\",\n",
    "                          np.std(self.teLabel_i), \", range:\", (np.amin(self.teLabel_i),np.amax(self.teLabel_i)))\n",
    "                    \n",
    "                elif k == 'teLabel_r':\n",
    "                    self.teLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"teLabel_r.shape: \", self.teLabel_r.shape)\n",
    "                    print(\"teLabel_r-> mean:\", np.mean(self.teLabel_r), \", var:\", np.var(self.teLabel_r), \", std:\",\n",
    "                          np.std(self.teLabel_r), \", range:\", (np.amin(self.teLabel_r),np.amax(self.teLabel_r)))\n",
    "                \n",
    "                elif k == 'teNumframes':\n",
    "                    print(\"teNumframes.shape: \", v.shape)\n",
    "                    self.teNumframes = np.transpose(np.array(v))\n",
    "            \n",
    "            self.teLabel = np.concatenate((self.teLabel_r, self.teLabel_i), axis=1)\n",
    "         \n",
    "        \n",
    "        # Spectrogram Data (Train, Dev, Test) \n",
    "        print(FILE_SPEC, os.path.isfile(FILE_SPEC))\n",
    "        \n",
    "        with h5py.File(FILE_SPEC, 'r') as f:\n",
    "            print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_TEST,len(f.keys())) )\n",
    "\n",
    "            for k, v in f.items():\n",
    "                print(k, end=', ')\n",
    "                \n",
    "                #Test Data\n",
    "                if k == 'teData':\n",
    "                    self.teData = np.transpose(np.array(v))\n",
    "                    print(\"teData.shape: \", self.teData.shape)\n",
    "                    print(\"teData-> mean:\", np.mean(self.teData), \", var:\", np.var(self.teData), \", std:\",\n",
    "                          np.std(self.teData), \", range:\", (np.amin(self.teData),np.amax(self.teData)))\n",
    "                    \n",
    "                elif k == 'teLabel_i':\n",
    "                    self.teLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"teLabel_i.shape: \", self.teLabel_i.shape)\n",
    "                    print(\"teLabel_i-> mean:\", np.mean(self.teLabel_i), \", var:\", np.var(self.teLabel_i), \", std:\",\n",
    "                          np.std(self.teLabel_i), \", range:\", (np.amin(self.teLabel_i),np.amax(self.teLabel_i)))\n",
    "                    \n",
    "                elif k == 'teLabel_r':\n",
    "                    self.teLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"teLabel_r.shape: \", self.teLabel_r.shape)\n",
    "                    print(\"teLabel_r-> mean:\", np.mean(self.teLabel_r), \", var:\", np.var(self.teLabel_r), \", std:\",\n",
    "                          np.std(self.teLabel_r), \", range:\", (np.amin(self.teLabel_r),np.amax(self.teLabel_r)))\n",
    "                \n",
    "                elif k == 'teNumframes':\n",
    "                    print(\"teNumframes.shape: \", v.shape)\n",
    "                    self.teNumframes = np.transpose(np.array(v))\n",
    "            \n",
    "            self.teLabel = np.concatenate((self.teLabel_r, self.teLabel_i), axis=1)\n",
    "\n",
    "    \n",
    "    def prepare3D_list(self, CYCLE):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        feat_vec_len = self.dim_input # 1230\n",
    "        out_vec_len = self.dim_output #963\n",
    "        \n",
    "        if CYCLE.lower()=='train':\n",
    "            data = self.trData\n",
    "            label_r = self.trLabel_r\n",
    "            label_i = self.trLabel_i\n",
    "            numframes = self.trNumframes\n",
    "            \n",
    "        elif CYCLE.lower()=='dev':\n",
    "            data = self.cvData\n",
    "            label_r = self.cvLabel_r\n",
    "            label_i = self.cvLabel_i\n",
    "            numframes = self.cvNumframes\n",
    "            \n",
    "        elif CYCLE.lower()=='test':\n",
    "            data = self.teData\n",
    "            label_r = self.teLabel_r\n",
    "            label_i = self.teLabel_i\n",
    "            numframes = self.teNumframes\n",
    "            \n",
    "\n",
    "        data3D, label3D_r, label3D_i = [], [], []\n",
    "        numframes = np.cumsum(numframes)\n",
    "        \n",
    "        for e, frames in enumerate(numframes):\n",
    "            frames= int(frames)\n",
    "            pre_frames= int(numframes[e-1])\n",
    "            \n",
    "            d = data[:frames] if len(data3D)==0 else data[pre_frames:frames]\n",
    "            r = label_r[:frames] if len(label3D_r)==0 else label_r[pre_frames:frames]\n",
    "            i = label_i[:frames] if len(label3D_i)==0 else label_i[pre_frames:frames]\n",
    "            \n",
    "            data3D.append( d )\n",
    "            label3D_r.append( r )\n",
    "            label3D_i.append( i )\n",
    "\n",
    "        return data3D, label3D_r, label3D_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt key: ['#refs#', 'opts']\n",
      "key: ARMA_order\n",
      "key: ada_grad_eps\n",
      "key: ada_sgd_scale\n",
      "key: change_momentum_point\n",
      "key: cost_function\n",
      "key: cv_interval\n",
      "key: dim_input\n",
      "dim_input 1230\n",
      "key: dim_output\n",
      "dim_output 963\n",
      "key: drop_ratio\n",
      "key: eval_on_gpu\n",
      "key: final_momentum\n",
      "key: hid_struct\n",
      "key: initial_momentum\n",
      "key: isDropout\n",
      "key: isDropoutInput\n",
      "key: isGPU\n",
      "key: isNormalize\n",
      "key: isPretrain\n",
      "key: learner\n",
      "key: net_struct\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "key: rbm_batch_size\n",
      "self.rbm_batch_size: 1024\n",
      "key: rbm_learn_rate_binary\n",
      "key: rbm_learn_rate_real\n",
      "key: rbm_max_epoch\n",
      "key: save_on_fly\n",
      "key: sgd_batch_size\n",
      "self.sgd_batch_size: 1024\n",
      "key: sgd_learn_rate\n",
      "key: sgd_max_epoch\n",
      "key: split_tanh1_c1\n",
      "key: split_tanh1_c2\n",
      "key: tr_mu\n",
      "key: tr_std\n",
      "key: unit_type_hidden\n",
      "key: unit_type_output\n",
      "\n",
      "\n",
      "File name <./dnn_models/DNN_datas_e10v5.mat>\n",
      "Opt h5py keys (Total 8):\n",
      "cvData, cvData.shape:  (449610, 1230)\n",
      "cvData-> mean: -3.12831e-06 , var: 0.607645 , std: 0.779516 , range: (-9.6298428, 10.49406)\n",
      "cvLabel_i, cvLabel_i.shape:  (449610, 963)\n",
      "cvLabel_i-> mean: -1.2493e-05 , var: 0.0129563 , std: 0.113826 , range: (-10.0, 10.0)\n",
      "cvLabel_r, cvLabel_r.shape:  (449610, 963)\n",
      "cvLabel_r-> mean: 0.028573 , var: 0.0212577 , std: 0.1458 , range: (-10.0, 10.0)\n",
      "cvNumframes, cvNumframes.shape:  (1, 3300)\n",
      "trData, trData.shape:  (1951920, 1230)\n",
      "trData-> mean: -0.000807387 , var: 0.612503 , std: 0.782626 , range: (-11.557238, 12.486189)\n",
      "trLabel_i, trLabel_i.shape:  (1951920, 963)\n",
      "trLabel_i-> mean: 1.29036e-06 , var: 0.0133129 , std: 0.115382 , range: (-10.0, 10.0)\n",
      "trLabel_r, trLabel_r.shape:  (1951920, 963)\n",
      "trLabel_r-> mean: 0.0295324 , var: 0.021696 , std: 0.147296 , range: (-10.0, 10.0)\n",
      "trNumframes, trNumframes.shape:  (1, 15000)\n",
      "./dnn_models/Test_datas_e04v2_nf.mat True\n",
      "\n",
      "\n",
      "File name <./dnn_models/Test_datas_e04v2_nf.mat>\n",
      "Opt h5py keys (Total 7):\n",
      "#refs#, #subsystem#, teData, teData.shape:  (72440, 1230)\n",
      "teData-> mean: -0.00110349 , var: 0.556445 , std: 0.745952 , range: (-4.8041673, 5.2418036)\n",
      "teFilename_mix, teLabel_i, teLabel_i.shape:  (72440, 963)\n",
      "teLabel_i-> mean: 0.0322572 , var: 0.002574 , std: 0.0507346 , range: (-0.25647101, 0.77932656)\n",
      "teLabel_r, teLabel_r.shape:  (72440, 963)\n",
      "teLabel_r-> mean: 0.0322567 , var: 0.00257397 , std: 0.0507343 , range: (-0.25709164, 0.77899867)\n",
      "teNumframes, teNumframes.shape:  (1, 545)\n"
     ]
    }
   ],
   "source": [
    "opts = Opts(DNN_PARAMS_FILE, DNN_DATA_FILE, DNN_TEST_FILE, SPECTROGRAM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For RNN style 3D batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(data, numFrames, maxlen=185):\n",
    "    list_tr = []\n",
    "    list_stft = []\n",
    "    list_stft_abs = []\n",
    "    list_length = []\n",
    "    z = ['000', '00', '0', '']\n",
    "    \n",
    "    for i in range(n_file):\n",
    "        if (i == 0):\n",
    "            j = 0\n",
    "        else:\n",
    "            j = int(math.log10(i))\n",
    "        s, sr = librosa.load(path + str_tr + z[j] + str(i) + '.wav', sr = None)\n",
    "        if (flag == 1):\n",
    "            list_tr.append(s)\n",
    "        \n",
    "        #Calculating STFT\n",
    "        stft = librosa.stft(s, n_fft= 1024, hop_length= 512)\n",
    "        \n",
    "        stft_len = stft.shape[1]\n",
    "        \n",
    "        #Appending STFT to list\n",
    "        if (flag == 1):\n",
    "            list_stft.append(stft)\n",
    "        \n",
    "        #Calculating Absolute of STFT\n",
    "        stft_abs = np.abs(stft)\n",
    "        \n",
    "        #Padding zeros to make length 300\n",
    "        stft_abs = np.pad(stft_abs, ((0,0),(0, max_length-stft_len)), 'constant')\n",
    "        \n",
    "        #Appending abs to list\n",
    "        list_stft_abs.append(stft_abs)\n",
    "        \n",
    "        #Appending time-length of STFT to list\n",
    "        list_length.append(stft_len)\n",
    "        \n",
    "    return list_tr, list_stft, list_stft_abs, list_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading all training noisy speech signals\n",
    "trData_padded = padding(opts.trData, opts.trNumframes, maxlen=185)\n",
    "\n",
    "# #Loading all training clean speech signals\n",
    "# trs, S, S_abs, S_len = loadfile(path, 'trs')\n",
    "\n",
    "# #Loading all training noise signals\n",
    "# trn, N, N_abs, N_len = loadfile(path, 'trn')\n",
    "\n",
    "cvData_padded = padding(opts.cvData, opts.cvNumframes, maxlen=185)\n",
    "teData_padded = padding(opts.teData, opts.teNumframes, maxlen=185)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch_rnn(opts, batch_size, CYCLE, maxlen=185):\n",
    "    # TRAIN: total_num_samples = self.trData.shape[0] = 1951920\n",
    "    # DEV: total_num_samples = self.trData.shape[0] = 449610\n",
    "\n",
    "    feat_vec_len = opts.dim_input #1230\n",
    "    out_vec_len = opts.dim_output #963\n",
    "\n",
    "#     while True:\n",
    "    if CYCLE.lower()=='train':\n",
    "        selected_indics = np.arange(len(opts.trNumframes))\n",
    "        data3D, label3D_r, label3D_i = opts.prepare3D_list(CYCLE)\n",
    "        np.random.shuffle(selected_indics)\n",
    "\n",
    "    elif CYCLE.lower()=='dev':\n",
    "        selected_indics = np.arange(len(opts.cvNumframes))\n",
    "        data3D, label3D_r, label3D_i = opts.prepare3D_list(CYCLE)\n",
    "        np.random.shuffle(selected_indics)\n",
    "\n",
    "    elif CYCLE.lower()=='test':\n",
    "        selected_indics = np.arange(len(opts.teNumframes))\n",
    "        data3D, label3D_r, label3D_i = opts.prepare3D_list(CYCLE)\n",
    "        si = selected_indics\n",
    "\n",
    "\n",
    "    f = 0\n",
    "    while (f*batch_size) < len(selected_indics):\n",
    "\n",
    "        if CYCLE.lower()=='train'or CYCLE.lower()=='dev':\n",
    "            if (f+1)*batch_size < len(selected_indics):\n",
    "                si = selected_indics[(f*batch_size):((f+1)*batch_size)]\n",
    "            else:\n",
    "                si = selected_indics[(f*batch_size):]\n",
    "            opts.batch_ids = si;\n",
    "\n",
    "        #print(CYCLE.lower(),f,end='->')\n",
    "        x, y = None, None\n",
    "        for indx in si:\n",
    "            d = data3D[indx]\n",
    "            d = np.concatenate( (d, np.zeros((maxlen-d.shape[0],feat_vec_len))),axis=0 )\n",
    "            d = np.expand_dims(d, axis=0)\n",
    "\n",
    "            l = np.concatenate((label3D_r[indx], label3D_i[indx]), axis=1)\n",
    "            l = np.concatenate( (l, np.zeros((maxlen-l.shape[0],out_vec_len*2))),axis=0 )\n",
    "            l = np.expand_dims(l, axis=0)\n",
    "\n",
    "            x = d if x is None else np.concatenate( (x,d),axis=0 )\n",
    "            y = l if y is None else np.concatenate( (y,l),axis=0 )\n",
    "\n",
    "        f += 1\n",
    "\n",
    "        if CYCLE.lower()=='test':\n",
    "            return x,y\n",
    "\n",
    "        elif x.shape[0]<batch_size:\n",
    "            x = np.concatenate( (x, np.zeros((batch_size-x.shape[0],maxlen,feat_vec_len))), axis=0)\n",
    "            y = np.concatenate( (y, np.zeros((batch_size-y.shape[0],maxlen,out_vec_len*2))), axis=0)\n",
    "\n",
    "        #print(\"x:\",x.shape, \", y:\", y.shape)    \n",
    "        yield x,y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e,[x,y] in enumerate(next_batch_rnn(opts,100,CYCLE='train')):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e,[x,y] in enumerate(next_batch_rnn(opts,100,CYCLE='dev')):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(opts.trNumframes)//256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RNN variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Max_RNN = 256\n",
    "\n",
    "# feat_vec_len = 1230\n",
    "# out_vec_len = 963\n",
    "\n",
    "# epochs = 50\n",
    "# train_size = 15000\n",
    "# dev_size = 3300\n",
    "# batch_size = 256\n",
    "\n",
    "\n",
    "batch_size = 100 #opts.sgd_batch_size\n",
    "epochs = 30\n",
    "\n",
    "Max_RNN = 256\n",
    "\n",
    "'''\n",
    "n_train, n_train_dim = <number of samples for training, input dimentions>, final will be (1951920, 1230)\n",
    "n_dev, n_dev_dim = <number of samples for development, input dimentions>, final will be (449610, 1230)\n",
    "n_test, n_test_dim =<number of samples for testing, input dimentions>, final will be (72440, 1230)\n",
    "\n",
    "'''\n",
    "n_train, n_train_dim = (195192, 1230) # opts.trData.shape = (1951920, 1230)\n",
    "n_dev, n_dev_dim = (44961, 1230) # opts.cvData.shape =(449610, 1230)\n",
    "n_test, n_test_dim = (72440, 1230) # opts.teData.shape =(72440, 1230)\n",
    "\n",
    "n_train_files = 1500 #15000\n",
    "n_dev_files = 330 # 3300\n",
    "n_test_files = 545\n",
    "\n",
    "n_classes = (963 + 963)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 1\n",
    "single bidirectional GRU layer\n",
    "\n",
    "real+img (963+963)=1926-d output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256, 256)          1522688   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256, 1926)         494982    \n",
      "=================================================================\n",
      "Total params: 2,017,670\n",
      "Trainable params: 2,017,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "15/15 [==============================] - 495s - loss: 0.0129 - acc: 0.0074 - mean_squared_error: 0.0129 - mean_absolute_error: 0.0484 - mean_absolute_percentage_error: 3172426.5250 - cosine_proximity: -1.1393e-05 - val_loss: 0.0097 - val_acc: 0.0251 - val_mean_squared_error: 0.0097 - val_mean_absolute_error: 0.0376 - val_mean_absolute_percentage_error: 4361606.5000 - val_cosine_proximity: -2.5875e-05\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 359s - loss: 0.0091 - acc: 0.0432 - mean_squared_error: 0.0091 - mean_absolute_error: 0.0326 - mean_absolute_percentage_error: 3712868.7667 - cosine_proximity: -3.3085e-05 - val_loss: 0.0088 - val_acc: 0.0031 - val_mean_squared_error: 0.0088 - val_mean_absolute_error: 0.0294 - val_mean_absolute_percentage_error: 2075201.1667 - val_cosine_proximity: -3.8719e-05\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 364s - loss: 0.0088 - acc: 0.0053 - mean_squared_error: 0.0088 - mean_absolute_error: 0.0275 - mean_absolute_percentage_error: 1575133.4750 - cosine_proximity: -4.2794e-05 - val_loss: 0.0089 - val_acc: 0.0075 - val_mean_squared_error: 0.0089 - val_mean_absolute_error: 0.0271 - val_mean_absolute_percentage_error: 1035086.6875 - val_cosine_proximity: -4.4962e-05\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 356s - loss: 0.0086 - acc: 0.0195 - mean_squared_error: 0.0086 - mean_absolute_error: 0.0256 - mean_absolute_percentage_error: 905584.0167 - cosine_proximity: -4.7859e-05 - val_loss: 0.0090 - val_acc: 0.4189 - val_mean_squared_error: 0.0090 - val_mean_absolute_error: 0.0265 - val_mean_absolute_percentage_error: 779888.5208 - val_cosine_proximity: -4.9120e-05\n",
      "Epoch 5/30\n",
      " 5/15 [=========>....................] - ETA: 51s - loss: 0.0088 - acc: 0.4495 - mean_squared_error: 0.0088 - mean_absolute_error: 0.0255 - mean_absolute_percentage_error: 757610.7625 - cosine_proximity: -5.1379e-05"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(Max_RNN, return_sequences=True, input_shape=(Max_RNN,opts.dim_input)))\n",
    "# model.add(Bidirectional(LSTM(Max_RNN, return_sequences=True), input_shape=(Max_RNN,feat_vec_len)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Bidirectional(GRU(Max_RNN, return_sequences=True, stateful=True)))\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(next_batch_rnn(opts,batch_size,maxlen=Max_RNN,CYCLE='train'), \n",
    "                    validation_data=next_batch_rnn(opts,batch_size,maxlen=Max_RNN,CYCLE='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train_files//batch_size, \n",
    "                    validation_steps=n_dev_files//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot metrics                                                       \n",
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle(\"Train result\")\n",
    "\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "# plt.plot(history.history['val_mean_absolute_percentage_error'])\n",
    "# plt.plot(history.history['val_cosine_proximity'])\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['acc'])\n",
    "\n",
    "# plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "# plt.plot(history.history['mean_absolute_percentage_error'])\n",
    "# plt.plot(history.history['cosine_proximity'])\n",
    "                                                           \n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"numbers\")\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = next_batch_rnn(opts, batch_size, CYCLE='test', maxlen=Max_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST_MODEL_FILE = \"./dnn_models/lstm_weights1\"+ Code_VERSION+\"_{epoch:02d}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(MODEL_FILE)\n",
    "y_hat = model.predict(x, batch_size=x.shape[0], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sio.savemat(OUTPUT_FILE, {'y_hat':y_hat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HAVE to use NET_STRUCTURE ***********\n",
    "n_input_dim = 195192.0\n",
    "n_input = 1230  # data input\n",
    "n_hidden_1 = 1024  # 1st layer number of neurons\n",
    "n_hidden_2 = 1024  # 2nd layer number of neurons\n",
    "n_hidden_3 = 1024  # 3rd layer number of neurons\n",
    "n_classes = (963 + 963)  # total classes (real+imaginary)\n",
    "\n",
    "# For checking\n",
    "# train_time = np.zeros(training_epochs)\n",
    "\n",
    "# validation_error = np.full((1), np.inf)\n",
    "# min_validation_error = np.full((1), np.inf)\n",
    "\n",
    "Best_Cost = - np.inf\n",
    "Best_Weight, Best_Bias = None, None\n",
    "Best_epoch = -1\n",
    "\n",
    "# For displaying\n",
    "PREVIOUS_10 = 10\n",
    "DIFF_THRESHOLD = 0.000001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected 3 layer Feed Forward Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "\n",
    "# Xavier Initialization\n",
    "def weight_variable(shape, stddev=None, stddev_2=None):\n",
    "    if stddev is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0 / sum(shape)))\n",
    "    elif stddev > 0.0:\n",
    "        if stddev_2 is not None:\n",
    "            r, c = shape\n",
    "            initial1 = tf.truncated_normal([r, c//2], stddev=stddev)\n",
    "            initial2 = tf.truncated_normal([r, c//2], stddev=stddev_2)\n",
    "            initial = tf.concat([initial1,initial2], axis=1)\n",
    "        else:\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "\n",
    "    else:\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape, stddev=None, stddev_2=None):\n",
    "    # initial = tf.constant(0.1, shape=shape)\n",
    "    if stddev is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=np.sqrt(1.0 / sum(shape)))\n",
    "    elif stddev > 0.0:\n",
    "        if stddev_2 is not None:\n",
    "            r, c = shape\n",
    "            initial1 = tf.truncated_normal([r, c//2], stddev=stddev)\n",
    "            initial2 = tf.truncated_normal([r, c//2], stddev=stddev_2)\n",
    "            initial = tf.concat([initial1,initial2], axis=1)\n",
    "        else:\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "\n",
    "    else:\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old way\n",
    "weights = {\n",
    "    'h1': weight_variable([n_input, n_hidden_1],0.001),\n",
    "    'h2': weight_variable([n_hidden_1, n_hidden_2],0.001),\n",
    "    'h3': weight_variable([n_hidden_2, n_hidden_3],0.001),\n",
    "    'out': weight_variable([n_hidden_3, n_classes],0.001,0.0)\n",
    "}\n",
    "biases = {\n",
    "    'b1': bias_variable([1, n_hidden_1],0.0),\n",
    "    'b2': bias_variable([1, n_hidden_2],0.0),\n",
    "    'b3': bias_variable([1, n_hidden_3],0.0),\n",
    "    'out': bias_variable([1, n_classes],0.001,0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_NN(x):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(x, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def calc(x, y):\n",
    "    # Returns predictions and error\n",
    "    predictions = multilayer_NN(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    r, c = y.get_shape().as_list()\n",
    "    r = opts.sgd_batch_size\n",
    "    print('r:', type(r), r, ',c:', type(c), c)\n",
    "\n",
    "    # TRAIN: matlab\n",
    "    # cost1 = 0.5 * sum(sum((pred_real - label_real). ^ 2)) / num_sample;\n",
    "    # cost2 = 0.5 * sum(sum((pred_imag - label_imag). ^ 2)) / num_sample;\n",
    "    # cost = cost1 + cost2;\n",
    "    cost1 = tf.reduce_sum(tf.squared_difference(y[:, :c//2], predictions[:, :c//2]))\n",
    "    cost2 = tf.reduce_sum(tf.squared_difference(y[:, c//2:], predictions[:, c//2:]))\n",
    "    loss_t = 0.5*(cost1+cost2)/r\n",
    "    # mse = tf.losses.mean_squared_error(labels=y, predictions=predictions,weights=0.5)\n",
    "    # loss_t = tf.divide(tf.reduce_sum(mse), r)\n",
    "\n",
    "\n",
    "    # DEV: matlab\n",
    "    # dev_perfs = -mean(sum((dev_label_real - dev_netout1). ^ 2)) - mean(sum((dev_label_imag - dev_netout2). ^ 2));\n",
    "    mse_r = tf.reduce_sum(tf.squared_difference(y[:, :c // 2], predictions[:, :c // 2]), axis=0)\n",
    "    mse_i = tf.reduce_sum(tf.squared_difference(y[:, c // 2:], predictions[:, c // 2:]), axis=0)\n",
    "\n",
    "    loss_d = -tf.reduce_mean(mse_r)-tf.reduce_mean(mse_i)\n",
    "\n",
    "\n",
    "    return [predictions, loss_t, mse_r, mse_i, loss_d ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(best_weights, best_biases, DNN_NET_FILE):\n",
    "    W_1, W_2, W_3, W_4 = [np.array(best_weights['h1'], ndmin=2), np.array(best_weights['h2'], ndmin=2),\n",
    "                          np.array(best_weights['h3'], ndmin=2), np.array([], ndmin=2)]\n",
    "    W_1, W_2, W_3, W_4 = W_1.T, W_2.T, W_3.T, W_4\n",
    "\n",
    "    print('W_1: ', W_1.shape, 'W_2: ', W_2.shape, 'W_3: ', W_3.shape, 'W_4: ', W_4.shape)\n",
    "\n",
    "    b_1, b_2, b_3, b_4 = [np.array(best_biases['b1'], ndmin=2), np.array(best_biases['b2'], ndmin=2),\n",
    "                          np.array(best_biases['b3'], ndmin=2), np.array([], ndmin=2)]\n",
    "    b_1, b_2, b_3, b_4 = b_1.T, b_2.T, b_3.T, b_4\n",
    "\n",
    "    print('b_1: ', b_1.shape, 'b_2: ', b_2.shape, 'b_3: ', b_3.shape, 'b_4: ', b_4.shape)\n",
    "\n",
    "    Wo, bo = np.array(best_weights['out'], ndmin=2), np.array(best_biases['out'], ndmin=2)\n",
    "\n",
    "    Wo1_1, Wo1_2, Wo1_3, Wo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:, :963].T]\n",
    "    bo1_1, bo1_2, bo1_3, bo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2),\n",
    "                                  bo[:, :963].T]\n",
    "    # Wo1_1, Wo1_2, Wo1_3, Wo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:963]]\n",
    "    # bo1_1, bo1_2, bo1_3, bo1_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2),\n",
    "    #                               np.reshape(np.transpose(bo[:963]), (963, 1))]\n",
    "\n",
    "\n",
    "    Wo2_1, Wo2_2, Wo2_3, Wo2_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:, 963:].T]\n",
    "    bo2_1, bo2_2, bo2_3, bo2_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2),\n",
    "                                  bo[:, 963:].T]\n",
    "    # Wo2_1, Wo2_2, Wo2_3, Wo2_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2), Wo[963:]]\n",
    "    # bo2_1, bo2_2, bo2_3, bo2_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2),\n",
    "    #                               np.reshape(np.transpose(bo[963:]), (963, 1))]\n",
    "\n",
    "    print('Wo1_1: ', Wo1_1.shape, 'Wo1_2: ', Wo1_2.shape, 'Wo1_3: ', Wo1_3.shape, 'Wo1_4: ', Wo1_4.shape)\n",
    "    print('bo1_1: ', bo1_1.shape, 'bo1_2: ', bo1_2.shape, 'bo1_3: ', bo1_3.shape, 'bo1_4: ', bo1_4.shape)\n",
    "\n",
    "    print('Wo2_1: ', Wo2_1.shape, 'Wo2_2: ', Wo2_2.shape, 'Wo2_3: ', Wo2_3.shape, 'Wo2_4: ', Wo2_4.shape)\n",
    "    print('bo2_1: ', bo2_1.shape, 'bo2_2: ', bo2_2.shape, 'bo2_3: ', bo2_3.shape, 'bo2_4: ', bo2_4.shape)\n",
    "\n",
    "    # Param_Dict = {'W': np.transpose([W_1,W_2, W_3]), 'b':np.transpose([b_1,b_2, b_3]) }\n",
    "    # Param_Dict = {'W': { (W_1, W_2, (W_3)}, 'b': {(b_1), (b_2), (b_3)}}\n",
    "    Param_Dict = np.core.records.fromarrays(\n",
    "        [[W_1, W_2, W_3, W_4], [b_1, b_2, b_3, b_4], [Wo1_1, Wo1_2, Wo1_3, Wo1_4], [bo1_1, bo1_2, bo1_3, bo1_4],\n",
    "         [Wo2_1, Wo2_2, Wo2_3, Wo2_4], [bo2_1, bo2_2, bo2_3, bo2_4]], names='W,b,Wo1,bo1,Wo2,bo2')\n",
    "\n",
    "    # print(Param_Dict.shape, Param_Dict)\n",
    "    master_dict = {'struct_net': [Param_Dict]}\n",
    "\n",
    "    sio.savemat(DNN_NET_FILE, master_dict, format='5', long_field_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN ops\n",
    "y_p, loss_op, m_r, m_i, _ = calc(X, Y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss=loss_op)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "\n",
    "for epoch in range(opts.sgd_max_epoch):\n",
    "\n",
    "    s = time.time()\n",
    "    cost_sum = 0.0\n",
    "\n",
    "    ##### Create Train Batch, evaluate COST, Weight, Bias #####\n",
    "    ###########################################################\n",
    "    for batch_num, (batch_x, batch_y) in enumerate(opts.next_batch(opts.trData.shape[0], opts.sgd_batch_size)):\n",
    "\n",
    "        _, c, epoch_w, epoch_b  = sess.run([train_op, loss_op, weights, biases ],\n",
    "                                          feed_dict={X: batch_x, Y: batch_y})\n",
    "        cost_sum += c\n",
    "\n",
    "        if batch_num % 1000 == 0:\n",
    "            print('[T] - Epoch:', epoch, ', batch_num:', batch_num, \", Cost:\", c, \"Cost Sum:\", cost_sum)\n",
    "\n",
    "    print('[T] - Epoch:', epoch, \",Sum:\", cost_sum)\n",
    "\n",
    "\n",
    "    ################ Validation in whole batch ################\n",
    "    ###########################################################\n",
    "\n",
    "    avg_cost, sum_mse_r, sum_mse_i = 0.0, np.zeros(n_classes//2), np.zeros(n_classes//2)\n",
    "    ##### Create DEV Batch, evaluate COST, Weight, Bias #####\n",
    "    ###########################################################\n",
    "    for batch_num, (batch_x, batch_y) in enumerate(opts.next_batch(opts.cvData.shape[0], opts.sgd_batch_size, isTrainCycle=False)):\n",
    "\n",
    "        mse_r, mse_i = sess.run([m_r, m_i], feed_dict={X: batch_x, Y: batch_y})\n",
    "        # print(sum_mse_r.shape, mse_r.shape, mse_i.shape)\n",
    "        sum_mse_r += (mse_r)\n",
    "        sum_mse_i += (mse_i)\n",
    "\n",
    "    avg_cost = - np.mean(sum_mse_r) - np.mean(sum_mse_i)\n",
    "    # print('[D] - Epoch:', epoch, \", Mean Real:\", - np.mean(sum_mse_r), \", Mean Img:\" , - np.mean(sum_mse_i), \", Avg Cost:\", avg_cost)\n",
    "\n",
    "    print(\"[D] - Epoch:{0}, Avg Cost:{1:.5f}, Real(mean,var):({2:.5f},{3:.5f}), Img(mean,var):({4:.5f},{5:.5f})\".format( epoch,avg_cost, -np.mean(sum_mse_r),-np.var(sum_mse_r), -np.mean(sum_mse_i),\n",
    "          -np.var(sum_mse_i)) )\n",
    "\n",
    "    ####### Min validation error, update weights, bias #########\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    if avg_cost > Best_Cost:\n",
    "        Best_Cost = avg_cost\n",
    "        Best_Weight = epoch_w\n",
    "        Best_Bias = epoch_b\n",
    "        Best_epoch = epoch\n",
    "\n",
    "        print('***** [D] - Best Model at Epoch:', epoch, \", Avg Cost:\", avg_cost, '*****')\n",
    "\n",
    "\n",
    "    ######################  Write Model File ###################\n",
    "    ############################################################\n",
    "\n",
    "    print('[] - Elapsed Time: {0:.2f}(s), {1:.2f}(min)'.format( time.time()-s, (time.time()-s)/60 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training DONE !!!\n",
    "print(\"Optimization Finished!\")\n",
    "print('***** [-] - Best Model at Epoch:', Best_epoch, \", Best Val Cost:\", Best_Cost, '*****')\n",
    "write_file(Best_Weight, Best_Bias, DNN_NET_FILE)\n",
    "print(\" File Write complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
