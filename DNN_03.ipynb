{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN\n",
    "### with pretraining\n",
    "*jul 12, 2018 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "- Read parameters from .mat files\n",
    "- save in Opt object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_VERSION = '_e10v5'\n",
    "Code_VERSION = '_e03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNN_DATA_FILE = \"./dnn_models/DNN_datas\"+Data_VERSION+\".mat\"\n",
    "\n",
    "DNN_MODEL_FILE = \"./dnn_models/DNN_params\"+Data_VERSION+\".mat\"\n",
    "\n",
    "DNN_NET_FILE = \"./dnn_models/DNN_net\"+Code_VERSION+\".mat\"\n",
    "\n",
    "MODEL_FILE = \"./dnn_models/weights_{epoch:02d}\"+ Code_VERSION+\".h5\"\n",
    "\n",
    "SAVE_MODEL_FILE = \"./dnn_models/py_model\"+ Code_VERSION+\".h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    opts_dict = dict()\n",
    "\n",
    "    def __init__(self, FILE, FILE_DATA):\n",
    "        \n",
    "        # Basic parameters\n",
    "        with h5py.File(FILE, 'r') as f:\n",
    "            key_list = list(f.keys())\n",
    "            print('Opt key:',key_list)\n",
    "\n",
    "            for k, v in f['opts'].items():\n",
    "\n",
    "                print('key:', k)\n",
    "\n",
    "                if k == 'ARMA_order':\n",
    "                    self.ARMA_order = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.ARMA_order\n",
    "                elif k == 'ada_grad_eps':\n",
    "                    self.ada_grad_eps = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.ada_grad_eps\n",
    "                elif k == 'ada_sgd_scale':\n",
    "                    self.ada_sgd_scale = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.ada_sgd_scale\n",
    "                elif k == 'change_momentum_point':\n",
    "                    self.change_momentum_point = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.change_momentum_point\n",
    "                elif k == 'cost_function':\n",
    "                    self.cost_function = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.cost_function += chr(c[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.cost_function\n",
    "\n",
    "                elif k == 'cv_interval':\n",
    "                    self.cv_interval = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.cv_interval\n",
    "                elif k == 'dim_input':\n",
    "                    self.dim_input = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.dim_input\n",
    "                elif k == 'dim_output':\n",
    "                    self.dim_output = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.dim_output\n",
    "                elif k == 'drop_ratio':\n",
    "                    self.drop_ratio = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.drop_ratio\n",
    "                elif k == 'eval_on_gpu':\n",
    "                    self.eval_on_gpu = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.eval_on_gpu\n",
    "                elif k == 'final_momentum':\n",
    "                    self.final_momentum = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.final_momentum\n",
    "                elif k == 'hid_struct':\n",
    "                    self.hid_struct = np.array(v)\n",
    "                    self.opts_dict[k] = self.hid_struct\n",
    "                elif k == 'initial_momentum':\n",
    "                    self.initial_momentum = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.initial_momentum\n",
    "                elif k == 'isDropout':\n",
    "                    self.isDropout = 0\n",
    "                    self.opts_dict[k] = self.isDropout\n",
    "                elif k == 'isDropoutInput':\n",
    "                    self.isDropoutInput = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isDropoutInput\n",
    "                elif k == 'isGPU':\n",
    "                    self.isGPU = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isGPU\n",
    "                elif k == 'isNormalize':\n",
    "                    self.isNormalize = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isNormalize\n",
    "                elif k == 'isPretrain':\n",
    "                    self.isPretrain = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isPretrain\n",
    "                elif k == 'learner':\n",
    "                    self.learner = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.learner += chr(c[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.learner\n",
    "\n",
    "                elif k == 'net_struct':\n",
    "                    self.net_struct = np.array(v)\n",
    "                    for n_s in np.array(v):\n",
    "                        print('Opts Net Stuct:',n_s[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.net_struct\n",
    "                elif k == 'rbm_batch_size':\n",
    "                    self.rbm_batch_size = int(np.array(v)[0][0])\n",
    "                    print(\"self.rbm_batch_size:\",self.rbm_batch_size)\n",
    "                    # self.opts_dict[k] = self.rbm_batch_size\n",
    "                elif k == 'rbm_learn_rate_binary':\n",
    "                    self.rbm_learn_rate_binary = np.array(v)\n",
    "                    # self.opts_dict[k] = self.rbm_learn_rate_binary\n",
    "                elif k == 'rbm_learn_rate_real':\n",
    "                    self.rbm_learn_rate_real = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_learn_rate_real\n",
    "                elif k == 'rbm_max_epoch':\n",
    "                    self.rbm_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_max_epoch\n",
    "                elif k == 'save_on_fly':\n",
    "                    self.save_on_fly = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.save_on_fly\n",
    "                elif k == 'sgd_batch_size':\n",
    "                    self.sgd_batch_size = int(np.array(v)[0][0])\n",
    "                    print(\"self.sgd_batch_size:\",self.sgd_batch_size)\n",
    "#                     self.sgd_batch_size = 1024//2 # BATCH_SIZE for training net\n",
    "                    # self.opts_dict[k] = self.sgd_batch_size\n",
    "                elif k == 'sgd_learn_rate':\n",
    "                    self.sgd_learn_rate = np.array(v)\n",
    "                    # self.opts_dict[k] = self.sgd_learn_rate\n",
    "                elif k == 'sgd_max_epoch':\n",
    "                    self.sgd_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.sgd_max_epoch\n",
    "                elif k == 'split_tanh1_c1':\n",
    "                    self.split_tanh1_c1 = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.split_tanh1_c1\n",
    "                elif k == 'split_tanh1_c2':\n",
    "                    self.split_tanh1_c2 = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.split_tanh1_c2\n",
    "                elif k == 'unit_type_hidden':\n",
    "                    self.unit_type_hidden = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_hidden += chr(c[0])\n",
    "\n",
    "                elif k == 'unit_type_output':\n",
    "                    self.unit_type_output = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_output += chr(c[0])\n",
    "\n",
    "        # Training and Dev Data \n",
    "        with h5py.File(FILE_DATA, 'r') as f:\n",
    "            print('Opts h5py keys:', list(f.keys()))\n",
    "            for k, v in f.items():\n",
    "\n",
    "                if k == 'trData':\n",
    "                    print(\"trData.shape: \", v.shape)\n",
    "                    self.trData = np.transpose(np.array(v))\n",
    "                    print(\"trData-> mean:\", np.mean(self.trData), \", var:\", np.var(self.trData), \", std:\",\n",
    "                          np.std(self.trData), \", range:\", (np.amin(self.trData),np.amax(self.trData)))\n",
    "                elif k == 'trLabel_i':\n",
    "                    print(\"trLabel_i.shape: \", v.shape)\n",
    "                    self.trLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_i-> mean:\", np.mean(self.trLabel_i), \", var:\", np.var(self.trLabel_i), \", std:\",\n",
    "                          np.std(self.trLabel_i), \", range:\", (np.amin(self.trLabel_i),np.amax(self.trLabel_i)))\n",
    "                elif k == 'trLabel_r':\n",
    "                    print(\"trLabel_r.shape: \", v.shape)\n",
    "                    self.trLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_r-> mean:\", np.mean(self.trLabel_r), \", var:\", np.var(self.trLabel_r), \", std:\",\n",
    "                          np.std(self.trLabel_r), \", range:\", (np.amin(self.trLabel_r),np.amax(self.trLabel_r)))\n",
    "                elif k == 'trNumframes':\n",
    "                    print(\"trNumframes.shape: \", v.shape)\n",
    "                    self.trNumframes = np.transpose(np.array(v))\n",
    "                    \n",
    "                elif k == 'cvData':\n",
    "                    print(\"cvData.shape: \", v.shape)\n",
    "                    self.cvData = np.transpose(np.array(v))\n",
    "                    print(\"cvData-> mean:\", np.mean(self.cvData), \", var:\", np.var(self.cvData), \", std:\",\n",
    "                          np.std(self.cvData), \", range:\", (np.amin(self.cvData),np.amax(self.cvData)))\n",
    "                elif k == 'cvLabel_i':\n",
    "                    print(\"cvLabel_i.shape: \", v.shape)\n",
    "                    self.cvLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_i-> mean:\", np.mean(self.cvLabel_i), \", var:\", np.var(self.cvLabel_i), \", std:\",\n",
    "                          np.std(self.cvLabel_i), \", range:\", (np.amin(self.cvLabel_i),np.amax(self.cvLabel_i)))\n",
    "                elif k == 'cvLabel_r':\n",
    "                    print(\"cvLabel_r.shape: \", v.shape)\n",
    "                    self.cvLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_r-> mean:\", np.mean(self.cvLabel_r), \", var:\", np.var(self.cvLabel_r), \", std:\",\n",
    "                          np.std(self.cvLabel_r), \", range:\", (np.amin(self.cvLabel_r),np.amax(self.cvLabel_r)))\n",
    "                elif k == 'cvNumframes':\n",
    "                    print(\"cvNumframes.shape: \", v.shape)\n",
    "                    self.cvNumframes = np.transpose(np.array(v))\n",
    "\n",
    "            self.trLabel = np.concatenate((self.trLabel_r, self.trLabel_i), axis=1)\n",
    "            self.cvLabel = np.concatenate((self.cvLabel_r, self.cvLabel_i), axis=1)\n",
    "\n",
    "\n",
    "    def ready_batchID(self, total_num_samples, batch_size):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        batchID = []\n",
    "        num_batch = math.ceil(total_num_samples/batch_size)\n",
    "\n",
    "        for b in range( int(num_batch) ):\n",
    "            s = b*batch_size\n",
    "            e = (b+1)*batch_size -1\n",
    "\n",
    "            if e >= total_num_samples:\n",
    "                e = total_num_samples - 1\n",
    "\n",
    "            batchID.append((s,e))\n",
    "\n",
    "        return np.array(batchID,ndmin=2)\n",
    "\n",
    "\n",
    "    def suffle_data(self, total_num_samples):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        return  np.random.permutation(total_num_samples)\n",
    "\n",
    "\n",
    "    def next_batch(self, total_num_samples, batch_size, isTrainCycle=True):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        while True:\n",
    "            batchID = self.ready_batchID(total_num_samples, batch_size) \n",
    "            seq = self.suffle_data(total_num_samples)\n",
    "\n",
    "            for batch in range(batchID.shape[0]):\n",
    "                if isTrainCycle:\n",
    "                    x = opts.trData[ seq[batchID[batch][0]:batchID[batch][1] ] ]\n",
    "                    y = opts.trLabel[ seq[batchID[batch][0]:batchID[batch][1] ] ]\n",
    "\n",
    "                else:\n",
    "                    x = opts.cvData[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "                    y = opts.cvLabel[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "\n",
    "                # print('Next Batch', x.shape, y.shape)\n",
    "                yield [x, y]\n",
    "\n",
    "\n",
    "    def next_batch2(self, batch_size, isTrainCycle=True):\n",
    "        if isTrainCycle:\n",
    "            # selected_indics = np.random.choice(10, size=batch_size, replace=False)\n",
    "            selected_indics = np.random.randint(195192, size=batch_size)\n",
    "        else:\n",
    "            # selected_indics = np.random.randint(7, size= batch_size)\n",
    "            selected_indics = np.random.randint(44961, size=batch_size)\n",
    "\n",
    "        # print (\"selected_indics: \",selected_indics)\n",
    "        if isTrainCycle:\n",
    "            x = opts.trData[selected_indics]\n",
    "            y = np.concatenate((opts.trLabel_r, opts.trLabel_i), axis=1)[selected_indics]\n",
    "\n",
    "        else:\n",
    "            x = opts.cvData[selected_indics]\n",
    "            y = np.concatenate((opts.cvLabel_r, opts.cvLabel_i), axis=1)[selected_indics]\n",
    "\n",
    "        # print('Next Batch', x.shape, y.shape)\n",
    "        return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt key: ['#refs#', 'opts']\n",
      "key: ARMA_order\n",
      "key: ada_grad_eps\n",
      "key: ada_sgd_scale\n",
      "key: change_momentum_point\n",
      "key: cost_function\n",
      "key: cv_interval\n",
      "key: dim_input\n",
      "key: dim_output\n",
      "key: drop_ratio\n",
      "key: eval_on_gpu\n",
      "key: final_momentum\n",
      "key: hid_struct\n",
      "key: initial_momentum\n",
      "key: isDropout\n",
      "key: isDropoutInput\n",
      "key: isGPU\n",
      "key: isNormalize\n",
      "key: isPretrain\n",
      "key: learner\n",
      "key: net_struct\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "Opts Net Stuct: <HDF5 object reference>\n",
      "key: rbm_batch_size\n",
      "self.rbm_batch_size: 1024\n",
      "key: rbm_learn_rate_binary\n",
      "key: rbm_learn_rate_real\n",
      "key: rbm_max_epoch\n",
      "key: save_on_fly\n",
      "key: sgd_batch_size\n",
      "self.sgd_batch_size: 1024\n",
      "key: sgd_learn_rate\n",
      "key: sgd_max_epoch\n",
      "key: split_tanh1_c1\n",
      "key: split_tanh1_c2\n",
      "key: tr_mu\n",
      "key: tr_std\n",
      "key: unit_type_hidden\n",
      "key: unit_type_output\n",
      "Opts h5py keys: ['cvData', 'cvLabel_i', 'cvLabel_r', 'cvNumframes', 'trData', 'trLabel_i', 'trLabel_r', 'trNumframes']\n",
      "cvData.shape:  (1230, 449610)\n",
      "cvData-> mean: -3.12831e-06 , var: 0.607645 , std: 0.779516 , range: (-9.6298428, 10.49406)\n",
      "cvLabel_i.shape:  (963, 449610)\n",
      "cvLabel_i-> mean: -1.2493e-05 , var: 0.0129563 , std: 0.113826 , range: (-10.0, 10.0)\n",
      "cvLabel_r.shape:  (963, 449610)\n",
      "cvLabel_r-> mean: 0.028573 , var: 0.0212577 , std: 0.1458 , range: (-10.0, 10.0)\n",
      "cvNumframes.shape:  (1, 3300)\n",
      "trData.shape:  (1230, 1951920)\n",
      "trData-> mean: -0.000807387 , var: 0.612503 , std: 0.782626 , range: (-11.557238, 12.486189)\n",
      "trLabel_i.shape:  (963, 1951920)\n",
      "trLabel_i-> mean: 1.29036e-06 , var: 0.0133129 , std: 0.115382 , range: (-10.0, 10.0)\n",
      "trLabel_r.shape:  (963, 1951920)\n",
      "trLabel_r-> mean: 0.0295324 , var: 0.021696 , std: 0.147296 , range: (-10.0, 10.0)\n",
      "trNumframes.shape:  (1, 15000)\n"
     ]
    }
   ],
   "source": [
    "opts = Opts(DNN_MODEL_FILE, DNN_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HAVE to use OPS PARAMS ***********\n",
    "learning_rate = 0.001\n",
    "# training_epochs = 80\n",
    "# batch_size = 256\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HAVE to use NET_STRUCTURE ***********\n",
    "n_input_dim = 195192.0\n",
    "n_input = 1230  # data input\n",
    "n_hidden_1 = 1024  # 1st layer number of neurons\n",
    "n_hidden_2 = 1024  # 2nd layer number of neurons\n",
    "n_hidden_3 = 1024  # 3rd layer number of neurons\n",
    "n_classes = (963 + 963)  # total classes (real+imaginary)\n",
    "\n",
    "# For checking\n",
    "# train_time = np.zeros(training_epochs)\n",
    "\n",
    "# validation_error = np.full((1), np.inf)\n",
    "# min_validation_error = np.full((1), np.inf)\n",
    "\n",
    "Best_Cost = - np.inf\n",
    "Best_Weight, Best_Bias = None, None\n",
    "Best_epoch = -1\n",
    "\n",
    "# For displaying\n",
    "PREVIOUS_10 = 10\n",
    "DIFF_THRESHOLD = 0.000001"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "################################################################\n",
    "'''\n",
    "Fully-Connected 3 layer Feed Forward Net\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "\n",
    "# Xavier Initialization\n",
    "def weight_variable(shape, stddev=None, stddev_2=None):\n",
    "    if stddev is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0 / sum(shape)))\n",
    "    elif stddev > 0.0:\n",
    "        if stddev_2 is not None:\n",
    "            r, c = shape\n",
    "            initial1 = tf.truncated_normal([r, c//2], stddev=stddev)\n",
    "            initial2 = tf.truncated_normal([r, c//2], stddev=stddev_2)\n",
    "            initial = tf.concat([initial1,initial2], axis=1)\n",
    "        else:\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "\n",
    "    else:\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape, stddev=None, stddev_2=None):\n",
    "    # initial = tf.constant(0.1, shape=shape)\n",
    "    if stddev is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=np.sqrt(1.0 / sum(shape)))\n",
    "    elif stddev > 0.0:\n",
    "        if stddev_2 is not None:\n",
    "            r, c = shape\n",
    "            initial1 = tf.truncated_normal([r, c//2], stddev=stddev)\n",
    "            initial2 = tf.truncated_normal([r, c//2], stddev=stddev_2)\n",
    "            initial = tf.concat([initial1,initial2], axis=1)\n",
    "        else:\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "\n",
    "    else:\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# matlab way\n",
    "weights = {\n",
    "    'h1': weight_variable([n_input, n_hidden_1],0.001),\n",
    "    'h2': weight_variable([n_hidden_1, n_hidden_2],0.001),\n",
    "    'h3': weight_variable([n_hidden_2, n_hidden_3],0.001),\n",
    "    'out': weight_variable([n_hidden_3, n_classes],0.001,0.0)\n",
    "}\n",
    "biases = {\n",
    "    'b1': bias_variable([1, n_hidden_1],0.0),\n",
    "    'b2': bias_variable([1, n_hidden_2],0.0),\n",
    "    'b3': bias_variable([1, n_hidden_3],0.0),\n",
    "    'out': bias_variable([1, n_classes],0.001,0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create model\n",
    "def multilayer_NN(x):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(x, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def calc(x, y):\n",
    "    # Returns predictions and error\n",
    "    predictions = multilayer_NN(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    r, c = y.get_shape().as_list()\n",
    "    r = opts.sgd_batch_size\n",
    "    print('r:', type(r), r, ',c:', type(c), c)\n",
    "\n",
    "    # TRAIN: matlab\n",
    "    # cost1 = 0.5 * sum(sum((pred_real - label_real). ^ 2)) / num_sample;\n",
    "    # cost2 = 0.5 * sum(sum((pred_imag - label_imag). ^ 2)) / num_sample;\n",
    "    # cost = cost1 + cost2;\n",
    "    cost1 = tf.reduce_sum(tf.squared_difference(y[:, :c//2], predictions[:, :c//2]))\n",
    "    cost2 = tf.reduce_sum(tf.squared_difference(y[:, c//2:], predictions[:, c//2:]))\n",
    "    loss_t = 0.5*(cost1+cost2)/r\n",
    "    # mse = tf.losses.mean_squared_error(labels=y, predictions=predictions,weights=0.5)\n",
    "    # loss_t = tf.divide(tf.reduce_sum(mse), r)\n",
    "\n",
    "\n",
    "    # DEV: matlab\n",
    "    # dev_perfs = -mean(sum((dev_label_real - dev_netout1). ^ 2)) - mean(sum((dev_label_imag - dev_netout2). ^ 2));\n",
    "    mse_r = tf.reduce_sum(tf.squared_difference(y[:, :c // 2], predictions[:, :c // 2]), axis=0)\n",
    "    mse_i = tf.reduce_sum(tf.squared_difference(y[:, c // 2:], predictions[:, c // 2:]), axis=0)\n",
    "\n",
    "    loss_d = -tf.reduce_mean(mse_r)-tf.reduce_mean(mse_i)\n",
    "\n",
    "\n",
    "    return [predictions, loss_t, mse_r, mse_i, loss_d ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(best_weights, best_biases, DNN_NET_FILE):\n",
    "    W_1, W_2, W_3, W_4 = [np.array(best_weights['h1'], ndmin=2), np.array(best_weights['h2'], ndmin=2),\n",
    "                          np.array(best_weights['h3'], ndmin=2), np.array([], ndmin=2)]\n",
    "    W_1, W_2, W_3, W_4 = W_1.T, W_2.T, W_3.T, W_4\n",
    "\n",
    "    print('W_1: ', W_1.shape, 'W_2: ', W_2.shape, 'W_3: ', W_3.shape, 'W_4: ', W_4.shape)\n",
    "\n",
    "    b_1, b_2, b_3, b_4 = [np.array(best_biases['b1'], ndmin=2), np.array(best_biases['b2'], ndmin=2),\n",
    "                          np.array(best_biases['b3'], ndmin=2), np.array([], ndmin=2)]\n",
    "    b_1, b_2, b_3, b_4 = b_1.T, b_2.T, b_3.T, b_4\n",
    "\n",
    "    print('b_1: ', b_1.shape, 'b_2: ', b_2.shape, 'b_3: ', b_3.shape, 'b_4: ', b_4.shape)\n",
    "\n",
    "    Wo, bo = np.array(best_weights['out'], ndmin=2), np.array(best_biases['out'], ndmin=2)\n",
    "\n",
    "    Wo1_1, Wo1_2, Wo1_3, Wo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:, :963].T]\n",
    "    bo1_1, bo1_2, bo1_3, bo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2),\n",
    "                                  bo[:, :963].T]\n",
    "    # Wo1_1, Wo1_2, Wo1_3, Wo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:963]]\n",
    "    # bo1_1, bo1_2, bo1_3, bo1_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2),\n",
    "    #                               np.reshape(np.transpose(bo[:963]), (963, 1))]\n",
    "\n",
    "\n",
    "    Wo2_1, Wo2_2, Wo2_3, Wo2_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:, 963:].T]\n",
    "    bo2_1, bo2_2, bo2_3, bo2_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2),\n",
    "                                  bo[:, 963:].T]\n",
    "    # Wo2_1, Wo2_2, Wo2_3, Wo2_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2), Wo[963:]]\n",
    "    # bo2_1, bo2_2, bo2_3, bo2_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2),\n",
    "    #                               np.reshape(np.transpose(bo[963:]), (963, 1))]\n",
    "\n",
    "    print('Wo1_1: ', Wo1_1.shape, 'Wo1_2: ', Wo1_2.shape, 'Wo1_3: ', Wo1_3.shape, 'Wo1_4: ', Wo1_4.shape)\n",
    "    print('bo1_1: ', bo1_1.shape, 'bo1_2: ', bo1_2.shape, 'bo1_3: ', bo1_3.shape, 'bo1_4: ', bo1_4.shape)\n",
    "\n",
    "    print('Wo2_1: ', Wo2_1.shape, 'Wo2_2: ', Wo2_2.shape, 'Wo2_3: ', Wo2_3.shape, 'Wo2_4: ', Wo2_4.shape)\n",
    "    print('bo2_1: ', bo2_1.shape, 'bo2_2: ', bo2_2.shape, 'bo2_3: ', bo2_3.shape, 'bo2_4: ', bo2_4.shape)\n",
    "\n",
    "    # Param_Dict = {'W': np.transpose([W_1,W_2, W_3]), 'b':np.transpose([b_1,b_2, b_3]) }\n",
    "    # Param_Dict = {'W': { (W_1, W_2, (W_3)}, 'b': {(b_1), (b_2), (b_3)}}\n",
    "    Param_Dict = np.core.records.fromarrays(\n",
    "        [[W_1, W_2, W_3, W_4], [b_1, b_2, b_3, b_4], [Wo1_1, Wo1_2, Wo1_3, Wo1_4], [bo1_1, bo1_2, bo1_3, bo1_4],\n",
    "         [Wo2_1, Wo2_2, Wo2_3, Wo2_4], [bo2_1, bo2_2, bo2_3, bo2_4]], names='W,b,Wo1,bo1,Wo2,bo2')\n",
    "\n",
    "    # print(Param_Dict.shape, Param_Dict)\n",
    "    master_dict = {'struct_net': [Param_Dict]}\n",
    "\n",
    "    sio.savemat(DNN_NET_FILE, master_dict, format='5', long_field_names=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################################################################\n",
    "'''\n",
    "main()\n",
    "'''\n",
    "\n",
    "# TRAIN ops\n",
    "y_p, loss_op, m_r, m_i, _ = calc(X, Y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss=loss_op)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "\n",
    "for epoch in range(opts.sgd_max_epoch):\n",
    "\n",
    "    s = time.time()\n",
    "    cost_sum = 0.0\n",
    "\n",
    "    ##### Create Train Batch, evaluate COST, Weight, Bias #####\n",
    "    ###########################################################\n",
    "    for batch_num, (batch_x, batch_y) in enumerate(opts.next_batch(opts.trData.shape[0], opts.sgd_batch_size)):\n",
    "\n",
    "        _, c, epoch_w, epoch_b  = sess.run([train_op, loss_op, weights, biases ],\n",
    "                                          feed_dict={X: batch_x, Y: batch_y})\n",
    "        cost_sum += c\n",
    "\n",
    "        if batch_num % 1000 == 0:\n",
    "            print('[T] - Epoch:', epoch, ', batch_num:', batch_num, \", Cost:\", c, \"Cost Sum:\", cost_sum)\n",
    "\n",
    "    print('[T] - Epoch:', epoch, \",Sum:\", cost_sum)\n",
    "\n",
    "\n",
    "    ################ Validation in whole batch ################\n",
    "    ###########################################################\n",
    "\n",
    "    avg_cost, sum_mse_r, sum_mse_i = 0.0, np.zeros(n_classes//2), np.zeros(n_classes//2)\n",
    "    ##### Create DEV Batch, evaluate COST, Weight, Bias #####\n",
    "    ###########################################################\n",
    "    for batch_num, (batch_x, batch_y) in enumerate(opts.next_batch(opts.cvData.shape[0], opts.sgd_batch_size, isTrainCycle=False)):\n",
    "\n",
    "        mse_r, mse_i = sess.run([m_r, m_i], feed_dict={X: batch_x, Y: batch_y})\n",
    "        # print(sum_mse_r.shape, mse_r.shape, mse_i.shape)\n",
    "        sum_mse_r += (mse_r)\n",
    "        sum_mse_i += (mse_i)\n",
    "\n",
    "    avg_cost = - np.mean(sum_mse_r) - np.mean(sum_mse_i)\n",
    "    # print('[D] - Epoch:', epoch, \", Mean Real:\", - np.mean(sum_mse_r), \", Mean Img:\" , - np.mean(sum_mse_i), \", Avg Cost:\", avg_cost)\n",
    "\n",
    "    print(\"[D] - Epoch:{0}, Avg Cost:{1:.5f}, Real(mean,var):({2:.5f},{3:.5f}), Img(mean,var):({4:.5f},{5:.5f})\".format( epoch,avg_cost, -np.mean(sum_mse_r),-np.var(sum_mse_r), -np.mean(sum_mse_i),\n",
    "          -np.var(sum_mse_i)) )\n",
    "\n",
    "    # val_batch_x, val_batch_y = opts.next_batch(44961, False)\n",
    "    #\n",
    "    # val_mse = tf.losses.mean_squared_error(labels=Y, predictions=multilayer_NN(X))\n",
    "    # val_loss = tf.reduce_sum(val_mse)\n",
    "    #\n",
    "    # validation_cost_epoch = sess.run(val_loss, feed_dict={X: val_batch_x, Y: val_batch_y})\n",
    "    # print('[V] - Epoch:', epoch, 'Validation error =', validation_cost_epoch, \"Time:\", time.time() - s)\n",
    "\n",
    "    ####### Min validation error, update weights, bias #########\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    if avg_cost > Best_Cost:\n",
    "        Best_Cost = avg_cost\n",
    "        Best_Weight = epoch_w\n",
    "        Best_Bias = epoch_b\n",
    "        Best_epoch = epoch\n",
    "\n",
    "        print('***** [D] - Best Model at Epoch:', epoch, \", Avg Cost:\", avg_cost, '*****'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######################  Write Model File ###################\n",
    "    ############################################################\n",
    "\n",
    "    print('[] - Elapsed Time: {0:.2f}(s), {1:.2f}(min)'.format( time.time()-s, (time.time()-s)/60 ) )\n",
    "\n",
    "\n",
    "# Training DONE !!!\n",
    "print(\"Optimization Finished!\")\n",
    "print('***** [-] - Best Model at Epoch:', Best_epoch, \", Best Val Cost:\", Best_Cost, '*****')\n",
    "write_file(Best_Weight, Best_Bias, DNN_NET_FILE)\n",
    "print(\" File Write complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    print('LOSS-fun:',type(yTrue), yTrue.get_shape()[0].value)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "#     r, c = yTrue.shape\n",
    "    c = 963 + 963\n",
    "    r = 128 # opts.sgd_batch_size\n",
    "\n",
    "    cost1 = K.sum(K.square(yTrue[:, :c//2]- yPred[:, :c//2]))\n",
    "    cost2 = K.sum(K.square(yTrue[:, c//2:]- yPred[:, c//2:]))\n",
    "    return 0.5*(cost1+cost2)/r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #opts.sgd_batch_size\n",
    "epochs = 30\n",
    "\n",
    "n_input_sz = 1951920\n",
    "n_out_sz = 449610\n",
    "\n",
    "n_input = 1230  # data input\n",
    "n_hidden_1 = 2048  # 1st layer number of neurons\n",
    "n_hidden_2 = 2048  # 2nd layer number of neurons\n",
    "n_hidden_3 = 2048  # 3rd layer number of neurons\n",
    "n_classes = (963 + 963)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS-fun: <class 'tensorflow.python.framework.ops.Tensor'> None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2048)              2521088   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1926)              3946374   \n",
      "=================================================================\n",
      "Total params: 6,467,462\n",
      "Trainable params: 6,467,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "15250/15250 [==============================] - 448s - loss: 15.9496 - acc: 0.0284 - mean_squared_error: 0.0167 - mean_absolute_error: 0.0459 - mean_absolute_percentage_error: 20734.2915 - cosine_proximity: -1.1466e-04 - val_loss: 15.6641 - val_acc: 0.0276 - val_mean_squared_error: 0.0164 - val_mean_absolute_error: 0.0465 - val_mean_absolute_percentage_error: 16721.4058 - val_cosine_proximity: -1.0705e-04\n",
      "Epoch 2/30\n",
      "15250/15250 [==============================] - 436s - loss: 15.7288 - acc: 0.0303 - mean_squared_error: 0.0165 - mean_absolute_error: 0.0443 - mean_absolute_percentage_error: 9474.1658 - cosine_proximity: -1.1867e-04 - val_loss: 15.4876 - val_acc: 0.0348 - val_mean_squared_error: 0.0162 - val_mean_absolute_error: 0.0444 - val_mean_absolute_percentage_error: 8371.4928 - val_cosine_proximity: -1.1224e-04\n",
      "Epoch 3/30\n",
      "15250/15250 [==============================] - 436s - loss: 15.7134 - acc: 0.0305 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0441 - mean_absolute_percentage_error: 8249.8394 - cosine_proximity: -1.1944e-04 - val_loss: 15.4468 - val_acc: 0.0396 - val_mean_squared_error: 0.0162 - val_mean_absolute_error: 0.0440 - val_mean_absolute_percentage_error: 5643.3060 - val_cosine_proximity: -1.1502e-0415.7140 - acc: 0.0304 - mean_squared_error: 0.0164 - mean_absolute_error: 0.\n",
      "Epoch 4/30\n",
      "15250/15250 [==============================] - 434s - loss: 15.7026 - acc: 0.0308 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0440 - mean_absolute_percentage_error: 7417.4094 - cosine_proximity: -1.1988e-04 - val_loss: 15.4772 - val_acc: 0.0335 - val_mean_squared_error: 0.0162 - val_mean_absolute_error: 0.0442 - val_mean_absolute_percentage_error: 8819.9516 - val_cosine_proximity: -1.1320e-04\n",
      "Epoch 5/30\n",
      "15250/15250 [==============================] - 426s - loss: 15.6995 - acc: 0.0307 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0439 - mean_absolute_percentage_error: 6954.8113 - cosine_proximity: -1.2008e-04 - val_loss: 15.4151 - val_acc: 0.0362 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0435 - val_mean_absolute_percentage_error: 4940.5804 - val_cosine_proximity: -1.1690e-04\n",
      "Epoch 6/30\n",
      "15250/15250 [==============================] - 435s - loss: 15.6967 - acc: 0.0311 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0438 - mean_absolute_percentage_error: 6664.2270 - cosine_proximity: -1.2018e-04 - val_loss: 15.4086 - val_acc: 0.0280 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0433 - val_mean_absolute_percentage_error: 4613.4742 - val_cosine_proximity: -1.1652e-04\n",
      "Epoch 7/30\n",
      "15250/15250 [==============================] - 436s - loss: 15.6952 - acc: 0.0311 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0438 - mean_absolute_percentage_error: 6340.7832 - cosine_proximity: -1.2018e-04 - val_loss: 15.4285 - val_acc: 0.0270 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0436 - val_mean_absolute_percentage_error: 4667.1922 - val_cosine_proximity: -1.1506e-04\n",
      "Epoch 8/30\n",
      "15250/15250 [==============================] - 429s - loss: 15.6945 - acc: 0.0310 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0438 - mean_absolute_percentage_error: 6048.4821 - cosine_proximity: -1.2011e-04 - val_loss: 15.4222 - val_acc: 0.0353 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0435 - val_mean_absolute_percentage_error: 6841.6995 - val_cosine_proximity: -1.1527e-04\n",
      "Epoch 9/30\n",
      "15250/15250 [==============================] - 332s - loss: 15.6954 - acc: 0.0311 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0438 - mean_absolute_percentage_error: 5892.0923 - cosine_proximity: -1.2010e-04 - val_loss: 15.4150 - val_acc: 0.0315 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0434 - val_mean_absolute_percentage_error: 4103.3875 - val_cosine_proximity: -1.1602e-04\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(n_hidden_1, activation='relu', input_shape=(n_input,)))\n",
    "# we will add 2 more layers\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = customLoss, optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "                    validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "                    epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "                    validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 2nd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS-fun: <class 'tensorflow.python.framework.ops.Tensor'> None\n",
      "[OLD] Number of layers (except input layer, including output layer)->\n",
      " [<keras.layers.core.Dense object at 0x7f55d0834cc0>, <keras.layers.core.Dense object at 0x7f55d0834f60>]\n",
      "[NEW] Number of layers (except input layer, including output layer)->\n",
      " [<keras.layers.core.Dense object at 0x7f55d0834cc0>, <keras.layers.core.Dense object at 0x7f55cf84c518>, <keras.layers.core.Dense object at 0x7f55cc1c54e0>]\n",
      "LOSS-fun: <class 'tensorflow.python.framework.ops.Tensor'> None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2048)              2521088   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1926)              3946374   \n",
      "=================================================================\n",
      "Total params: 10,663,814\n",
      "Trainable params: 10,663,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "15250/15250 [==============================] - 332s - loss: 15.5864 - acc: 0.0264 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0428 - mean_absolute_percentage_error: 4280.0448 - cosine_proximity: -1.2764e-04 - val_loss: 15.2695 - val_acc: 0.0290 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0420 - val_mean_absolute_percentage_error: 2774.5538 - val_cosine_proximity: -1.2536e-04\n",
      "Epoch 2/30\n",
      "15250/15250 [==============================] - 359s - loss: 15.5583 - acc: 0.0267 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0425 - mean_absolute_percentage_error: 3630.7065 - cosine_proximity: -1.2891e-04 - val_loss: 15.2682 - val_acc: 0.0273 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0419 - val_mean_absolute_percentage_error: 2624.7578 - val_cosine_proximity: -1.2497e-04\n",
      "Epoch 3/30\n",
      "15250/15250 [==============================] - 360s - loss: 15.5547 - acc: 0.0265 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0425 - mean_absolute_percentage_error: 3600.6487 - cosine_proximity: -1.2906e-04 - val_loss: 15.2735 - val_acc: 0.0276 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0420 - val_mean_absolute_percentage_error: 2751.6488 - val_cosine_proximity: -1.2561e-04\n",
      "Epoch 4/30\n",
      "15250/15250 [==============================] - 363s - loss: 15.5545 - acc: 0.0268 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0425 - mean_absolute_percentage_error: 3512.2707 - cosine_proximity: -1.2921e-04 - val_loss: 15.2675 - val_acc: 0.0288 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2151.8352 - val_cosine_proximity: -1.2578e-04\n",
      "Epoch 5/30\n",
      "15250/15250 [==============================] - 310s - loss: 15.5505 - acc: 0.0274 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0425 - mean_absolute_percentage_error: 3442.5286 - cosine_proximity: -1.2938e-04 - val_loss: 15.2614 - val_acc: 0.0286 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2308.0587 - val_cosine_proximity: -1.2557e-04\n",
      "Epoch 6/30\n",
      "15250/15250 [==============================] - 303s - loss: 15.5518 - acc: 0.0271 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0425 - mean_absolute_percentage_error: 3420.9332 - cosine_proximity: -1.2941e-04 - val_loss: 15.2797 - val_acc: 0.0299 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0419 - val_mean_absolute_percentage_error: 2763.9357 - val_cosine_proximity: -1.2516e-04\n",
      "Epoch 7/30\n",
      "15250/15250 [==============================] - 354s - loss: 15.5526 - acc: 0.0276 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3415.7156 - cosine_proximity: -1.2941e-04 - val_loss: 15.2662 - val_acc: 0.0248 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0419 - val_mean_absolute_percentage_error: 2649.3382 - val_cosine_proximity: -1.2568e-04\n",
      "Epoch 8/30\n",
      "15250/15250 [==============================] - 357s - loss: 15.5505 - acc: 0.0281 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3404.6857 - cosine_proximity: -1.2935e-04 - val_loss: 15.2605 - val_acc: 0.0303 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0419 - val_mean_absolute_percentage_error: 2618.5225 - val_cosine_proximity: -1.2669e-04\n",
      "Epoch 9/30\n",
      "15250/15250 [==============================] - 317s - loss: 15.5487 - acc: 0.0280 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3370.8419 - cosine_proximity: -1.2944e-04 - val_loss: 15.2724 - val_acc: 0.0284 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 3035.9230 - val_cosine_proximity: -1.2421e-04\n",
      "Epoch 10/30\n",
      "15250/15250 [==============================] - 302s - loss: 15.5503 - acc: 0.0279 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3367.3743 - cosine_proximity: -1.2941e-04 - val_loss: 15.2544 - val_acc: 0.0296 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2165.5272 - val_cosine_proximity: -1.2689e-04\n",
      "Epoch 11/30\n",
      "15250/15250 [==============================] - 303s - loss: 15.5488 - acc: 0.0278 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3356.4353 - cosine_proximity: -1.2941e-04 - val_loss: 15.2640 - val_acc: 0.0290 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2007.2138 - val_cosine_proximity: -1.2532e-04\n",
      "Epoch 12/30\n",
      "15250/15250 [==============================] - 302s - loss: 15.5474 - acc: 0.0278 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3332.4620 - cosine_proximity: -1.2942e-04 - val_loss: 15.2663 - val_acc: 0.0272 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2170.3765 - val_cosine_proximity: -1.2605e-04\n",
      "Epoch 13/30\n",
      "15250/15250 [==============================] - 300s - loss: 15.5505 - acc: 0.0280 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3345.7090 - cosine_proximity: -1.2936e-04 - val_loss: 15.2702 - val_acc: 0.0283 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0421 - val_mean_absolute_percentage_error: 2495.7241 - val_cosine_proximity: -1.2482e-04\n",
      "Epoch 14/30\n",
      "15250/15250 [==============================] - 303s - loss: 15.5503 - acc: 0.0279 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0424 - mean_absolute_percentage_error: 3324.0474 - cosine_proximity: -1.2939e-04 - val_loss: 15.2697 - val_acc: 0.0294 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2479.1636 - val_cosine_proximity: -1.2553e-04\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL_FILE_2 = \"./dnn_models/py_model_2_\"+ Code_VERSION+\".h5\"\n",
    "model = load_model(SAVE_MODEL_FILE, custom_objects={'customLoss':customLoss})\n",
    "\n",
    "print('[OLD] Number of layers (except input layer, including output layer)->\\n', model.layers)\n",
    "\n",
    "model.pop()\n",
    "model.add(Dense(n_hidden_2, activation='relu'))\n",
    "# we will add 1 more layers\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "print('[NEW] Number of layers (except input layer, including output layer)->\\n', model.layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = customLoss, optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "                    validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "                    epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "                    validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS-fun: <class 'tensorflow.python.framework.ops.Tensor'> None\n",
      "[OLD] Number of layers (except input layer, including output layer)->\n",
      " [<keras.layers.core.Dense object at 0x7f52e46ac278>, <keras.layers.core.Dense object at 0x7f52e46ac4e0>, <keras.layers.core.Dense object at 0x7f52e47beb00>]\n",
      "[NEW] Number of layers (except input layer, including output layer)->\n",
      " [<keras.layers.core.Dense object at 0x7f52e46ac278>, <keras.layers.core.Dense object at 0x7f52e46ac4e0>, <keras.layers.core.Dense object at 0x7f52cc756eb8>, <keras.layers.core.Dense object at 0x7f52cc756518>]\n",
      "LOSS-fun: <class 'tensorflow.python.framework.ops.Tensor'> None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2048)              2521088   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1926)              3946374   \n",
      "=================================================================\n",
      "Total params: 14,860,166\n",
      "Trainable params: 14,860,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "15250/15250 [==============================] - 308s - loss: 15.5314 - acc: 0.0265 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0423 - mean_absolute_percentage_error: 3569.7275 - cosine_proximity: -1.3091e-04 - val_loss: 15.2380 - val_acc: 0.0260 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2321.1440 - val_cosine_proximity: -1.2847e-04\n",
      "Epoch 2/30\n",
      "15250/15250 [==============================] - 304s - loss: 15.5012 - acc: 0.0263 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3218.7374 - cosine_proximity: -1.3227e-04 - val_loss: 15.2396 - val_acc: 0.0275 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2542.8610 - val_cosine_proximity: -1.2802e-04\n",
      "Epoch 3/30\n",
      "15250/15250 [==============================] - 305s - loss: 15.4997 - acc: 0.0265 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3139.0593 - cosine_proximity: -1.3246e-04 - val_loss: 15.2267 - val_acc: 0.0275 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0416 - val_mean_absolute_percentage_error: 2050.9813 - val_cosine_proximity: -1.2932e-04\n",
      "Epoch 4/30\n",
      "15250/15250 [==============================] - 344s - loss: 15.4947 - acc: 0.0267 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3126.3974 - cosine_proximity: -1.3258e-04 - val_loss: 15.2438 - val_acc: 0.0276 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0415 - val_mean_absolute_percentage_error: 2363.5740 - val_cosine_proximity: -1.2840e-04\n",
      "Epoch 5/30\n",
      "15250/15250 [==============================] - 315s - loss: 15.4958 - acc: 0.0266 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3101.2042 - cosine_proximity: -1.3257e-04 - val_loss: 15.2336 - val_acc: 0.0288 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0415 - val_mean_absolute_percentage_error: 2187.6759 - val_cosine_proximity: -1.2887e-04\n",
      "Epoch 6/30\n",
      "15250/15250 [==============================] - 366s - loss: 15.4962 - acc: 0.0266 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3097.3664 - cosine_proximity: -1.3258e-04 - val_loss: 15.2324 - val_acc: 0.0284 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2266.3272 - val_cosine_proximity: -1.2791e-04\n",
      "Epoch 7/30\n",
      "15250/15250 [==============================] - 379s - loss: 15.4966 - acc: 0.0265 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3095.7343 - cosine_proximity: -1.3251e-04 - val_loss: 15.2379 - val_acc: 0.0261 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0415 - val_mean_absolute_percentage_error: 2042.8262 - val_cosine_proximity: -1.2872e-04\n",
      "Epoch 8/30\n",
      "15250/15250 [==============================] - 369s - loss: 15.4968 - acc: 0.0262 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3117.7997 - cosine_proximity: -1.3258e-04 - val_loss: 15.2221 - val_acc: 0.0249 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 2094.5092 - val_cosine_proximity: -1.2796e-04\n",
      "Epoch 9/30\n",
      "15250/15250 [==============================] - 380s - loss: 15.4942 - acc: 0.0261 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3139.6093 - cosine_proximity: -1.3256e-04 - val_loss: 15.2290 - val_acc: 0.0274 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0415 - val_mean_absolute_percentage_error: 2106.2396 - val_cosine_proximity: -1.2946e-04\n",
      "Epoch 10/30\n",
      "15250/15250 [==============================] - 377s - loss: 15.4974 - acc: 0.0262 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3104.8115 - cosine_proximity: -1.3259e-04 - val_loss: 15.2496 - val_acc: 0.0271 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2329.3043 - val_cosine_proximity: -1.2742e-04\n",
      "Epoch 11/30\n",
      "15250/15250 [==============================] - 379s - loss: 15.4947 - acc: 0.0267 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3069.1240 - cosine_proximity: -1.3248e-04 - val_loss: 15.2204 - val_acc: 0.0270 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0415 - val_mean_absolute_percentage_error: 2177.5558 - val_cosine_proximity: -1.2896e-04\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL_FILE_3 = \"./dnn_models/py_model_3_\"+ Code_VERSION+\".h5\"\n",
    "model = load_model(SAVE_MODEL_FILE_2, custom_objects={'customLoss':customLoss})\n",
    "\n",
    "print('[OLD] Number of layers (except input layer, including output layer)->\\n', model.layers)\n",
    "\n",
    "model.pop()\n",
    "model.add(Dense(n_hidden_2, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "print('[NEW] Number of layers (except input layer, including output layer)->\\n', model.layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = customLoss, optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "                    validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "                    epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "                    validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
