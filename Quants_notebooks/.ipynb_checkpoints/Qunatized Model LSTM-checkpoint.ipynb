{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Model LSTM\n",
    "\n",
    "Make clean quantized wavs to /data directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/knayem/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "import array\n",
    "\n",
    "import re\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(sys.executable)\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "\n",
    "from skimage.restoration import unwrap_phase\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helping Functions (a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rev(string, old, new, times=1):\n",
    "    '''\n",
    "    Replace a substring (old) with another substring (new) from a string (string) \n",
    "    in total a fixed number (times) of times.\n",
    "    '''\n",
    "    \n",
    "    ls = string.split(old)\n",
    "    length = len(ls)\n",
    "    \n",
    "    # times can be atmost (length-1)\n",
    "    times = times if (length-1)>=times else (length-1)\n",
    "    \n",
    "    new_string = old.join(ls[:length-times])\n",
    "    \n",
    "    for t in range(times,0,-1):\n",
    "        new_string = new.join([new_string,ls[length-t]])\n",
    "                               \n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variables (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .NPY FILE PATH\n",
    "FILE_SAVE_PATH = '/data/knayem/Quantized_DataFiles' # store .npy data file path for quick access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/data'\n",
    "USER_PATH = 'knayem'\n",
    "\n",
    "ROOT_USER_PATH = os.path.join(ROOT_PATH,USER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Dev, Test Folders for Clean and Mixs\n",
    "TRAIN_CLEAN_FOLDER = 'train_16k'\n",
    "DEV_CLEAN_FOLDER = 'dev_16k'\n",
    "TEST_CLEAN_FOLDER = 'test_16k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture folders -> SSN, Factory, Cafe, Babble\n",
    "SSN_MIXTURE_FOLDER = 'ssn'\n",
    "FACTORY_MIXTURE_FOLDER = 'factory'\n",
    "CAFE_MIXTURE_FOLDER = 'cafe'\n",
    "BABBLE_MIXTURE_FOLDER = 'babble'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEE MALE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEEE_MALE_CLEAN_PATH = os.path.join(ROOT_USER_PATH,'IEEE_male_clean_16k') # male\n",
    "IEEE_MALE_CLEAN_TRAIN_PATH = os.path.join(IEEE_MALE_CLEAN_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_CLEAN_DEV_PATH = os.path.join(IEEE_MALE_CLEAN_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_CLEAN_TEST_PATH = os.path.join(IEEE_MALE_CLEAN_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "IEEE_MALE_QUANT_CLEAN_PATH = os.path.join(ROOT_USER_PATH,'IEEE_male_clean_16k_Quantized')\n",
    "IEEE_MALE_QUANT_CLEAN_TRAIN_PATH = os.path.join(IEEE_MALE_QUANT_CLEAN_PATH, TRAIN_CLEAN_FOLDER) # train+QUANT_STEP\n",
    "IEEE_MALE_QUANT_CLEAN_DEV_PATH = os.path.join(IEEE_MALE_QUANT_CLEAN_PATH, DEV_CLEAN_FOLDER) # dev+QUANT_STEP\n",
    "IEEE_MALE_QUANT_CLEAN_TEST_PATH = os.path.join(IEEE_MALE_QUANT_CLEAN_PATH, TEST_CLEAN_FOLDER) # test+QUANT_STEP\n",
    "\n",
    "IEEE_MALE_MIX_PATH = os.path.join(ROOT_USER_PATH,'IEEE_male_mixture')\n",
    "IEEE_MALE_MIX_SSN_PATH = os.path.join(IEEE_MALE_MIX_PATH, SSN_MIXTURE_FOLDER)\n",
    "IEEE_MALE_MIX_FACTORY_PATH = os.path.join(IEEE_MALE_MIX_PATH, FACTORY_MIXTURE_FOLDER)\n",
    "IEEE_MALE_MIX_CAFE_PATH = os.path.join(IEEE_MALE_MIX_PATH, CAFE_MIXTURE_FOLDER)\n",
    "IEEE_MALE_MIX_BABBLE_PATH = os.path.join(IEEE_MALE_MIX_PATH, BABBLE_MIXTURE_FOLDER)\n",
    "\n",
    "#SSN\n",
    "IEEE_MALE_MIX_SSN_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_SSN_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_SSN_DEV_PATH = os.path.join(IEEE_MALE_MIX_SSN_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_SSN_TEST_PATH = os.path.join(IEEE_MALE_MIX_SSN_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "#Factory\n",
    "IEEE_MALE_MIX_FACTORY_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_FACTORY_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_FACTORY_DEV_PATH = os.path.join(IEEE_MALE_MIX_FACTORY_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_FACTORY_TEST_PATH = os.path.join(IEEE_MALE_MIX_FACTORY_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "#Cafe\n",
    "IEEE_MALE_MIX_CAFE_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_CAFE_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_CAFE_DEV_PATH = os.path.join(IEEE_MALE_MIX_CAFE_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_CAFE_TEST_PATH = os.path.join(IEEE_MALE_MIX_CAFE_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "#Babble\n",
    "IEEE_MALE_MIX_BABBLE_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_BABBLE_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_BABBLE_DEV_PATH = os.path.join(IEEE_MALE_MIX_BABBLE_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_BABBLE_TEST_PATH = os.path.join(IEEE_MALE_MIX_BABBLE_PATH, TEST_CLEAN_FOLDER) # test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path, /data\n",
      "\t|-> Root User path, /data/knayem\n",
      "\n",
      "IEEE Male Clean path, /data/knayem/IEEE_male_clean_16k\n",
      "\t|-> Train Clean .WAV path, /data/knayem/IEEE_male_clean_16k/train_16k\n",
      "\t|-> Dev Clean .WAV path, /data/knayem/IEEE_male_clean_16k/dev_16k\n",
      "\t|-> Test Clean .WAV path, /data/knayem/IEEE_male_clean_16k/test_16k\n",
      "\n",
      "IEEE Male Quantized Clean path, /data/knayem/IEEE_male_clean_16k_Quantized\n",
      "\t|-> Train Quantized Clean .WAV path, /data/knayem/IEEE_male_clean_16k_Quantized/train_16k\n",
      "\t|-> Dev Quantized Clean .WAV path, /data/knayem/IEEE_male_clean_16k_Quantized/dev_16k\n",
      "\t|-> Test Quantized Clean .WAV path, /data/knayem/IEEE_male_clean_16k_Quantized/test_16k\n",
      "\n",
      "IEEE Male MIXTURE .WAV path, /data/knayem/IEEE_male_mixture\n",
      "\t|-> Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn\n",
      "\t\t|-> Train Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn/train_16k\n",
      "\t\t|-> Dev Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn/dev_16k\n",
      "\t\t|-> Test Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn/test_16k\n",
      "\t|-> Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory\n",
      "\t\t|-> Train Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory/train_16k\n",
      "\t\t|-> Dev Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory/dev_16k\n",
      "\t\t|-> Test Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory/test_16k\n",
      "\t|->Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe\n",
      "\t\t|-> Train Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe/train_16k\n",
      "\t\t|-> Dev Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe/dev_16k\n",
      "\t\t|-> Test Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe/test_16k\n",
      "\t|->Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble\n",
      "\t\t|-> Train Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble/train_16k\n",
      "\t\t|-> Dev Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble/dev_16k\n",
      "\t\t|-> Test Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble/test_16k\n"
     ]
    }
   ],
   "source": [
    "print(\"Root path,\", ROOT_PATH)\n",
    "print(\"\\t|-> Root User path,\", ROOT_USER_PATH)\n",
    "print()\n",
    "print(\"IEEE Male Clean path,\", IEEE_MALE_CLEAN_PATH)\n",
    "print(\"\\t|-> Train Clean .WAV path,\", IEEE_MALE_CLEAN_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Clean .WAV path,\", IEEE_MALE_CLEAN_DEV_PATH)\n",
    "print(\"\\t|-> Test Clean .WAV path,\", IEEE_MALE_CLEAN_TEST_PATH)\n",
    "print()\n",
    "print(\"IEEE Male Quantized Clean path,\", IEEE_MALE_QUANT_CLEAN_PATH)\n",
    "print(\"\\t|-> Train Quantized Clean .WAV path,\", IEEE_MALE_QUANT_CLEAN_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Quantized Clean .WAV path,\", IEEE_MALE_QUANT_CLEAN_DEV_PATH)\n",
    "print(\"\\t|-> Test Quantized Clean .WAV path,\", IEEE_MALE_QUANT_CLEAN_TEST_PATH)\n",
    "print()\n",
    "print(\"IEEE Male MIXTURE .WAV path,\", IEEE_MALE_MIX_PATH)\n",
    "print(\"\\t|-> Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_PATH)\n",
    "print(\"\\t\\t|-> Train Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_TEST_PATH)\n",
    "\n",
    "print(\"\\t|-> Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_PATH)\n",
    "print(\"\\t\\t|-> Train Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_TEST_PATH)\n",
    "\n",
    "print(\"\\t|->Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_PATH)\n",
    "print(\"\\t\\t|-> Train Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_TEST_PATH)\n",
    "\n",
    "print(\"\\t|->Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_PATH)\n",
    "print(\"\\t\\t|-> Train Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEE FEMALE Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IEEE_FEMALE_CORPORA_PATH = os.path.join(ROOT_PATH,'SpeechCorpora/IEEE_female') # female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summaray"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Root path,\", ROOT_PATH)\n",
    "print(\"\\t|-> Root User path,\", ROOT_USER_PATH)\n",
    "print()\n",
    "print(\"IEEE Male Data Corpora path,\", IEEE_MALE_CORPORA_PATH)\n",
    "print(\"IEEE Female Data Corpora path,\", IEEE_FEMALE_CORPORA_PATH)\n",
    "print()\n",
    "print(\"Clean .WAV path,\", CLEAN_wavs_PATH)\n",
    "print(\"\\t|-> Train Clean .WAV path,\", CLEAN_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Clean .WAV path,\", CLEAN_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Clean .WAV path,\", CLEAN_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix SSN .WAV path,\", SSN_wavs_PATH)\n",
    "print(\"\\t|-> Train Mix SSN .WAV path,\", SSN_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix SSN .WAV path,\", SSN_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix SSN .WAV path,\", SSN_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix CAFE .WAV path,\", CAFE_MIXTURE_PATH)\n",
    "print(\"\\t|-> Train Mix CAFE .WAV path,\", CAFE_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix CAFE .WAV path,\", CAFE_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix CAFE .WAV path,\", CAFE_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix BABBLE .WAV path,\", BABBLE_MIXTURE_PATH)\n",
    "print(\"\\t|-> Train Mix BABBLE .WAV path,\", BABBLE_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix BABBLE .WAV path,\", BABBLE_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix BABBLE .WAV path,\", BABBLE_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix FACTORY .WAV path,\", FACTORY_MIXTURE_PATH)\n",
    "print(\"\\t|-> Train Mix FACTORY .WAV path,\", FACTORY_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix FACTORY .WAV path,\", FACTORY_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix FACTORY .WAV path,\", FACTORY_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Enhanced .WAV path,\", Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> SSN Enhanced .WAV path,\", SSN_Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> FACTORY Enhanced .WAV path,\", FACTORY_Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> BABBLE Enhanced .WAV path,\", BABBLE_Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> FACTORY Enhanced .WAV path,\", FACTORY_Enhanced_wavs_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMIT Dataset "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# .NPY FILE PATH\n",
    "FILE_SAVE_PATH = '/data/knayem/TIMIT_DataFiles'\n",
    "\n",
    "# SSN PATH\n",
    "SSN_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/ssn'\n",
    "\n",
    "# CAFE PATH\n",
    "CAFE_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/cafe'\n",
    "\n",
    "# BABBLE PATH\n",
    "BABBLE_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/babble'\n",
    "\n",
    "# FACTORY PATH\n",
    "FACTORY_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/factory'\n",
    "\n",
    "\n",
    "# Train, Dev, Test\n",
    "TRAIN_MIXTURE_PATH = 'train_16k'\n",
    "DEV_MIXTURE_PATH = 'dev_16k'\n",
    "TEST_MIXTURE_PATH = 'test_16k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PATH = os.path.join(CLEAN_PATH,TRAIN_CLEAN_PATH) # clean train\n",
    "# PATH = os.path.join(CLEAN_PATH,DEV_CLEAN_PATH) # clean dev\n",
    "PATH = os.path.join(CLEAN_PATH,TEST_CLEAN_PATH) # clean test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a Parameters \n",
    "\n",
    "Followings are the basic parameter for calculating STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 640, noverlap: 320, nfft: 640, fs: 16000, hop_length: 320\n"
     ]
    }
   ],
   "source": [
    "fs = int(16e3)\n",
    "\n",
    "n_fft = 640\n",
    "win_length = int(40e-3*fs) # librosa needs scalar value\n",
    "overlap = int(20e-3*fs)\n",
    "hop_length = win_length - overlap # librosa needs scalar value\n",
    "\n",
    "NUMS_PRINTS = 10\n",
    "\n",
    "print('window: {0}, noverlap: {1}, nfft: {2}, fs: {3}, hop_length: {4}'.\n",
    "      format(win_length,overlap,n_fft,fs,hop_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b STFT function\n",
    "\n",
    "Calculate Magnitude and Group Delay of the PATH (train, dev, test of IEEE/TIMIT) to get an overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_gd_phase(filename, fs, n_fft, hop_length, win_length):\n",
    "    \n",
    "    y, sr = librosa.load(filename, sr=fs)\n",
    "    s_stft = librosa.stft(y,n_fft,hop_length,win_length)\n",
    "    mag, phase = librosa.magphase(s_stft)\n",
    "    angle = np.angle(phase)\n",
    "\n",
    "    unwrap_angle = np.unwrap(angle, axis=0) # freq, MATLAB implementation\n",
    "    unwrap_angle_s = np.roll(unwrap_angle, 1, axis=0) # roll across freq\n",
    "    unwrap_GD = np.angle(np.exp(1j*(unwrap_angle - unwrap_angle_s))) # paper implementation\n",
    "\n",
    "    return len(y), mag, unwrap_GD, phase, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fixed step Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_AMP, MIN_AMP = 100, 0\n",
    "\n",
    "QUANTIZED_DIRECTORY_TAG = \"Quantized\"\n",
    "Fixed_Step_Quantization_TAG = \"FS\"\n",
    "\n",
    "QUANT_STEP = 0.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_val(val, quant_boundary):\n",
    "    \n",
    "    proximity = abs(quant_boundary-val)\n",
    "    closest_boundary_index = np.argmin(proximity)\n",
    "    return quant_boundary[closest_boundary_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_indx(val, quant_boundary):\n",
    "    \n",
    "    proximity = abs(quant_boundary-val)\n",
    "    return np.argmin(proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_matrix(matrix, QUANT_STEP, MAX_AMP=100,MIN_AMP=0):\n",
    "    \n",
    "    quant_boundary = np.linspace(MIN_AMP,MAX_AMP,MAX_AMP//QUANT_STEP)\n",
    "    m_shape = matrix.shape\n",
    "    \n",
    "    quantized_list = [quantized_val(v,quant_boundary) for row in matrix for v in row]\n",
    "    return np.array(quantized_list).reshape(m_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_indx_matrix(matrix, QUANT_STEP, MAX_AMP=100,MIN_AMP=0):\n",
    "    \n",
    "    quant_boundary = np.linspace(MIN_AMP,MAX_AMP,MAX_AMP//QUANT_STEP)\n",
    "    m_shape = matrix.shape\n",
    "    \n",
    "    quantized_list = [quantized_indx(v,quant_boundary) for row in matrix for v in row]\n",
    "    return np.array(quantized_list).reshape(m_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def save_enhanced(mag, phase, fs, n_fft, hop_length, win_length, target_directory, filename, tags=None):\n",
    "    \n",
    "    D = mag*phase\n",
    "    enhanced = librosa.istft(D,hop_length,win_length)\n",
    "    \n",
    "    # enhanced filename creation\n",
    "    name = filename.split('.')[0]\n",
    "    \n",
    "    if tags is not None:\n",
    "        if 'quantization_tag' in tags:\n",
    "            name = \"_\".join([name,tags['quantization_tag'],str(tags['step'])])\n",
    "        if 'avg_step' in tags:\n",
    "            name = \"_\".join([name,str(tags['avg_step'])])\n",
    "\n",
    "    name = \".\".join([name,\"wav\"])\n",
    "\n",
    "    \n",
    "    # directory creation   \n",
    "    if not os.path.exists(target_directory):\n",
    "        print(False,target_directory)\n",
    "        os.makedirs(target_directory)\n",
    "    else:\n",
    "        print(True,target_directory)\n",
    "        pass\n",
    "    \n",
    "    wav_filepath = os.path.join(target_directory,name)\n",
    "                                       \n",
    "    # save file\n",
    "    sf.write(wav_filepath, enhanced, int(fs))\n",
    "    #print(wav_filepath)\n",
    "    \n",
    "    return wav_filepath\n",
    "                                       "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpora_path_list = [CLEAN_wavs_PATH]\n",
    "\n",
    "# [0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125 0.00390625]\n",
    "QUANT_STEP_LIST = [0.0625]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for QUANT_STEP in QUANT_STEP_LIST:\n",
    "    \n",
    "    for enum1, corpora in enumerate(sorted(corpora_path_list)) :\n",
    "        print(enum1,\"CORPORA:\", corpora)\n",
    "        QUANTIZED_DIRECTORY = corpora+\"_\"+QUANTIZED_DIRECTORY_TAG\n",
    "\n",
    "        for root, dirs, files in os.walk(corpora): \n",
    "            # .wav files only\n",
    "            wav_files = list( filter(lambda x: x.split('.')[-1] == 'wav', files) )\n",
    "            print(\"ROOT:\",root, \", len(DIR):\", len(dirs), \", len(FILES):\",len(wav_files),root.split('/')[-1])\n",
    "            \n",
    "            # folder name\n",
    "            if len(dirs)==0:\n",
    "                folder_name = root.split('/')[-1]\n",
    "                QUANTIZED_DIRECTORY_PATH = os.path.join(QUANTIZED_DIRECTORY,folder_name)\n",
    "                QUANTIZED_DIRECTORY_PATH = \"_\".join([QUANTIZED_DIRECTORY_PATH,Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "\n",
    "                npy_list = []\n",
    "#                 plt.figure()\n",
    "                \n",
    "            for enum2, filename in enumerate(sorted(wav_files)):\n",
    "                clean_wav_full_path = os.path.join(root, filename)\n",
    "                                                   \n",
    "                len_y, mag, unwrap_GD, phase, angle = mag_gd_phase(clean_wav_full_path, fs, n_fft, hop_length, win_length)\n",
    "                quantized_mag = quantized_matrix(mag, QUANT_STEP, MAX_AMP, MIN_AMP)\n",
    "\n",
    "                diff_mag = abs(mag-quantized_mag)\n",
    "                total_diff = np.sum(diff_mag)\n",
    "#                 print(enum2,\"|Error| = \", total_diff)\n",
    "\n",
    "                D = librosa.amplitude_to_db(mag, ref=np.max)\n",
    "                q_D = librosa.amplitude_to_db(quantized_mag, ref=np.max)\n",
    "\n",
    "                quant_wav_full_path = save_enhanced(quantized_mag, phase, fs, n_fft, hop_length, win_length, \n",
    "                                                    QUANTIZED_DIRECTORY_PATH, filename,\n",
    "                                                    {'quantization_tag':Fixed_Step_Quantization_TAG,'step':QUANT_STEP})\n",
    "\n",
    "                print(clean_wav_full_path,\"<->\",quant_wav_full_path)\n",
    "                npy_list.append( [filename, clean_wav_full_path, len_y, mag.shape[1]])\n",
    "                \n",
    "                \n",
    "                # plot the spectrogram\n",
    "#                 plt.subplot(len(wav_files), enum2+1, 1)\n",
    "#                 plt.subplot(3, 1, enum2+1)\n",
    "#                 librosa.display.specshow(D, y_axis='hz', x_axis='time', sr=fs)\n",
    "#                 plt.colorbar(format='%+2.0f dB')\n",
    "#                 plt.title(\":\".join([str(enum2),'mag',filename]))\n",
    "#                 plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "#                 plt.subplot(3, 2, enum2+1)\n",
    "#                 librosa.display.specshow(q_D, y_axis='hz', x_axis='time', sr=fs)\n",
    "#                 plt.colorbar(format='%+2.0f dB')\n",
    "#                 plt.title(\":\".join([str(enum2),'quant-mag',quant_wav_full_path.split('/')[-1]]))\n",
    "#                 plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "#                 plt.subplot(3, 3, enum2+1)\n",
    "#                 librosa.display.specshow(librosa.amplitude_to_db(librosa.amplitude_to_db(diff_mag, ref=np.max), ref=np.max), y_axis='hz', x_axis='time', sr=fs)\n",
    "#                 plt.colorbar(format='%+2.0f dB')\n",
    "#                 plt.title(\":\".join([str(enum2),'|Error|',str(total_diff)]))\n",
    "#                 plt.subplots_adjust(hspace=0.5)\n",
    "                \n",
    "#                 plt.draw()\n",
    "                \n",
    "#                 if enum2>=10:\n",
    "#                     break\n",
    "                    \n",
    "            if len(dirs)==0:    \n",
    "                npy_path = os.path.join(FILE_SAVE_PATH,QUANTIZED_DIRECTORY_PATH.split('/')[-1])\n",
    "#                 plt_path = os.path.join(FILE_SAVE_PATH,QUANTIZED_DIRECTORY_PATH.split('/')[-1]+\".pdf\")\n",
    "\n",
    "                np.save(npy_path, npy_list)\n",
    "#                 plt.savefig(plt_path,bbox_inches='tight')\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_NPY(npy_list, todo_list):\n",
    "    '''\n",
    "    npy_list[0]: clean file name\n",
    "    npy_list[1]: full clean .wav path *\n",
    "    npy_list[2]: frame length in time domain\n",
    "    npy_list[3]: time frame length in freq domain *\n",
    "    '''\n",
    "    if 'CLEAN_QUANT_pair' in todo_list:\n",
    "        CLEAN_QUANT_Pair_DICT = dict()\n",
    "        quant_step = str(0.0625)\n",
    "        quant_tag = \"FS\"\n",
    "    if 'CLEAN_FullClean_pair' in todo_list:\n",
    "        CLEAN_FullClean_Pair_DICT = dict()\n",
    "    if 'CLEAN_tfLen_pair' in todo_list:\n",
    "        CLEAN_tfLen_Pair_DICT = dict()\n",
    "    \n",
    "    for entry in npy_list:\n",
    "        key = entry[0].split('.')[0]\n",
    "        \n",
    "        if 'CLEAN_QUANT_pair' in todo_list:\n",
    "            quant_full_path = entry[1].replace('clean_16k','clean_16k_Quantized')\n",
    "            quant_full_path = replace_rev(quant_full_path,'16k','_'.join(['16k',quant_tag,quant_step]),2)\n",
    "            CLEAN_QUANT_Pair_DICT[key] = quant_full_path\n",
    "            \n",
    "        if 'CLEAN_FullClean_pair' in todo_list:\n",
    "            CLEAN_FullClean_Pair_DICT[key] = entry[1]\n",
    "            \n",
    "        if 'CLEAN_tfLen_pair' in todo_list:\n",
    "            CLEAN_tfLen_Pair_DICT[key] = int(entry[3])\n",
    "            \n",
    "    return_dicts = []\n",
    "    if 'CLEAN_QUANT_pair' in todo_list:\n",
    "        return_dicts.append(CLEAN_QUANT_Pair_DICT)\n",
    "    if 'CLEAN_FullClean_pair' in todo_list:\n",
    "        return_dicts.append(CLEAN_FullClean_Pair_DICT)\n",
    "    if 'CLEAN_tfLen_pair' in todo_list:\n",
    "        return_dicts.append(CLEAN_tfLen_Pair_DICT)\n",
    "\n",
    "    return return_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/knayem/Quantized_DataFiles/train_16k_FS_0.0625'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_QUANT_LIST_NPY_PATH = \"_\".join([os.path.join(FILE_SAVE_PATH,TRAIN_CLEAN_FOLDER),\n",
    "                                      Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "DEV_QUANT_LIST_NPY_PATH = \"_\".join([os.path.join(FILE_SAVE_PATH,DEV_CLEAN_FOLDER),\n",
    "                                      Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "TEST_QUANT_LIST_NPY_PATH = \"_\".join([os.path.join(FILE_SAVE_PATH,TEST_CLEAN_FOLDER),\n",
    "                                      Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "\n",
    "\n",
    "TRAIN_QUANT_LIST = np.load(TRAIN_QUANT_LIST_NPY_PATH+\".npy\")\n",
    "DEV_QUANT_LIST = np.load(DEV_QUANT_LIST_NPY_PATH+\".npy\")\n",
    "TEST_QUANT_LIST = np.load(TEST_QUANT_LIST_NPY_PATH+\".npy\")\n",
    "\n",
    "TRAIN_QUANT_LIST[0]\n",
    "TRAIN_QUANT_LIST_NPY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key -> clean .wav filename\n",
    "# value -> full path of Clean Quantized .wav (Pair_CLEAN_QUAT_DICT)\n",
    "#       -> number of Freq frame (Pair_CLEAN_FreqFrame_DICT)\n",
    "\n",
    "todo_list = {'CLEAN_QUANT_pair','CLEAN_FullClean_pair','CLEAN_tfLen_pair'}\n",
    "Train_CLEAN_QUANT_Pair_DICT, Train_CLEAN_FullClean_Pair_DICT, Train_CLEAN_FreqFrame_Pair_DICT = process_NPY(TRAIN_QUANT_LIST,todo_list)\n",
    "Dev_CLEAN_QUANT_Pair_DICT, Dev_CLEAN_FullClean_Pair_DICT, Dev_CLEAN_FreqFrame_Pair_DICT = process_NPY(DEV_QUANT_LIST,todo_list)\n",
    "Test_CLEAN_QUANT_Pair_DICT, Test_CLEAN_FullClean_Pair_DICT, Test_CLEAN_FreqFrame_Pair_DICT = process_NPY(TEST_QUANT_LIST,todo_list)\n",
    "\n",
    "all_pair_dictionaries={'train':[Train_CLEAN_QUANT_Pair_DICT, Train_CLEAN_FullClean_Pair_DICT, Train_CLEAN_FreqFrame_Pair_DICT],\n",
    "          'dev':[Dev_CLEAN_QUANT_Pair_DICT, Dev_CLEAN_FullClean_Pair_DICT, Dev_CLEAN_FreqFrame_Pair_DICT],\n",
    "           'test':[Test_CLEAN_QUANT_Pair_DICT, Test_CLEAN_FullClean_Pair_DICT, Test_CLEAN_FreqFrame_Pair_DICT]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Test_CLEAN_FreqFrame_Pair_DICT.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Functions "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def group_all_files(src_folder_list):\n",
    "    '''\n",
    "    parameters:\n",
    "        <src_folder_list> = list of folders from which .wav files will be used\n",
    "        \n",
    "    returns:\n",
    "        <all_file_list> = a list of tuple (key,full_file_path), \n",
    "                            key-> 1st 11 char of the filename,\n",
    "    '''\n",
    "    \n",
    "    all_file_list = []\n",
    "    \n",
    "    for enum1, src in enumerate(src_folder_list) :\n",
    "        #print(enum1,\"SRC:\", src)\n",
    "        \n",
    "        for root, dirs, files in os.walk(src): \n",
    "            #print(\"ROOT:\",root, \", len(DIR):\", len(dirs), \", len(FILES):\",len(files))\n",
    "\n",
    "            for enum2, filename in enumerate(sorted(files)):\n",
    "                key = filename[:11]\n",
    "                all_file_list.append( (key, \"/\".join([root,filename])) )\n",
    "                \n",
    "    return all_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, src_folders_list, pair_dictionaries, batch_size=32, freq_frames_len = 321,\n",
    "                 max_time_frame = 200, n_channels=1, n_classes=1600, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.list_IDs = self.group_all_files(src_folder_list)\n",
    "        \n",
    "        self.cleanQuant_pair_dict = pair_dictionaries[0]\n",
    "        self.cleanFullClean_pair_dict = pair_dictionaries[1]\n",
    "        self.cleanFreqFrame_pair_dict = pair_dictionaries[2]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_time_frame = max_time_frame\n",
    "        self.dim = (freq_frames_len,max_time_frame)\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels)) #(batch_size, 321, 200, n_channels)\n",
    "        \n",
    "        y_cls = np.empty((*self.dim, self.n_classes), dtype=np.int32) #(321, 200, 1600)\n",
    "        y_cln = np.empty(self.dim, dtype=np.float32) #(321, 200)\n",
    "        \n",
    "        dt = np.dtype([ ('cls',np.int32,(*self.dim, self.n_classes)),('cln',np.float32,self.dim) ])\n",
    "        Y = np.empty( (self.batch_size,),dtype=dt ) #(batch_size,('q','c'))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for i, (key, x_path) in enumerate(list_IDs_temp):\n",
    "            _,x_mag,_,_,_ = mag_gd_phase(x_path, fs, n_fft, hop_length, win_length)\n",
    "            _,y_clean_mag,_,_,_ = mag_gd_phase(self.cleanFullClean_pair_dict[key], fs, n_fft, hop_length, win_length)\n",
    "            _,y_quant_mag,_,_,_ = mag_gd_phase(self.cleanQuant_pair_dict[key], fs, n_fft, hop_length, win_length)\n",
    "            \n",
    "            #input features\n",
    "            x = pad_sequences(x_mag, dtype='float32', maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "            #ground truth mag value\n",
    "            y_clean = pad_sequences(y_clean_mag,\n",
    "                                    dtype='float32', maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "            #ground truth quantized catagory\n",
    "            y_quant = pad_sequences(quantized_indx_matrix(y_quant_mag, QUANT_STEP=0.0625),\n",
    "                                    maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "\n",
    "            y_class = to_categorical(y_quant, num_classes=self.n_classes)\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,] = x\n",
    "\n",
    "            # Store class\n",
    "            Y[i] = (y_class,y_clean)\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    \n",
    "    def group_all_files(self, src_folder_list):\n",
    "        '''\n",
    "        parameters:\n",
    "            <src_folder_list> = list of folders from which .wav files will be used\n",
    "\n",
    "        returns:\n",
    "            <all_file_list> = a list of tuple (key,full_file_path), \n",
    "                                key-> 1st 11 char of the filename,\n",
    "        '''\n",
    "        all_file_list = []\n",
    "\n",
    "        for enum1, src in enumerate(src_folder_list) :\n",
    "            for root, dirs, files in os.walk(src): \n",
    "                for enum2, filename in enumerate(sorted(files)):\n",
    "                    key = filename[:11]\n",
    "                    all_file_list.append( (key, \"/\".join([root,filename])) )\n",
    "\n",
    "        return all_file_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def next_batch(src_folder_list, batch_size, max_time_frames, TASK=\"TRAIN\"):\n",
    "    '''\n",
    "    parameters:\n",
    "        <src_folder_list> = list of folders from which .wav files will be used\n",
    "        <batch_size>\n",
    "        <max_time_frames>\n",
    "        <TASK>\n",
    "        \n",
    "    returns:\n",
    "        x, y\n",
    "    '''\n",
    "    \n",
    "    all_file_list = group_all_files(src_folder_list)\n",
    "    permuted_sequence = np.random.permutation(len(all_file_list))\n",
    "    \n",
    "    for indx in permuted_sequence:\n",
    "        key, x_path = all_file_list[indx]\n",
    "        \n",
    "        if TASK==\"TRAIN\":\n",
    "            y_quant_path = Train_CLEAN_QUANT_Pair_DICT[key]\n",
    "            y_clean_path = Train_CLEAN_FullClean_Pair_DICT[key]\n",
    "        elif TASK==\"DEV\":\n",
    "            y_quant_path = Dev_CLEAN_QUANT_Pair_DICT[key]\n",
    "            y_clean_path = Dev_CLEAN_FullClean_Pair_DICT[key]\n",
    "        elif TASK==\"TEST\":\n",
    "            y_quant_path = Test_CLEAN_QUANT_Pair_DICT[key]\n",
    "            y_clean_path = Test_CLEAN_FullClean_Pair_DICT[key]\n",
    "            \n",
    "        print(x_path,'\\n',y_quant_path,'\\n',y_clean_path)\n",
    "        #len(y), mag, unwrap_GD, phase, angle = mag_gd_phase(filename, fs, n_fft, hop_length, win_length)\n",
    "        _,x_mag,_,_,_ = mag_gd_phase(x_path, fs, n_fft, hop_length, win_length)\n",
    "        _,y_quant_mag,_,_,_ = mag_gd_phase(y_quant_path, fs, n_fft, hop_length, win_length)\n",
    "        _,y_clean_mag,_,_,_ = mag_gd_phase(y_clean_path, fs, n_fft, hop_length, win_length)\n",
    "        \n",
    "        #input features\n",
    "        x = pad_sequences(x_mag, dtype='float32', maxlen=max_time_frames, padding='post', truncating='post')\n",
    "        #ground truth mag value\n",
    "        y_clean = pad_sequences(y_clean_mag,\n",
    "                                dtype='float32', maxlen=max_time_frames, padding='post', truncating='post')\n",
    "        #ground truth quantized catagory\n",
    "        y_quant = pad_sequences(quantized_indx_matrix(y_quant_mag, QUANT_STEP=0.0625),\n",
    "                                maxlen=max_time_frames, padding='post', truncating='post')\n",
    "        \n",
    "        y_class = to_categorical(y_quant, num_classes=int((MAX_AMP-MIN_AMP)//QUANT_STEP))\n",
    "        \n",
    "        print(x.shape,y_quant.shape, y_class.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'y_quant_path' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-b157d7fe7229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msrc_folder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mIEEE_MALE_MIX_SSN_TRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIEEE_MALE_MIX_FACTORY_TRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIEEE_MALE_MIX_CAFE_TRAIN_PATH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_pair_dictionaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_TIME_FRAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-ca7bf5526dee>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(src_folder_list, batch_size, max_time_frames, TASK)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0my_clean_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTest_CLEAN_FullClean_Pair_DICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_quant_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_clean_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#len(y), mag, unwrap_GD, phase, angle = mag_gd_phase(filename, fs, n_fft, hop_length, win_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_mag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmag_gd_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'y_quant_path' referenced before assignment"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_TIME_FRAMES = 200\n",
    "\n",
    "src_folder_list = [IEEE_MALE_MIX_SSN_TRAIN_PATH,IEEE_MALE_MIX_FACTORY_TRAIN_PATH,IEEE_MALE_MIX_CAFE_TRAIN_PATH]\n",
    "\n",
    "matrix = next_batch(src_folder_list,BATCH_SIZE,MAX_TIME_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knayem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(MIN_AMP,MAX_AMP,MAX_AMP//QUANT_STEP).shape[0]\n",
    "# MAX_AMP//QUANT_STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knayem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "group_all_files() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-103dba78b15f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mdev_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdev_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-d0cffc27cfd4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_folders_list, pair_dictionaries, batch_size, freq_frames_len, max_time_frame, n_channels, n_classes, shuffle)\u001b[0m\n\u001b[1;32m      4\u001b[0m                  n_classes=1600, shuffle=True):\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'Initialization'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_IDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_all_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanQuant_pair_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_dictionaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: group_all_files() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "FRQ_FRAMES_LEN = 321\n",
    "MAX_TIME_FRAMES = 200\n",
    "\n",
    "NUM_CLASSES = np.linspace(MIN_AMP,MAX_AMP,MAX_AMP//QUANT_STEP).shape[0]\n",
    "\n",
    "RNN_UNITS = 256\n",
    "\n",
    "\n",
    "# Parameters\n",
    "train_params = {'src_folders_list': [IEEE_MALE_MIX_SSN_TRAIN_PATH,IEEE_MALE_MIX_FACTORY_TRAIN_PATH,IEEE_MALE_MIX_CAFE_TRAIN_PATH],\n",
    "          'pair_dictionaries': all_pair_dictionaries['train'],\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'freq_frames_len': FRQ_FRAMES_LEN,\n",
    "          'max_time_frame': MAX_TIME_FRAMES,\n",
    "          'n_channels': 1,\n",
    "          'n_classes': NUM_CLASSES,\n",
    "          'shuffle': True}\n",
    "\n",
    "dev_params = {'src_folders_list': [IEEE_MALE_MIX_SSN_DEV_PATH,IEEE_MALE_MIX_FACTORY_DEV_PATH,IEEE_MALE_MIX_CAFE_DEV_PATH],\n",
    "          'pair_dictionaries': all_pair_dictionaries['dev'],\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'freq_frames_len': FRQ_FRAMES_LEN,\n",
    "          'max_time_frame': MAX_TIME_FRAMES,\n",
    "          'n_channels': 1,\n",
    "          'n_classes': NUM_CLASSES,\n",
    "          'shuffle': True}\n",
    "\n",
    "test_params = {'src_folders_list': [IEEE_MALE_MIX_SSN_TEST_PATH,IEEE_MALE_MIX_FACTORY_TEST_PATH,IEEE_MALE_MIX_CAFE_TEST_PATH],\n",
    "          'pair_dictionaries': all_pair_dictionaries['test'],\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'freq_frames_len': FRQ_FRAMES_LEN,\n",
    "          'max_time_frame': MAX_TIME_FRAMES,\n",
    "          'n_channels': 1,\n",
    "          'n_classes': NUM_CLASSES,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "train_generator = DataGenerator(**train_params)\n",
    "dev_generator = DataGenerator(**dev_params)\n",
    "test_generator = DataGenerator(**test_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 32, 256)           591872    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 32, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 32, 1600)          411200    \n",
      "=================================================================\n",
      "Total params: 1,528,384\n",
      "Trainable params: 1,528,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-323910d2e961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Set callback functions to early stop training and save the best model so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n\u001b[0;32m---> 17\u001b[0;31m              \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMODEL_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m              LossFunctionCallback(model,opts)]\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "BatchNormalization()\n",
    "model.add(LSTM(RNN_UNITS, return_sequences=True, input_shape=(MAX_TIME_FRAMES,FRQ_LEN)))\n",
    "BatchNormalization()\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(RNN_UNITS, return_sequences=True))\n",
    "\n",
    "BatchNormalization()\n",
    "model.add(TimeDistributed(Dense(units=1600, activation='linear')))\n",
    "print(model.summary())\n",
    "#model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True),\n",
    "             LossFunctionCallback(model,opts)]\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='train'), \n",
    "                    validation_data=next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train_files//batch_size, \n",
    "                    validation_steps=n_dev_files//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
