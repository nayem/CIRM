{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Model LSTM\n",
    "\n",
    "Make clean quantized wavs to /data directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/knayem/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/knayem/anaconda3/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knayem/anaconda3/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version: 2.3.1\n",
      "Tensorflow version: 2.1.0 , tf keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "import array\n",
    "\n",
    "import re\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(sys.executable)\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "\n",
    "from skimage.restoration import unwrap_phase\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, TensorBoard\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print('keras version:', keras.__version__)\n",
    "print('Tensorflow version:', tf.__version__,', tf keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helping Functions (a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rev(string, old, new, times=1):\n",
    "    '''\n",
    "    Replace a substring (old) with another substring (new) from a string (string) \n",
    "    in total a fixed number (times) of times.\n",
    "    '''\n",
    "    \n",
    "    ls = string.split(old)\n",
    "    length = len(ls)\n",
    "    \n",
    "    # times can be atmost (length-1)\n",
    "    times = times if (length-1)>=times else (length-1)\n",
    "    \n",
    "    new_string = old.join(ls[:length-times])\n",
    "    \n",
    "    for t in range(times,0,-1):\n",
    "        new_string = new.join([new_string,ls[length-t]])\n",
    "                               \n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variables (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .NPY FILE PATH\n",
    "FILE_SAVE_PATH = '/data/knayem/Quantized_DataFiles' # store .npy data file path for quick access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/data'\n",
    "USER_PATH = 'knayem'\n",
    "\n",
    "ROOT_USER_PATH = os.path.join(ROOT_PATH,USER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Dev, Test Folders for Clean and Mixs\n",
    "TRAIN_CLEAN_FOLDER = 'train_16k'\n",
    "DEV_CLEAN_FOLDER = 'dev_16k'\n",
    "TEST_CLEAN_FOLDER = 'test_16k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture folders -> SSN, Factory, Cafe, Babble\n",
    "SSN_MIXTURE_FOLDER = 'ssn'\n",
    "FACTORY_MIXTURE_FOLDER = 'factory'\n",
    "CAFE_MIXTURE_FOLDER = 'cafe'\n",
    "BABBLE_MIXTURE_FOLDER = 'babble'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEE MALE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEEE_MALE_CLEAN_PATH = os.path.join(ROOT_USER_PATH,'IEEE_male_clean_16k') # male\n",
    "IEEE_MALE_CLEAN_TRAIN_PATH = os.path.join(IEEE_MALE_CLEAN_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_CLEAN_DEV_PATH = os.path.join(IEEE_MALE_CLEAN_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_CLEAN_TEST_PATH = os.path.join(IEEE_MALE_CLEAN_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "IEEE_MALE_QUANT_CLEAN_PATH = os.path.join(ROOT_USER_PATH,'IEEE_male_clean_16k_Quantized')\n",
    "IEEE_MALE_QUANT_CLEAN_TRAIN_PATH = os.path.join(IEEE_MALE_QUANT_CLEAN_PATH, TRAIN_CLEAN_FOLDER) # train+QUANT_STEP\n",
    "IEEE_MALE_QUANT_CLEAN_DEV_PATH = os.path.join(IEEE_MALE_QUANT_CLEAN_PATH, DEV_CLEAN_FOLDER) # dev+QUANT_STEP\n",
    "IEEE_MALE_QUANT_CLEAN_TEST_PATH = os.path.join(IEEE_MALE_QUANT_CLEAN_PATH, TEST_CLEAN_FOLDER) # test+QUANT_STEP\n",
    "\n",
    "IEEE_MALE_MIX_PATH = os.path.join(ROOT_USER_PATH,'IEEE_male_mixture')\n",
    "IEEE_MALE_MIX_SSN_PATH = os.path.join(IEEE_MALE_MIX_PATH, SSN_MIXTURE_FOLDER)\n",
    "IEEE_MALE_MIX_FACTORY_PATH = os.path.join(IEEE_MALE_MIX_PATH, FACTORY_MIXTURE_FOLDER)\n",
    "IEEE_MALE_MIX_CAFE_PATH = os.path.join(IEEE_MALE_MIX_PATH, CAFE_MIXTURE_FOLDER)\n",
    "IEEE_MALE_MIX_BABBLE_PATH = os.path.join(IEEE_MALE_MIX_PATH, BABBLE_MIXTURE_FOLDER)\n",
    "\n",
    "#SSN\n",
    "IEEE_MALE_MIX_SSN_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_SSN_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_SSN_DEV_PATH = os.path.join(IEEE_MALE_MIX_SSN_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_SSN_TEST_PATH = os.path.join(IEEE_MALE_MIX_SSN_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "#Factory\n",
    "IEEE_MALE_MIX_FACTORY_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_FACTORY_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_FACTORY_DEV_PATH = os.path.join(IEEE_MALE_MIX_FACTORY_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_FACTORY_TEST_PATH = os.path.join(IEEE_MALE_MIX_FACTORY_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "#Cafe\n",
    "IEEE_MALE_MIX_CAFE_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_CAFE_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_CAFE_DEV_PATH = os.path.join(IEEE_MALE_MIX_CAFE_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_CAFE_TEST_PATH = os.path.join(IEEE_MALE_MIX_CAFE_PATH, TEST_CLEAN_FOLDER) # test\n",
    "\n",
    "#Babble\n",
    "IEEE_MALE_MIX_BABBLE_TRAIN_PATH = os.path.join(IEEE_MALE_MIX_BABBLE_PATH, TRAIN_CLEAN_FOLDER) # train\n",
    "IEEE_MALE_MIX_BABBLE_DEV_PATH = os.path.join(IEEE_MALE_MIX_BABBLE_PATH, DEV_CLEAN_FOLDER) # dev\n",
    "IEEE_MALE_MIX_BABBLE_TEST_PATH = os.path.join(IEEE_MALE_MIX_BABBLE_PATH, TEST_CLEAN_FOLDER) # test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path, /data\n",
      "\t|-> Root User path, /data/knayem\n",
      "\n",
      "IEEE Male Clean path, /data/knayem/IEEE_male_clean_16k\n",
      "\t|-> Train Clean .WAV path, /data/knayem/IEEE_male_clean_16k/train_16k\n",
      "\t|-> Dev Clean .WAV path, /data/knayem/IEEE_male_clean_16k/dev_16k\n",
      "\t|-> Test Clean .WAV path, /data/knayem/IEEE_male_clean_16k/test_16k\n",
      "\n",
      "IEEE Male Quantized Clean path, /data/knayem/IEEE_male_clean_16k_Quantized\n",
      "\t|-> Train Quantized Clean .WAV path, /data/knayem/IEEE_male_clean_16k_Quantized/train_16k\n",
      "\t|-> Dev Quantized Clean .WAV path, /data/knayem/IEEE_male_clean_16k_Quantized/dev_16k\n",
      "\t|-> Test Quantized Clean .WAV path, /data/knayem/IEEE_male_clean_16k_Quantized/test_16k\n",
      "\n",
      "IEEE Male MIXTURE .WAV path, /data/knayem/IEEE_male_mixture\n",
      "\t|-> Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn\n",
      "\t\t|-> Train Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn/train_16k\n",
      "\t\t|-> Dev Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn/dev_16k\n",
      "\t\t|-> Test Mix SSN .WAV path, /data/knayem/IEEE_male_mixture/ssn/test_16k\n",
      "\t|-> Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory\n",
      "\t\t|-> Train Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory/train_16k\n",
      "\t\t|-> Dev Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory/dev_16k\n",
      "\t\t|-> Test Mix FACTORY .WAV path, /data/knayem/IEEE_male_mixture/factory/test_16k\n",
      "\t|->Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe\n",
      "\t\t|-> Train Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe/train_16k\n",
      "\t\t|-> Dev Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe/dev_16k\n",
      "\t\t|-> Test Mix CAFE .WAV path, /data/knayem/IEEE_male_mixture/cafe/test_16k\n",
      "\t|->Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble\n",
      "\t\t|-> Train Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble/train_16k\n",
      "\t\t|-> Dev Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble/dev_16k\n",
      "\t\t|-> Test Mix BABBLE .WAV path, /data/knayem/IEEE_male_mixture/babble/test_16k\n"
     ]
    }
   ],
   "source": [
    "print(\"Root path,\", ROOT_PATH)\n",
    "print(\"\\t|-> Root User path,\", ROOT_USER_PATH)\n",
    "print()\n",
    "print(\"IEEE Male Clean path,\", IEEE_MALE_CLEAN_PATH)\n",
    "print(\"\\t|-> Train Clean .WAV path,\", IEEE_MALE_CLEAN_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Clean .WAV path,\", IEEE_MALE_CLEAN_DEV_PATH)\n",
    "print(\"\\t|-> Test Clean .WAV path,\", IEEE_MALE_CLEAN_TEST_PATH)\n",
    "print()\n",
    "print(\"IEEE Male Quantized Clean path,\", IEEE_MALE_QUANT_CLEAN_PATH)\n",
    "print(\"\\t|-> Train Quantized Clean .WAV path,\", IEEE_MALE_QUANT_CLEAN_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Quantized Clean .WAV path,\", IEEE_MALE_QUANT_CLEAN_DEV_PATH)\n",
    "print(\"\\t|-> Test Quantized Clean .WAV path,\", IEEE_MALE_QUANT_CLEAN_TEST_PATH)\n",
    "print()\n",
    "print(\"IEEE Male MIXTURE .WAV path,\", IEEE_MALE_MIX_PATH)\n",
    "print(\"\\t|-> Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_PATH)\n",
    "print(\"\\t\\t|-> Train Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix SSN .WAV path,\", IEEE_MALE_MIX_SSN_TEST_PATH)\n",
    "\n",
    "print(\"\\t|-> Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_PATH)\n",
    "print(\"\\t\\t|-> Train Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix FACTORY .WAV path,\", IEEE_MALE_MIX_FACTORY_TEST_PATH)\n",
    "\n",
    "print(\"\\t|->Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_PATH)\n",
    "print(\"\\t\\t|-> Train Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix CAFE .WAV path,\", IEEE_MALE_MIX_CAFE_TEST_PATH)\n",
    "\n",
    "print(\"\\t|->Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_PATH)\n",
    "print(\"\\t\\t|-> Train Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_TRAIN_PATH)\n",
    "print(\"\\t\\t|-> Dev Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_DEV_PATH)\n",
    "print(\"\\t\\t|-> Test Mix BABBLE .WAV path,\", IEEE_MALE_MIX_BABBLE_TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEE FEMALE Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IEEE_FEMALE_CORPORA_PATH = os.path.join(ROOT_PATH,'SpeechCorpora/IEEE_female') # female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summaray"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Root path,\", ROOT_PATH)\n",
    "print(\"\\t|-> Root User path,\", ROOT_USER_PATH)\n",
    "print()\n",
    "print(\"IEEE Male Data Corpora path,\", IEEE_MALE_CORPORA_PATH)\n",
    "print(\"IEEE Female Data Corpora path,\", IEEE_FEMALE_CORPORA_PATH)\n",
    "print()\n",
    "print(\"Clean .WAV path,\", CLEAN_wavs_PATH)\n",
    "print(\"\\t|-> Train Clean .WAV path,\", CLEAN_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Clean .WAV path,\", CLEAN_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Clean .WAV path,\", CLEAN_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix SSN .WAV path,\", SSN_wavs_PATH)\n",
    "print(\"\\t|-> Train Mix SSN .WAV path,\", SSN_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix SSN .WAV path,\", SSN_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix SSN .WAV path,\", SSN_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix CAFE .WAV path,\", CAFE_MIXTURE_PATH)\n",
    "print(\"\\t|-> Train Mix CAFE .WAV path,\", CAFE_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix CAFE .WAV path,\", CAFE_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix CAFE .WAV path,\", CAFE_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix BABBLE .WAV path,\", BABBLE_MIXTURE_PATH)\n",
    "print(\"\\t|-> Train Mix BABBLE .WAV path,\", BABBLE_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix BABBLE .WAV path,\", BABBLE_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix BABBLE .WAV path,\", BABBLE_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Mix FACTORY .WAV path,\", FACTORY_MIXTURE_PATH)\n",
    "print(\"\\t|-> Train Mix FACTORY .WAV path,\", FACTORY_wavs_TRAIN_PATH)\n",
    "print(\"\\t|-> Dev Mix FACTORY .WAV path,\", FACTORY_wavs_DEV_PATH)\n",
    "print(\"\\t|-> Test Mix FACTORY .WAV path,\", FACTORY_wavs_TEST_PATH)\n",
    "print()\n",
    "print(\"Enhanced .WAV path,\", Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> SSN Enhanced .WAV path,\", SSN_Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> FACTORY Enhanced .WAV path,\", FACTORY_Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> BABBLE Enhanced .WAV path,\", BABBLE_Enhanced_wavs_PATH)\n",
    "print(\"\\t|-> FACTORY Enhanced .WAV path,\", FACTORY_Enhanced_wavs_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMIT Dataset "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# .NPY FILE PATH\n",
    "FILE_SAVE_PATH = '/data/knayem/TIMIT_DataFiles'\n",
    "\n",
    "# SSN PATH\n",
    "SSN_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/ssn'\n",
    "\n",
    "# CAFE PATH\n",
    "CAFE_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/cafe'\n",
    "\n",
    "# BABBLE PATH\n",
    "BABBLE_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/babble'\n",
    "\n",
    "# FACTORY PATH\n",
    "FACTORY_MIXTURE_PATH = '/data/knayem/TIMIT_mixture/factory'\n",
    "\n",
    "\n",
    "# Train, Dev, Test\n",
    "TRAIN_MIXTURE_PATH = 'train_16k'\n",
    "DEV_MIXTURE_PATH = 'dev_16k'\n",
    "TEST_MIXTURE_PATH = 'test_16k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PATH = os.path.join(CLEAN_PATH,TRAIN_CLEAN_PATH) # clean train\n",
    "# PATH = os.path.join(CLEAN_PATH,DEV_CLEAN_PATH) # clean dev\n",
    "PATH = os.path.join(CLEAN_PATH,TEST_CLEAN_PATH) # clean test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a Parameters \n",
    "\n",
    "Followings are the basic parameter for calculating STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 640, noverlap: 320, nfft: 640, fs: 16000, hop_length: 320\n"
     ]
    }
   ],
   "source": [
    "fs = int(16e3)\n",
    "\n",
    "n_fft = 640\n",
    "win_length = int(40e-3*fs) # librosa needs scalar value\n",
    "overlap = int(20e-3*fs)\n",
    "hop_length = win_length - overlap # librosa needs scalar value\n",
    "\n",
    "NUMS_PRINTS = 10\n",
    "\n",
    "print('window: {0}, noverlap: {1}, nfft: {2}, fs: {3}, hop_length: {4}'.\n",
    "      format(win_length,overlap,n_fft,fs,hop_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b STFT function\n",
    "\n",
    "Calculate Magnitude and Group Delay of the PATH (train, dev, test of IEEE/TIMIT) to get an overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_gd_phase(filename, fs, n_fft, hop_length, win_length):\n",
    "    \n",
    "    y, sr = librosa.load(filename, sr=fs)\n",
    "    s_stft = librosa.stft(y,n_fft,hop_length,win_length)\n",
    "    mag, phase = librosa.magphase(s_stft)\n",
    "    angle = np.angle(phase)\n",
    "\n",
    "    unwrap_angle = np.unwrap(angle, axis=0) # freq, MATLAB implementation\n",
    "    unwrap_angle_s = np.roll(unwrap_angle, 1, axis=0) # roll across freq\n",
    "    unwrap_GD = np.angle(np.exp(1j*(unwrap_angle - unwrap_angle_s))) # paper implementation\n",
    "\n",
    "    return len(y), mag, unwrap_GD, phase, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fixed step Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_AMP, MIN_AMP = 100, 0\n",
    "\n",
    "QUANTIZED_DIRECTORY_TAG = \"Quantized\"\n",
    "Fixed_Step_Quantization_TAG = \"FS\"\n",
    "\n",
    "QUANT_STEP = 0.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_val(val, quant_boundary):\n",
    "    \n",
    "    proximity = abs(quant_boundary-val)\n",
    "    closest_boundary_index = np.argmin(proximity)\n",
    "    return quant_boundary[closest_boundary_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_indx(val, quant_boundary):\n",
    "    \n",
    "    proximity = abs(quant_boundary-val)\n",
    "    return np.argmin(proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_matrix(matrix, QUANT_STEP, MAX_AMP=100,MIN_AMP=0):\n",
    "    \n",
    "    quant_boundary = np.linspace(MIN_AMP,MAX_AMP,int(MAX_AMP//QUANT_STEP))\n",
    "    m_shape = matrix.shape\n",
    "    \n",
    "    quantized_list = [quantized_val(v,quant_boundary) for row in matrix for v in row]\n",
    "    return np.array(quantized_list).reshape(m_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_indx_matrix(matrix, QUANT_STEP, MAX_AMP=100,MIN_AMP=0):\n",
    "    \n",
    "    quant_boundary = np.linspace(MIN_AMP,MAX_AMP,int(MAX_AMP//QUANT_STEP))\n",
    "    m_shape = matrix.shape\n",
    "    \n",
    "    quantized_list = [quantized_indx(v,quant_boundary) for row in matrix for v in row]\n",
    "    return np.array(quantized_list).reshape(m_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def save_enhanced(mag, phase, fs, n_fft, hop_length, win_length, target_directory, filename, tags=None):\n",
    "    \n",
    "    D = mag*phase\n",
    "    enhanced = librosa.istft(D,hop_length,win_length)\n",
    "    \n",
    "    # enhanced filename creation\n",
    "    name = filename.split('.')[0]\n",
    "    \n",
    "    if tags is not None:\n",
    "        if 'quantization_tag' in tags:\n",
    "            name = \"_\".join([name,tags['quantization_tag'],str(tags['step'])])\n",
    "        if 'avg_step' in tags:\n",
    "            name = \"_\".join([name,str(tags['avg_step'])])\n",
    "\n",
    "    name = \".\".join([name,\"wav\"])\n",
    "\n",
    "    \n",
    "    # directory creation   \n",
    "    if not os.path.exists(target_directory):\n",
    "        print(False,target_directory)\n",
    "        os.makedirs(target_directory)\n",
    "    else:\n",
    "        print(True,target_directory)\n",
    "        pass\n",
    "    \n",
    "    wav_filepath = os.path.join(target_directory,name)\n",
    "                                       \n",
    "    # save file\n",
    "    sf.write(wav_filepath, enhanced, int(fs))\n",
    "    #print(wav_filepath)\n",
    "    \n",
    "    return wav_filepath\n",
    "                                       "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpora_path_list = [CLEAN_wavs_PATH]\n",
    "\n",
    "# [0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125 0.00390625]\n",
    "QUANT_STEP_LIST = [0.0625]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for QUANT_STEP in QUANT_STEP_LIST:\n",
    "    \n",
    "    for enum1, corpora in enumerate(sorted(corpora_path_list)) :\n",
    "        print(enum1,\"CORPORA:\", corpora)\n",
    "        QUANTIZED_DIRECTORY = corpora+\"_\"+QUANTIZED_DIRECTORY_TAG\n",
    "\n",
    "        for root, dirs, files in os.walk(corpora): \n",
    "            # .wav files only\n",
    "            wav_files = list( filter(lambda x: x.split('.')[-1] == 'wav', files) )\n",
    "            print(\"ROOT:\",root, \", len(DIR):\", len(dirs), \", len(FILES):\",len(wav_files),root.split('/')[-1])\n",
    "            \n",
    "            # folder name\n",
    "            if len(dirs)==0:\n",
    "                folder_name = root.split('/')[-1]\n",
    "                QUANTIZED_DIRECTORY_PATH = os.path.join(QUANTIZED_DIRECTORY,folder_name)\n",
    "                QUANTIZED_DIRECTORY_PATH = \"_\".join([QUANTIZED_DIRECTORY_PATH,Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "\n",
    "                npy_list = []\n",
    "#                 plt.figure()\n",
    "                \n",
    "            for enum2, filename in enumerate(sorted(wav_files)):\n",
    "                clean_wav_full_path = os.path.join(root, filename)\n",
    "                                                   \n",
    "                len_y, mag, unwrap_GD, phase, angle = mag_gd_phase(clean_wav_full_path, fs, n_fft, hop_length, win_length)\n",
    "                quantized_mag = quantized_matrix(mag, QUANT_STEP, MAX_AMP, MIN_AMP)\n",
    "\n",
    "                diff_mag = abs(mag-quantized_mag)\n",
    "                total_diff = np.sum(diff_mag)\n",
    "#                 print(enum2,\"|Error| = \", total_diff)\n",
    "\n",
    "                D = librosa.amplitude_to_db(mag, ref=np.max)\n",
    "                q_D = librosa.amplitude_to_db(quantized_mag, ref=np.max)\n",
    "\n",
    "                quant_wav_full_path = save_enhanced(quantized_mag, phase, fs, n_fft, hop_length, win_length, \n",
    "                                                    QUANTIZED_DIRECTORY_PATH, filename,\n",
    "                                                    {'quantization_tag':Fixed_Step_Quantization_TAG,'step':QUANT_STEP})\n",
    "\n",
    "                print(clean_wav_full_path,\"<->\",quant_wav_full_path)\n",
    "                npy_list.append( [filename, clean_wav_full_path, len_y, mag.shape[1]])\n",
    "                \n",
    "                \n",
    "                # plot the spectrogram\n",
    "#                 plt.subplot(len(wav_files), enum2+1, 1)\n",
    "#                 plt.subplot(3, 1, enum2+1)\n",
    "#                 librosa.display.specshow(D, y_axis='hz', x_axis='time', sr=fs)\n",
    "#                 plt.colorbar(format='%+2.0f dB')\n",
    "#                 plt.title(\":\".join([str(enum2),'mag',filename]))\n",
    "#                 plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "#                 plt.subplot(3, 2, enum2+1)\n",
    "#                 librosa.display.specshow(q_D, y_axis='hz', x_axis='time', sr=fs)\n",
    "#                 plt.colorbar(format='%+2.0f dB')\n",
    "#                 plt.title(\":\".join([str(enum2),'quant-mag',quant_wav_full_path.split('/')[-1]]))\n",
    "#                 plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "#                 plt.subplot(3, 3, enum2+1)\n",
    "#                 librosa.display.specshow(librosa.amplitude_to_db(librosa.amplitude_to_db(diff_mag, ref=np.max), ref=np.max), y_axis='hz', x_axis='time', sr=fs)\n",
    "#                 plt.colorbar(format='%+2.0f dB')\n",
    "#                 plt.title(\":\".join([str(enum2),'|Error|',str(total_diff)]))\n",
    "#                 plt.subplots_adjust(hspace=0.5)\n",
    "                \n",
    "#                 plt.draw()\n",
    "                \n",
    "#                 if enum2>=10:\n",
    "#                     break\n",
    "                    \n",
    "            if len(dirs)==0:    \n",
    "                npy_path = os.path.join(FILE_SAVE_PATH,QUANTIZED_DIRECTORY_PATH.split('/')[-1])\n",
    "#                 plt_path = os.path.join(FILE_SAVE_PATH,QUANTIZED_DIRECTORY_PATH.split('/')[-1]+\".pdf\")\n",
    "\n",
    "                np.save(npy_path, npy_list)\n",
    "#                 plt.savefig(plt_path,bbox_inches='tight')\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_NPY(npy_list, todo_list):\n",
    "    '''\n",
    "    npy_list[0]: clean file name\n",
    "    npy_list[1]: full clean .wav path *\n",
    "    npy_list[2]: frame length in time domain\n",
    "    npy_list[3]: time frame length in freq domain *\n",
    "    '''\n",
    "    if 'CLEAN_QUANT_pair' in todo_list:\n",
    "        CLEAN_QUANT_Pair_DICT = dict()\n",
    "        quant_step = str(0.0625)\n",
    "        quant_tag = \"FS\"\n",
    "    if 'CLEAN_FullClean_pair' in todo_list:\n",
    "        CLEAN_FullClean_Pair_DICT = dict()\n",
    "    if 'CLEAN_tfLen_pair' in todo_list:\n",
    "        CLEAN_tfLen_Pair_DICT = dict()\n",
    "    \n",
    "    for entry in npy_list:\n",
    "        key = entry[0].split('.')[0]\n",
    "        \n",
    "        if 'CLEAN_QUANT_pair' in todo_list:\n",
    "            quant_full_path = entry[1].replace('clean_16k','clean_16k_Quantized')\n",
    "            quant_full_path = replace_rev(quant_full_path,'16k','_'.join(['16k',quant_tag,quant_step]),2)\n",
    "            CLEAN_QUANT_Pair_DICT[key] = quant_full_path\n",
    "            \n",
    "        if 'CLEAN_FullClean_pair' in todo_list:\n",
    "            CLEAN_FullClean_Pair_DICT[key] = entry[1]\n",
    "            \n",
    "        if 'CLEAN_tfLen_pair' in todo_list:\n",
    "            CLEAN_tfLen_Pair_DICT[key] = int(entry[3])\n",
    "            \n",
    "    return_dicts = []\n",
    "    if 'CLEAN_QUANT_pair' in todo_list:\n",
    "        return_dicts.append(CLEAN_QUANT_Pair_DICT)\n",
    "    if 'CLEAN_FullClean_pair' in todo_list:\n",
    "        return_dicts.append(CLEAN_FullClean_Pair_DICT)\n",
    "    if 'CLEAN_tfLen_pair' in todo_list:\n",
    "        return_dicts.append(CLEAN_tfLen_Pair_DICT)\n",
    "\n",
    "    return return_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/knayem/Quantized_DataFiles/train_16k_FS_0.0625'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_QUANT_LIST_NPY_PATH = \"_\".join([os.path.join(FILE_SAVE_PATH,TRAIN_CLEAN_FOLDER),\n",
    "                                      Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "DEV_QUANT_LIST_NPY_PATH = \"_\".join([os.path.join(FILE_SAVE_PATH,DEV_CLEAN_FOLDER),\n",
    "                                      Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "TEST_QUANT_LIST_NPY_PATH = \"_\".join([os.path.join(FILE_SAVE_PATH,TEST_CLEAN_FOLDER),\n",
    "                                      Fixed_Step_Quantization_TAG,str(QUANT_STEP)])\n",
    "\n",
    "\n",
    "TRAIN_QUANT_LIST = np.load(TRAIN_QUANT_LIST_NPY_PATH+\".npy\")\n",
    "DEV_QUANT_LIST = np.load(DEV_QUANT_LIST_NPY_PATH+\".npy\")\n",
    "TEST_QUANT_LIST = np.load(TEST_QUANT_LIST_NPY_PATH+\".npy\")\n",
    "\n",
    "TRAIN_QUANT_LIST[0]\n",
    "TRAIN_QUANT_LIST_NPY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key -> clean .wav filename\n",
    "# value -> full path of Clean Quantized .wav (Pair_CLEAN_QUAT_DICT)\n",
    "#       -> number of Freq frame (Pair_CLEAN_FreqFrame_DICT)\n",
    "\n",
    "todo_list = {'CLEAN_QUANT_pair','CLEAN_FullClean_pair','CLEAN_tfLen_pair'}\n",
    "Train_CLEAN_QUANT_Pair_DICT, Train_CLEAN_FullClean_Pair_DICT, Train_CLEAN_FreqFrame_Pair_DICT = process_NPY(TRAIN_QUANT_LIST,todo_list)\n",
    "Dev_CLEAN_QUANT_Pair_DICT, Dev_CLEAN_FullClean_Pair_DICT, Dev_CLEAN_FreqFrame_Pair_DICT = process_NPY(DEV_QUANT_LIST,todo_list)\n",
    "Test_CLEAN_QUANT_Pair_DICT, Test_CLEAN_FullClean_Pair_DICT, Test_CLEAN_FreqFrame_Pair_DICT = process_NPY(TEST_QUANT_LIST,todo_list)\n",
    "\n",
    "all_pair_dictionaries={'train':[Train_CLEAN_QUANT_Pair_DICT, Train_CLEAN_FullClean_Pair_DICT, Train_CLEAN_FreqFrame_Pair_DICT],\n",
    "          'dev':[Dev_CLEAN_QUANT_Pair_DICT, Dev_CLEAN_FullClean_Pair_DICT, Dev_CLEAN_FreqFrame_Pair_DICT],\n",
    "           'test':[Test_CLEAN_QUANT_Pair_DICT, Test_CLEAN_FullClean_Pair_DICT, Test_CLEAN_FreqFrame_Pair_DICT]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Test_CLEAN_FreqFrame_Pair_DICT.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Functions "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def group_all_files(src_folder_list):\n",
    "    '''\n",
    "    parameters:\n",
    "        <src_folder_list> = list of folders from which .wav files will be used\n",
    "        \n",
    "    returns:\n",
    "        <all_file_list> = a list of tuple (key,full_file_path), \n",
    "                            key-> 1st 11 char of the filename,\n",
    "    '''\n",
    "    \n",
    "    all_file_list = []\n",
    "    \n",
    "    for enum1, src in enumerate(src_folder_list) :\n",
    "        #print(enum1,\"SRC:\", src)\n",
    "        \n",
    "        for root, dirs, files in os.walk(src): \n",
    "            #print(\"ROOT:\",root, \", len(DIR):\", len(dirs), \", len(FILES):\",len(files))\n",
    "\n",
    "            for enum2, filename in enumerate(sorted(files)):\n",
    "                key = filename[:11]\n",
    "                all_file_list.append( (key, \"/\".join([root,filename])) )\n",
    "                \n",
    "    return all_file_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "This class produces tuple like output which is not working correctly.\n",
    "'''\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, src_folders_list, pair_dictionaries, batch_size=32, freq_frames_len = 321,\n",
    "                 max_time_frame = 200, n_channels=1, n_classes=1600, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.list_IDs = self.group_all_files(src_folder_list)\n",
    "        \n",
    "        self.cleanQuant_pair_dict = pair_dictionaries[0]\n",
    "        self.cleanFullClean_pair_dict = pair_dictionaries[1]\n",
    "        self.cleanFreqFrame_pair_dict = pair_dictionaries[2]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_time_frame = max_time_frame\n",
    "        self.dim = (freq_frames_len,max_time_frame)\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels)) #(batch_size, 321, 200, n_channels)\n",
    "        \n",
    "        y_cls = np.empty((*self.dim, self.n_classes), dtype=np.int32) #(321, 200, 1600)\n",
    "        y_cln = np.empty(self.dim, dtype=np.float32) #(321, 200)\n",
    "        \n",
    "        dt = np.dtype([ ['cls',np.int32,(*self.dim, self.n_classes)],['cln',np.float32,self.dim] ])\n",
    "        Y = np.empty( (self.batch_size,),dtype=dt ) #(batch_size,('q','c'))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for i, (key, x_path) in enumerate(list_IDs_temp):\n",
    "            _,x_mag,_,_,_ = mag_gd_phase(x_path, fs, n_fft, hop_length, win_length)\n",
    "            _,y_clean_mag,_,_,_ = mag_gd_phase(self.cleanFullClean_pair_dict[key], fs, n_fft, hop_length, win_length)\n",
    "            _,y_quant_mag,_,_,_ = mag_gd_phase(self.cleanQuant_pair_dict[key], fs, n_fft, hop_length, win_length)\n",
    "            \n",
    "            #input features\n",
    "            x = pad_sequences(x_mag, dtype='float32', maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "            #ground truth mag value\n",
    "            y_clean = pad_sequences(y_clean_mag,\n",
    "                                    dtype='float32', maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "            #ground truth quantized catagory\n",
    "            y_quant = pad_sequences(quantized_indx_matrix(y_quant_mag, QUANT_STEP=0.0625),\n",
    "                                    maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "\n",
    "            y_class = to_categorical(y_quant, num_classes=self.n_classes)\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,] = np.expand_dims(x, axis=-1)\n",
    "\n",
    "            # Store class\n",
    "            Y[i] = [y_class,y_clean]\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    \n",
    "    def group_all_files(self, src_folder_list):\n",
    "        '''\n",
    "        parameters:\n",
    "            <src_folder_list> = list of folders from which .wav files will be used\n",
    "\n",
    "        returns:\n",
    "            <all_file_list> = a list of tuple (key,full_file_path), \n",
    "                                key-> 1st 11 char of the filename,\n",
    "        '''\n",
    "        all_file_list = []\n",
    "\n",
    "        for enum1, src in enumerate(src_folder_list) :\n",
    "            for root, dirs, files in os.walk(src): \n",
    "                for enum2, filename in enumerate(sorted(files)):\n",
    "                    key = filename[:11]\n",
    "                    all_file_list.append( (key, \"/\".join([root,filename])) )\n",
    "\n",
    "        return all_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, src_folders_list, pair_dictionaries, batch_size=32, freq_frames_len = 321,\n",
    "                 max_time_frame = 200, n_classes=1600, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.list_IDs = self.group_all_files(src_folder_list)\n",
    "        \n",
    "        self.cleanQuant_pair_dict = pair_dictionaries[0]\n",
    "        self.cleanFullClean_pair_dict = pair_dictionaries[1]\n",
    "        self.cleanFreqFrame_pair_dict = pair_dictionaries[2]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_time_frame = max_time_frame\n",
    "        self.dim = (freq_frames_len,max_time_frame)\n",
    "        self.out_dim = (freq_frames_len,max_time_frame*2)\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim)) #(batch_size, 321, 200)\n",
    "#         Y = np.empty( (self.batch_size, *self.out_dim)) #(batch_size, 321, 200x2)\n",
    "        Y = np.empty( (self.batch_size, *self.dim))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, (key, x_path) in enumerate(list_IDs_temp):\n",
    "            _,x_mag,_,_,_ = mag_gd_phase(x_path, fs, n_fft, hop_length, win_length)\n",
    "            _,y_clean_mag,_,_,_ = mag_gd_phase(self.cleanFullClean_pair_dict[key], fs, n_fft, hop_length, win_length)\n",
    "            _,y_quant_mag,_,_,_ = mag_gd_phase(self.cleanQuant_pair_dict[key], fs, n_fft, hop_length, win_length)\n",
    "            \n",
    "            #input features\n",
    "            x = pad_sequences(x_mag, dtype='float32', maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "            #ground truth mag value\n",
    "            y_clean = pad_sequences(y_clean_mag,\n",
    "                                    dtype='float32', maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "            #ground truth quantized catagory\n",
    "            y_quant = pad_sequences(quantized_indx_matrix(y_quant_mag, QUANT_STEP=0.0625),\n",
    "                                    maxlen=self.max_time_frame, padding='post', truncating='post')\n",
    "\n",
    "#             y_class = to_categorical(y_quant, num_classes=self.n_classes)\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,] = x\n",
    "\n",
    "            # Store class\n",
    "#             Y[i] = np.array([ np.append(q,c) for q,c in zip(y_quant,y_clean)])\n",
    "            Y[i,] = y_quant\n",
    "\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    \n",
    "    def group_all_files(self, src_folder_list):\n",
    "        '''\n",
    "        parameters:\n",
    "            <src_folder_list> = list of folders from which .wav files will be used\n",
    "\n",
    "        returns:\n",
    "            <all_file_list> = a list of tuple (key,full_file_path), \n",
    "                                key-> 1st 11 char of the filename,\n",
    "        '''\n",
    "        all_file_list = []\n",
    "\n",
    "        for enum1, src in enumerate(src_folder_list) :\n",
    "            for root, dirs, files in os.walk(src): \n",
    "                for enum2, filename in enumerate(sorted(files)):\n",
    "                    key = filename[:11]\n",
    "                    all_file_list.append( (key, \"/\".join([root,filename])) )\n",
    "\n",
    "        return all_file_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def next_batch(src_folder_list, batch_size, max_time_frames, TASK=\"TRAIN\"):\n",
    "    '''\n",
    "    parameters:\n",
    "        <src_folder_list> = list of folders from which .wav files will be used\n",
    "        <batch_size>\n",
    "        <max_time_frames>\n",
    "        <TASK>\n",
    "        \n",
    "    returns:\n",
    "        x, y\n",
    "    '''\n",
    "    \n",
    "    all_file_list = group_all_files(src_folder_list)\n",
    "    permuted_sequence = np.random.permutation(len(all_file_list))\n",
    "    \n",
    "    for indx in permuted_sequence:\n",
    "        key, x_path = all_file_list[indx]\n",
    "        \n",
    "        if TASK==\"TRAIN\":\n",
    "            y_quant_path = Train_CLEAN_QUANT_Pair_DICT[key]\n",
    "            y_clean_path = Train_CLEAN_FullClean_Pair_DICT[key]\n",
    "        elif TASK==\"DEV\":\n",
    "            y_quant_path = Dev_CLEAN_QUANT_Pair_DICT[key]\n",
    "            y_clean_path = Dev_CLEAN_FullClean_Pair_DICT[key]\n",
    "        elif TASK==\"TEST\":\n",
    "            y_quant_path = Test_CLEAN_QUANT_Pair_DICT[key]\n",
    "            y_clean_path = Test_CLEAN_FullClean_Pair_DICT[key]\n",
    "            \n",
    "        print(x_path,'\\n',y_quant_path,'\\n',y_clean_path)\n",
    "        #len(y), mag, unwrap_GD, phase, angle = mag_gd_phase(filename, fs, n_fft, hop_length, win_length)\n",
    "        _,x_mag,_,_,_ = mag_gd_phase(x_path, fs, n_fft, hop_length, win_length)\n",
    "        _,y_quant_mag,_,_,_ = mag_gd_phase(y_quant_path, fs, n_fft, hop_length, win_length)\n",
    "        _,y_clean_mag,_,_,_ = mag_gd_phase(y_clean_path, fs, n_fft, hop_length, win_length)\n",
    "        \n",
    "        #input features\n",
    "        x = pad_sequences(x_mag, dtype='float32', maxlen=max_time_frames, padding='post', truncating='post')\n",
    "        #ground truth mag value\n",
    "        y_clean = pad_sequences(y_clean_mag,\n",
    "                                dtype='float32', maxlen=max_time_frames, padding='post', truncating='post')\n",
    "        #ground truth quantized catagory\n",
    "        y_quant = pad_sequences(quantized_indx_matrix(y_quant_mag, QUANT_STEP=0.0625),\n",
    "                                maxlen=max_time_frames, padding='post', truncating='post')\n",
    "        \n",
    "        y_class = to_categorical(y_quant, num_classes=int((MAX_AMP-MIN_AMP)//QUANT_STEP))\n",
    "        \n",
    "        print(x.shape,y_quant.shape, y_class.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/knayem/IEEE_male_mixture/cafe/train_16k/S_38_08_16k_7_0dB_CAFE_noisyspeech.wav \n",
      " /data/knayem/IEEE_male_clean_16k_Quantized/train_16k_FS_0.0625/S_38_08_16k_FS_0.0625.wav \n",
      " /data/knayem/IEEE_male_clean_16k/train_16k/S_38_08_16k.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knayem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(321, 200) (321, 200) (321, 200, 1600)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_TIME_FRAMES = 200\n",
    "\n",
    "src_folder_list = [IEEE_MALE_MIX_SSN_TRAIN_PATH,IEEE_MALE_MIX_FACTORY_TRAIN_PATH,IEEE_MALE_MIX_CAFE_TRAIN_PATH]\n",
    "\n",
    "matrix = next_batch(src_folder_list,BATCH_SIZE,MAX_TIME_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(**train_params)\n",
    "a,b = train_generator.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 321, 200)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def customLoss_train(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    y_quant = yTrue[:,:,:MAX_TIME_FRAMES]\n",
    "    y_clean = yTrue[:,:,:MAX_TIME_FRAMES]\n",
    "    \n",
    "    y_class = tf.one_hot(tf.to_int32(y_quant), depth=1600)\n",
    "    print(K.shape(yTrue),K.shape(yPred),K.shape(y_class),K.shape(y_quant))\n",
    "    cce = tf.losses.sigmoid_cross_entropy(y_class,yPred)\n",
    "    return cce\n",
    "\n",
    "def customLoss_val(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.cvNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_mag = K.variable(value=0)\n",
    "    for e,n in enumerate(numframes):\n",
    "        cost_mag += K.sum(K.square(yTrue[e,:n]- yPred[e,:n]))\n",
    "\n",
    "    cost = cost_mag/r\n",
    "    return cost\n",
    "\n",
    "def customLoss_test(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    numframes = opts.teNumframes[opts.batch_ids]\n",
    "    numframes = [int(f[0]) for f in numframes]\n",
    "    \n",
    "    cost_mag = K.variable(value=0)\n",
    "    for e,n in enumerate(numframes):\n",
    "        cost_mag += K.sum(K.square(yTrue[e,:n]- yPred[e,:n]))\n",
    "\n",
    "    cost = cost_mag/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctionCallback(Callback):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.model.loss = customLoss_train\n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.model.loss = customLoss_val\n",
    "        pass\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        # print('-> on_train_end=',self.params)\n",
    "        self.model.loss = customLoss_test\n",
    "        pass\n",
    "        \n",
    " \n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "FRQ_FRAMES_LEN = 321\n",
    "MAX_TIME_FRAMES = 200\n",
    "\n",
    "NUM_CLASSES = np.linspace(MIN_AMP,MAX_AMP, int(MAX_AMP//QUANT_STEP)).shape[0]\n",
    "\n",
    "RNN_UNITS = 256\n",
    "\n",
    "# saved model\n",
    "MODEL_FILE = \"./model_files/model_{epoch:02d}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "train_params = {'src_folders_list': [IEEE_MALE_MIX_SSN_TRAIN_PATH,IEEE_MALE_MIX_FACTORY_TRAIN_PATH,IEEE_MALE_MIX_CAFE_TRAIN_PATH],\n",
    "          'pair_dictionaries': all_pair_dictionaries['train'],\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'freq_frames_len': FRQ_FRAMES_LEN,\n",
    "          'max_time_frame': MAX_TIME_FRAMES,\n",
    "          #'n_channels': 1,\n",
    "          'n_classes': NUM_CLASSES,\n",
    "          'shuffle': True}\n",
    "\n",
    "dev_params = {'src_folders_list': [IEEE_MALE_MIX_SSN_DEV_PATH,IEEE_MALE_MIX_FACTORY_DEV_PATH,IEEE_MALE_MIX_CAFE_DEV_PATH],\n",
    "          'pair_dictionaries': all_pair_dictionaries['dev'],\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'freq_frames_len': FRQ_FRAMES_LEN,\n",
    "          'max_time_frame': MAX_TIME_FRAMES,\n",
    "          #'n_channels': 1,\n",
    "          'n_classes': NUM_CLASSES,\n",
    "          'shuffle': True}\n",
    "\n",
    "test_params = {'src_folders_list': [IEEE_MALE_MIX_SSN_TEST_PATH,IEEE_MALE_MIX_FACTORY_TEST_PATH,IEEE_MALE_MIX_CAFE_TEST_PATH],\n",
    "          'pair_dictionaries': all_pair_dictionaries['test'],\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'freq_frames_len': FRQ_FRAMES_LEN,\n",
    "          'max_time_frame': MAX_TIME_FRAMES,\n",
    "          #'n_channels': 1,\n",
    "          'n_classes': NUM_CLASSES,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "train_generator = DataGenerator(**train_params)\n",
    "dev_generator = DataGenerator(**dev_params)\n",
    "test_generator = DataGenerator(**test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_91 (LSTM)               (None, 200, 256)          591872    \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, 200, 1600)         411200    \n",
      "=================================================================\n",
      "Total params: 1,003,072\n",
      "Trainable params: 1,003,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_91_input to have shape (200, 321) but got array with shape (321, 200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-352-6f1dad41e141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m history = model.fit_generator(train_generator, validation_data=dev_generator,\n\u001b[1;32m     19\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# model.save(SAVE_MODEL_FILE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_91_input to have shape (200, 321) but got array with shape (321, 200)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(RNN_UNITS, return_sequences=True, input_shape=(MAX_TIME_FRAMES,FRQ_FRAMES_LEN)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(RNN_UNITS, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(units=1600, activation='softmax')))\n",
    "print(model.summary())\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,write_graph=True, write_images=False)\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True),\n",
    "             tensorboard]\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator, validation_data=dev_generator,\n",
    "                    use_multiprocessing=True, workers=6,\n",
    "                    verbose=1)\n",
    "\n",
    "# model.save(SAVE_MODEL_FILE)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
