{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM Series\n",
    "\n",
    "** - Matlab initialization strategy **\n",
    "\n",
    "** - Train and Dev different loss functions **\n",
    "\n",
    "** - Adam Optimizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read parameters from .mat files\n",
    "save in Opt object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_VERSION = \"_e11v1\"\n",
    "Code_VERSION = \"_e11v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNN_DATA_FILE = \"./dnn_models/DNN_datas\"+ Data_VERSION+\".mat\"\n",
    "# DNN_DATA_FILE = \"./dnn_models/BR2_DNN_datas.mat\"\n",
    "\n",
    "DNN_MODEL_FILE = \"./dnn_models/DNN_params\"+ Data_VERSION+\".mat\"\n",
    "# DNN_MODEL_FILE = \"./dnn_models/BR2_DNN_params.mat\"\n",
    "\n",
    "DNN_NET_FILE = \"./dnn_models/DNN_net\"+ Code_VERSION+\".mat\"\n",
    "\n",
    "# ModelFN_FILE = \"./dnn_models/dnncirm.noiseSSN_05.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files saved by Matlab reading class. Data, Parameters are read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    opts_dict = dict()\n",
    "\n",
    "    def __init__(self, FILE, FILE_DATA):\n",
    "        \n",
    "        # Basic parameters\n",
    "        with h5py.File(FILE, 'r') as f:\n",
    "            key_list = list(f.keys())\n",
    "            print('Opt key:',key_list)\n",
    "\n",
    "            for k, v in f['opts'].items():\n",
    "\n",
    "                print('key:', k)\n",
    "\n",
    "                if k == 'ARMA_order':\n",
    "                    self.ARMA_order = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.ARMA_order\n",
    "                elif k == 'ada_grad_eps':\n",
    "                    self.ada_grad_eps = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.ada_grad_eps\n",
    "                elif k == 'ada_sgd_scale':\n",
    "                    self.ada_sgd_scale = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.ada_sgd_scale\n",
    "                elif k == 'change_momentum_point':\n",
    "                    self.change_momentum_point = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.change_momentum_point\n",
    "                elif k == 'cost_function':\n",
    "                    self.cost_function = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.cost_function += chr(c[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.cost_function\n",
    "\n",
    "                elif k == 'cv_interval':\n",
    "                    self.cv_interval = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.cv_interval\n",
    "                elif k == 'dim_input':\n",
    "                    self.dim_input = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.dim_input\n",
    "                elif k == 'dim_output':\n",
    "                    self.dim_output = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.dim_output\n",
    "                elif k == 'drop_ratio':\n",
    "                    self.drop_ratio = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.drop_ratio\n",
    "                elif k == 'eval_on_gpu':\n",
    "                    self.eval_on_gpu = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.eval_on_gpu\n",
    "                elif k == 'final_momentum':\n",
    "                    self.final_momentum = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.final_momentum\n",
    "                elif k == 'hid_struct':\n",
    "                    self.hid_struct = np.array(v)\n",
    "                    self.opts_dict[k] = self.hid_struct\n",
    "                elif k == 'initial_momentum':\n",
    "                    self.initial_momentum = np.array(v)[0][0]\n",
    "                    self.opts_dict[k] = self.initial_momentum\n",
    "                elif k == 'isDropout':\n",
    "                    self.isDropout = 0\n",
    "                    self.opts_dict[k] = self.isDropout\n",
    "                elif k == 'isDropoutInput':\n",
    "                    self.isDropoutInput = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isDropoutInput\n",
    "                elif k == 'isGPU':\n",
    "                    self.isGPU = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isGPU\n",
    "                elif k == 'isNormalize':\n",
    "                    self.isNormalize = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isNormalize\n",
    "                elif k == 'isPretrain':\n",
    "                    self.isPretrain = int(np.array(v)[0][0])\n",
    "                    self.opts_dict[k] = self.isPretrain\n",
    "                elif k == 'learner':\n",
    "                    self.learner = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.learner += chr(c[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.learner\n",
    "\n",
    "                elif k == 'net_struct':\n",
    "                    self.net_struct = np.array(v)\n",
    "                    for n_s in np.array(v):\n",
    "                        print('Opts Net Stuct:',n_s[0])\n",
    "\n",
    "                    self.opts_dict[k] = self.net_struct\n",
    "                elif k == 'rbm_batch_size':\n",
    "                    self.rbm_batch_size = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_batch_size\n",
    "                elif k == 'rbm_learn_rate_binary':\n",
    "                    self.rbm_learn_rate_binary = np.array(v)\n",
    "                    # self.opts_dict[k] = self.rbm_learn_rate_binary\n",
    "                elif k == 'rbm_learn_rate_real':\n",
    "                    self.rbm_learn_rate_real = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_learn_rate_real\n",
    "                elif k == 'rbm_max_epoch':\n",
    "                    self.rbm_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.rbm_max_epoch\n",
    "                elif k == 'save_on_fly':\n",
    "                    self.save_on_fly = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.save_on_fly\n",
    "                elif k == 'sgd_batch_size':\n",
    "                    self.sgd_batch_size = int(np.array(v)[0][0])\n",
    "                    self.sgd_batch_size = 1024//2 # BATCH_SIZE for training net\n",
    "                    # self.opts_dict[k] = self.sgd_batch_size\n",
    "                elif k == 'sgd_learn_rate':\n",
    "                    self.sgd_learn_rate = np.array(v)\n",
    "                    # self.opts_dict[k] = self.sgd_learn_rate\n",
    "                elif k == 'sgd_max_epoch':\n",
    "                    self.sgd_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.sgd_max_epoch\n",
    "                elif k == 'split_tanh1_c1':\n",
    "                    self.split_tanh1_c1 = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.split_tanh1_c1\n",
    "                elif k == 'split_tanh1_c2':\n",
    "                    self.split_tanh1_c2 = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.split_tanh1_c2\n",
    "                elif k == 'unit_type_hidden':\n",
    "                    self.unit_type_hidden = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_hidden += chr(c[0])\n",
    "\n",
    "                elif k == 'unit_type_output':\n",
    "                    self.unit_type_output = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_output += chr(c[0])\n",
    "\n",
    "        # Training and Dev Data \n",
    "        with h5py.File(FILE_DATA, 'r') as f:\n",
    "            print('Opts h5py keys:', list(f.keys()))\n",
    "            for k, v in f.items():\n",
    "\n",
    "                if k == 'trData':\n",
    "                    print(\"trData.shape: \", v.shape)\n",
    "                    self.trData = np.transpose(np.array(v))\n",
    "                    print(\"trData-> mean:\", np.mean(self.trData),\", var:\",np.var(self.trData), \", std:\",np.std(self.trData))\n",
    "                elif k == 'trLabel_i':\n",
    "                    print(\"trLabel_i.shape: \", v.shape)\n",
    "                    self.trLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_i-> mean:\", np.mean(self.trLabel_i), \", var:\", np.var(self.trLabel_i), \", std:\",\n",
    "                          np.std(self.trLabel_i))\n",
    "                elif k == 'trLabel_r':\n",
    "                    print(\"trLabel_r.shape: \", v.shape)\n",
    "                    self.trLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_r-> mean:\", np.mean(self.trLabel_r), \", var:\", np.var(self.trLabel_r), \", std:\",\n",
    "                          np.std(self.trLabel_r))\n",
    "                elif k == 'trNumframes':\n",
    "                    print(\"trNumframes.shape: \", v.shape)\n",
    "                    self.trNumframes = np.transpose(np.array(v))\n",
    "                    \n",
    "                elif k == 'cvData':\n",
    "                     print(\"cvData.shape: \", v.shape)\n",
    "                    self.cvData = np.transpose(np.array(v))\n",
    "                    print(\"cvData-> mean:\", np.mean(self.cvData), \", var:\", np.var(self.cvData), \", std:\",\n",
    "                          np.std(self.cvData))\n",
    "                elif k == 'cvLabel_i':\n",
    "                    print(\"cvLabel_i.shape: \", v.shape)\n",
    "                    self.cvLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_i-> mean:\", np.mean(self.cvLabel_i), \", var:\", np.var(self.cvLabel_i), \", std:\",\n",
    "                          np.std(self.cvLabel_i))\n",
    "                elif k == 'cvLabel_r':\n",
    "                    print(\"cvLabel_r.shape: \", v.shape)\n",
    "                    self.cvLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_r-> mean:\", np.mean(self.cvLabel_r), \", var:\", np.var(self.cvLabel_r), \", std:\",\n",
    "                          np.std(self.cvLabel_r))\n",
    "                elif k == 'cvNumframes':\n",
    "                    print(\"cvNumframes.shape: \", v.shape)\n",
    "                    self.cvNumframes = np.transpose(np.array(v))\n",
    "\n",
    "            self.trLabel = np.concatenate((self.trLabel_r, self.trLabel_i), axis=1)\n",
    "            self.cvLabel = np.concatenate((self.cvLabel_r, self.cvLabel_i), axis=1)\n",
    "\n",
    "\n",
    "            \n",
    "    def prepare3D_data(self, isTRAINING = True):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        if isTRAINING:\n",
    "            data = self.trData\n",
    "            label_r = self.trData_r\n",
    "            label_i = self.trData_i\n",
    "            numframes = self.trNumframes\n",
    "        else:\n",
    "            data = self.cvData\n",
    "            label_r = self.cvData_r\n",
    "            label_i = self.cvData_i\n",
    "            numframes = self.cvNumframes\n",
    "\n",
    "        data3D, label3D_r, label3D_i = None, None, None\n",
    "        numframes = np.cumsum(numframes)\n",
    "        \n",
    "        for frames in range(numframes):\n",
    "            data3D.append( data[:frames] if data3D is None else data[:frames]\n",
    "\n",
    "        return np.array(batchID,ndmin=2)\n",
    "    \n",
    "    def ready_batchID(self, total_num_samples, batch_size):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        batchID = []\n",
    "        num_batch = math.ceil(total_num_samples/batch_size)\n",
    "\n",
    "        for b in range( int(num_batch) ):\n",
    "            s = b*batch_size\n",
    "            e = (b+1)*batch_size -1\n",
    "\n",
    "            if e >= total_num_samples:\n",
    "                e = total_num_samples - 1\n",
    "\n",
    "            batchID.append((s,e))\n",
    "\n",
    "        return np.array(batchID,ndmin=2)\n",
    "\n",
    "\n",
    "    def suffle_data(self, total_num_samples):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        return  np.random.permutation(total_num_samples)\n",
    "\n",
    "\n",
    "    def next_batch(self, total_num_samples, batch_size, isTrainCycle=True):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        batchID = self.ready_batchID(total_num_samples, batch_size)\n",
    "        seq = self.suffle_data(total_num_samples)\n",
    "\n",
    "        for batch in range(batchID.shape[0]):\n",
    "            if isTrainCycle:\n",
    "                x = opts.trData[ seq[batchID[batch][0]:batchID[batch][1] ] ]\n",
    "                y = opts.trLabel[ seq[batchID[batch][0]:batchID[batch][1] ] ]\n",
    "\n",
    "            else:\n",
    "                x = opts.cvData[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "                y = opts.cvLabel[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "\n",
    "            # print('Next Batch', x.shape, y.shape)\n",
    "            yield [x, y]\n",
    "\n",
    "    # not used rightnow\n",
    "    def next_batch2(self, batch_size, isTrainCycle=True):\n",
    "        if isTrainCycle:\n",
    "            # selected_indics = np.random.choice(10, size=batch_size, replace=False)\n",
    "            selected_indics = np.random.randint(195192, size=batch_size)\n",
    "        else:\n",
    "            # selected_indics = np.random.randint(7, size= batch_size)\n",
    "            selected_indics = np.random.randint(44961, size=batch_size)\n",
    "\n",
    "        # print (\"selected_indics: \",selected_indics)\n",
    "        if isTrainCycle:\n",
    "            x = opts.trData[selected_indics]\n",
    "            y = np.concatenate((opts.trLabel_r, opts.trLabel_i), axis=1)[selected_indics]\n",
    "\n",
    "        else:\n",
    "            x = opts.cvData[selected_indics]\n",
    "            y = np.concatenate((opts.cvLabel_r, opts.cvLabel_i), axis=1)[selected_indics]\n",
    "\n",
    "        # print('Next Batch', x.shape, y.shape)\n",
    "        return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opts = Opts(DNN_MODEL_FILE, DNN_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HAVE to use OPS PARAMS ***********\n",
    "learning_rate = 0.001\n",
    "# training_epochs = 80\n",
    "# batch_size = 256\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HAVE to use NET_STRUCTURE ***********\n",
    "n_input_dim = 195192.0\n",
    "n_input = 1230  # data input\n",
    "n_hidden_1 = 1024  # 1st layer number of neurons\n",
    "n_hidden_2 = 1024  # 2nd layer number of neurons\n",
    "n_hidden_3 = 1024  # 3rd layer number of neurons\n",
    "n_classes = (963 + 963)  # total classes (real+imaginary)\n",
    "\n",
    "# For checking\n",
    "# train_time = np.zeros(training_epochs)\n",
    "\n",
    "# validation_error = np.full((1), np.inf)\n",
    "# min_validation_error = np.full((1), np.inf)\n",
    "\n",
    "Best_Cost = - np.inf\n",
    "Best_Weight, Best_Bias = None, None\n",
    "Best_epoch = -1\n",
    "\n",
    "# For displaying\n",
    "PREVIOUS_10 = 10\n",
    "DIFF_THRESHOLD = 0.000001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected 3 layer Feed Forward Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "\n",
    "# Xavier Initialization\n",
    "def weight_variable(shape, stddev=None, stddev_2=None):\n",
    "    if stddev is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0 / sum(shape)))\n",
    "    elif stddev > 0.0:\n",
    "        if stddev_2 is not None:\n",
    "            r, c = shape\n",
    "            initial1 = tf.truncated_normal([r, c//2], stddev=stddev)\n",
    "            initial2 = tf.truncated_normal([r, c//2], stddev=stddev_2)\n",
    "            initial = tf.concat([initial1,initial2], axis=1)\n",
    "        else:\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "\n",
    "    else:\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape, stddev=None, stddev_2=None):\n",
    "    # initial = tf.constant(0.1, shape=shape)\n",
    "    if stddev is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=np.sqrt(1.0 / sum(shape)))\n",
    "    elif stddev > 0.0:\n",
    "        if stddev_2 is not None:\n",
    "            r, c = shape\n",
    "            initial1 = tf.truncated_normal([r, c//2], stddev=stddev)\n",
    "            initial2 = tf.truncated_normal([r, c//2], stddev=stddev_2)\n",
    "            initial = tf.concat([initial1,initial2], axis=1)\n",
    "        else:\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "\n",
    "    else:\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old way\n",
    "weights = {\n",
    "    'h1': weight_variable([n_input, n_hidden_1],0.001),\n",
    "    'h2': weight_variable([n_hidden_1, n_hidden_2],0.001),\n",
    "    'h3': weight_variable([n_hidden_2, n_hidden_3],0.001),\n",
    "    'out': weight_variable([n_hidden_3, n_classes],0.001,0.0)\n",
    "}\n",
    "biases = {\n",
    "    'b1': bias_variable([1, n_hidden_1],0.0),\n",
    "    'b2': bias_variable([1, n_hidden_2],0.0),\n",
    "    'b3': bias_variable([1, n_hidden_3],0.0),\n",
    "    'out': bias_variable([1, n_classes],0.001,0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_NN(x):\n",
    "    layer_1 = tf.nn.relu(tf.matmul(x, weights['h1']) + biases['b1'])\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def calc(x, y):\n",
    "    # Returns predictions and error\n",
    "    predictions = multilayer_NN(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    r, c = y.get_shape().as_list()\n",
    "    r = opts.sgd_batch_size\n",
    "    print('r:', type(r), r, ',c:', type(c), c)\n",
    "\n",
    "    # TRAIN: matlab\n",
    "    # cost1 = 0.5 * sum(sum((pred_real - label_real). ^ 2)) / num_sample;\n",
    "    # cost2 = 0.5 * sum(sum((pred_imag - label_imag). ^ 2)) / num_sample;\n",
    "    # cost = cost1 + cost2;\n",
    "    cost1 = tf.reduce_sum(tf.squared_difference(y[:, :c//2], predictions[:, :c//2]))\n",
    "    cost2 = tf.reduce_sum(tf.squared_difference(y[:, c//2:], predictions[:, c//2:]))\n",
    "    loss_t = 0.5*(cost1+cost2)/r\n",
    "    # mse = tf.losses.mean_squared_error(labels=y, predictions=predictions,weights=0.5)\n",
    "    # loss_t = tf.divide(tf.reduce_sum(mse), r)\n",
    "\n",
    "\n",
    "    # DEV: matlab\n",
    "    # dev_perfs = -mean(sum((dev_label_real - dev_netout1). ^ 2)) - mean(sum((dev_label_imag - dev_netout2). ^ 2));\n",
    "    mse_r = tf.reduce_sum(tf.squared_difference(y[:, :c // 2], predictions[:, :c // 2]), axis=0)\n",
    "    mse_i = tf.reduce_sum(tf.squared_difference(y[:, c // 2:], predictions[:, c // 2:]), axis=0)\n",
    "\n",
    "    loss_d = -tf.reduce_mean(mse_r)-tf.reduce_mean(mse_i)\n",
    "\n",
    "\n",
    "    return [predictions, loss_t, mse_r, mse_i, loss_d ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(best_weights, best_biases, DNN_NET_FILE):\n",
    "    W_1, W_2, W_3, W_4 = [np.array(best_weights['h1'], ndmin=2), np.array(best_weights['h2'], ndmin=2),\n",
    "                          np.array(best_weights['h3'], ndmin=2), np.array([], ndmin=2)]\n",
    "    W_1, W_2, W_3, W_4 = W_1.T, W_2.T, W_3.T, W_4\n",
    "\n",
    "    print('W_1: ', W_1.shape, 'W_2: ', W_2.shape, 'W_3: ', W_3.shape, 'W_4: ', W_4.shape)\n",
    "\n",
    "    b_1, b_2, b_3, b_4 = [np.array(best_biases['b1'], ndmin=2), np.array(best_biases['b2'], ndmin=2),\n",
    "                          np.array(best_biases['b3'], ndmin=2), np.array([], ndmin=2)]\n",
    "    b_1, b_2, b_3, b_4 = b_1.T, b_2.T, b_3.T, b_4\n",
    "\n",
    "    print('b_1: ', b_1.shape, 'b_2: ', b_2.shape, 'b_3: ', b_3.shape, 'b_4: ', b_4.shape)\n",
    "\n",
    "    Wo, bo = np.array(best_weights['out'], ndmin=2), np.array(best_biases['out'], ndmin=2)\n",
    "\n",
    "    Wo1_1, Wo1_2, Wo1_3, Wo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:, :963].T]\n",
    "    bo1_1, bo1_2, bo1_3, bo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2),\n",
    "                                  bo[:, :963].T]\n",
    "    # Wo1_1, Wo1_2, Wo1_3, Wo1_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:963]]\n",
    "    # bo1_1, bo1_2, bo1_3, bo1_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2),\n",
    "    #                               np.reshape(np.transpose(bo[:963]), (963, 1))]\n",
    "\n",
    "\n",
    "    Wo2_1, Wo2_2, Wo2_3, Wo2_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2), Wo[:, 963:].T]\n",
    "    bo2_1, bo2_2, bo2_3, bo2_4 = [np.array([], ndmin=2), np.array([], ndmin=2), np.array([], ndmin=2),\n",
    "                                  bo[:, 963:].T]\n",
    "    # Wo2_1, Wo2_2, Wo2_3, Wo2_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2), Wo[963:]]\n",
    "    # bo2_1, bo2_2, bo2_3, bo2_4 = [np.array([],ndmin=2), np.array([],ndmin=2), np.array([],ndmin=2),\n",
    "    #                               np.reshape(np.transpose(bo[963:]), (963, 1))]\n",
    "\n",
    "    print('Wo1_1: ', Wo1_1.shape, 'Wo1_2: ', Wo1_2.shape, 'Wo1_3: ', Wo1_3.shape, 'Wo1_4: ', Wo1_4.shape)\n",
    "    print('bo1_1: ', bo1_1.shape, 'bo1_2: ', bo1_2.shape, 'bo1_3: ', bo1_3.shape, 'bo1_4: ', bo1_4.shape)\n",
    "\n",
    "    print('Wo2_1: ', Wo2_1.shape, 'Wo2_2: ', Wo2_2.shape, 'Wo2_3: ', Wo2_3.shape, 'Wo2_4: ', Wo2_4.shape)\n",
    "    print('bo2_1: ', bo2_1.shape, 'bo2_2: ', bo2_2.shape, 'bo2_3: ', bo2_3.shape, 'bo2_4: ', bo2_4.shape)\n",
    "\n",
    "    # Param_Dict = {'W': np.transpose([W_1,W_2, W_3]), 'b':np.transpose([b_1,b_2, b_3]) }\n",
    "    # Param_Dict = {'W': { (W_1, W_2, (W_3)}, 'b': {(b_1), (b_2), (b_3)}}\n",
    "    Param_Dict = np.core.records.fromarrays(\n",
    "        [[W_1, W_2, W_3, W_4], [b_1, b_2, b_3, b_4], [Wo1_1, Wo1_2, Wo1_3, Wo1_4], [bo1_1, bo1_2, bo1_3, bo1_4],\n",
    "         [Wo2_1, Wo2_2, Wo2_3, Wo2_4], [bo2_1, bo2_2, bo2_3, bo2_4]], names='W,b,Wo1,bo1,Wo2,bo2')\n",
    "\n",
    "    # print(Param_Dict.shape, Param_Dict)\n",
    "    master_dict = {'struct_net': [Param_Dict]}\n",
    "\n",
    "    sio.savemat(DNN_NET_FILE, master_dict, format='5', long_field_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN ops\n",
    "y_p, loss_op, m_r, m_i, _ = calc(X, Y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss=loss_op)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "\n",
    "for epoch in range(opts.sgd_max_epoch):\n",
    "\n",
    "    s = time.time()\n",
    "    cost_sum = 0.0\n",
    "\n",
    "    ##### Create Train Batch, evaluate COST, Weight, Bias #####\n",
    "    ###########################################################\n",
    "    for batch_num, (batch_x, batch_y) in enumerate(opts.next_batch(opts.trData.shape[0], opts.sgd_batch_size)):\n",
    "\n",
    "        _, c, epoch_w, epoch_b  = sess.run([train_op, loss_op, weights, biases ],\n",
    "                                          feed_dict={X: batch_x, Y: batch_y})\n",
    "        cost_sum += c\n",
    "\n",
    "        if batch_num % 1000 == 0:\n",
    "            print('[T] - Epoch:', epoch, ', batch_num:', batch_num, \", Cost:\", c, \"Cost Sum:\", cost_sum)\n",
    "\n",
    "    print('[T] - Epoch:', epoch, \",Sum:\", cost_sum)\n",
    "\n",
    "\n",
    "    ################ Validation in whole batch ################\n",
    "    ###########################################################\n",
    "\n",
    "    avg_cost, sum_mse_r, sum_mse_i = 0.0, np.zeros(n_classes//2), np.zeros(n_classes//2)\n",
    "    ##### Create DEV Batch, evaluate COST, Weight, Bias #####\n",
    "    ###########################################################\n",
    "    for batch_num, (batch_x, batch_y) in enumerate(opts.next_batch(opts.cvData.shape[0], opts.sgd_batch_size, isTrainCycle=False)):\n",
    "\n",
    "        mse_r, mse_i = sess.run([m_r, m_i], feed_dict={X: batch_x, Y: batch_y})\n",
    "        # print(sum_mse_r.shape, mse_r.shape, mse_i.shape)\n",
    "        sum_mse_r += (mse_r)\n",
    "        sum_mse_i += (mse_i)\n",
    "\n",
    "    avg_cost = - np.mean(sum_mse_r) - np.mean(sum_mse_i)\n",
    "    # print('[D] - Epoch:', epoch, \", Mean Real:\", - np.mean(sum_mse_r), \", Mean Img:\" , - np.mean(sum_mse_i), \", Avg Cost:\", avg_cost)\n",
    "\n",
    "    print(\"[D] - Epoch:{0}, Avg Cost:{1:.5f}, Real(mean,var):({2:.5f},{3:.5f}), Img(mean,var):({4:.5f},{5:.5f})\".format( epoch,avg_cost, -np.mean(sum_mse_r),-np.var(sum_mse_r), -np.mean(sum_mse_i),\n",
    "          -np.var(sum_mse_i)) )\n",
    "\n",
    "    ####### Min validation error, update weights, bias #########\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    if avg_cost > Best_Cost:\n",
    "        Best_Cost = avg_cost\n",
    "        Best_Weight = epoch_w\n",
    "        Best_Bias = epoch_b\n",
    "        Best_epoch = epoch\n",
    "\n",
    "        print('***** [D] - Best Model at Epoch:', epoch, \", Avg Cost:\", avg_cost, '*****')\n",
    "\n",
    "\n",
    "    ######################  Write Model File ###################\n",
    "    ############################################################\n",
    "\n",
    "    print('[] - Elapsed Time: {0:.2f}(s), {1:.2f}(min)'.format( time.time()-s, (time.time()-s)/60 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training DONE !!!\n",
    "print(\"Optimization Finished!\")\n",
    "print('***** [-] - Best Model at Epoch:', Best_epoch, \", Best Val Cost:\", Best_Cost, '*****')\n",
    "write_file(Best_Weight, Best_Bias, DNN_NET_FILE)\n",
    "print(\" File Write complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
