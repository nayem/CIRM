{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN\n",
    "### with pretraining, non-freeze previous layers\n",
    "Sep 25, 2018\n",
    "\n",
    "e04_nf-> batch size 256, 1/10 train dev data, each layer 1024 cell\n",
    "\n",
    "e04v2_nf-> batch size 256, 1/10 train dev data, each layer 2048 cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.11</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "- Read parameters from .mat files\n",
    "- save in Opt object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# %%pixie_debugger\n",
    "Data_VERSION = '_e10v5'\n",
    "Test_Data_VERSION = '_e10v5'\n",
    "\n",
    "Code_VERSION = '_e04v2_nf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "DNN_DATA_FILE = \"./dnn_models/DNN_datas\"+Data_VERSION+\".mat\"\n",
    "\n",
    "DNN_TEST_FILE = \"./dnn_models/Test_datas\"+Test_Data_VERSION+\".mat\"\n",
    "\n",
    "\n",
    "DNN_PARAMS_FILE = \"./dnn_models/DNN_params\"+Data_VERSION+\".mat\"\n",
    "\n",
    "DNN_NET_FILE = \"./dnn_models/DNN_net\"+Code_VERSION+\".mat\"\n",
    "\n",
    "\n",
    "# Best model, after adding 1st dense layer\n",
    "MODEL_FILE_1 = \"./dnn_models/weights1\"+ Code_VERSION+\"_{epoch:02d}.h5\"\n",
    "\n",
    "# Best model, after adding 2nd dense layer\n",
    "MODEL_FILE_2 = \"./dnn_models/weights2\"+ Code_VERSION+\"_{epoch:02d}.h5\"\n",
    "\n",
    "# Best model, after adding 3rd dense layer (final)\n",
    "MODEL_FILE_3 = \"./dnn_models/weights3\"+ Code_VERSION+\"_{epoch:02d}.h5\"\n",
    "\n",
    "\n",
    "#dummy save file\n",
    "SAVE_MODEL_FILE = \"./dnn_models/py_model\"+ Code_VERSION+\".h5\"\n",
    "\n",
    "# estimated real+imag for test dataset\n",
    "OUTPUT_FILE = \"./dnn_models/Real_Imag\"+Code_VERSION+\".mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "class Opts:\n",
    "\n",
    "    def __init__(self, FILE_PARA, FILE_DATA, FILE_TEST):\n",
    "        \n",
    "        # Basic parameters\n",
    "        with h5py.File(FILE_PARA, 'r') as f:\n",
    "            key_list = list(f.keys())\n",
    "            print('File name <{0}>\\nOpt keys (Total {1}):'.format(FILE_PARA,len(f['opts'].items())) )\n",
    "\n",
    "            for k, v in f['opts'].items():\n",
    "\n",
    "                print(k,end=', ')\n",
    "\n",
    "                if k == 'ARMA_order':\n",
    "                    self.ARMA_order = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'ada_grad_eps':\n",
    "                    self.ada_grad_eps = np.array(v)[0][0]\n",
    "                    \n",
    "                elif k == 'ada_sgd_scale':\n",
    "                    self.ada_sgd_scale = np.array(v)[0][0]\n",
    "                    \n",
    "                elif k == 'change_momentum_point':\n",
    "                    self.change_momentum_point = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'cost_function':\n",
    "                    self.cost_function = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.cost_function += chr(c[0])\n",
    "\n",
    "                elif k == 'cv_interval':\n",
    "                    self.cv_interval = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'dim_input':\n",
    "                    self.dim_input = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'dim_output':\n",
    "                    self.dim_output = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'drop_ratio':\n",
    "                    self.drop_ratio = np.array(v)[0][0]\n",
    "                    \n",
    "                elif k == 'eval_on_gpu':\n",
    "                    self.eval_on_gpu = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'final_momentum':\n",
    "                    self.final_momentum = int(np.array(v)[0][0])\n",
    "                   \n",
    "                elif k == 'hid_struct':\n",
    "                    self.hid_struct = np.array(v)\n",
    "                    \n",
    "                elif k == 'initial_momentum':\n",
    "                    self.initial_momentum = np.array(v)[0][0]\n",
    "                    \n",
    "                elif k == 'isDropout':\n",
    "                    self.isDropout = 0\n",
    "                    \n",
    "                elif k == 'isDropoutInput':\n",
    "                    self.isDropoutInput = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'isGPU':\n",
    "                    self.isGPU = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'isNormalize':\n",
    "                    self.isNormalize = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'isPretrain':\n",
    "                    self.isPretrain = int(np.array(v)[0][0])\n",
    "                    \n",
    "                elif k == 'learner':\n",
    "                    self.learner = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.learner += chr(c[0])\n",
    "\n",
    "                elif k == 'net_struct':\n",
    "                    self.net_struct = np.array(v)\n",
    "#                     for n_s in np.array(v):\n",
    "#                         print('Opts Net Stuct:',n_s[0])\n",
    "\n",
    "                elif k == 'rbm_batch_size':\n",
    "                    self.rbm_batch_size = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'rbm_learn_rate_binary':\n",
    "                    self.rbm_learn_rate_binary = np.array(v)\n",
    "\n",
    "                elif k == 'rbm_learn_rate_real':\n",
    "                    self.rbm_learn_rate_real = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'rbm_max_epoch':\n",
    "                    self.rbm_max_epoch = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'save_on_fly':\n",
    "                    self.save_on_fly = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'sgd_batch_size':\n",
    "                    self.sgd_batch_size = int(np.array(v)[0][0]) # BATCH_SIZE for training net\n",
    "                    print(\"self.sgd_batch_size:\",self.sgd_batch_size)\n",
    "\n",
    "                elif k == 'sgd_learn_rate':\n",
    "                    self.sgd_learn_rate = np.array(v)\n",
    "\n",
    "                elif k == 'sgd_max_epoch':\n",
    "                    self.sgd_max_epoch = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'split_tanh1_c1':\n",
    "                    self.split_tanh1_c1 = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'split_tanh1_c2':\n",
    "                    self.split_tanh1_c2 = int(np.array(v)[0][0])\n",
    "\n",
    "                elif k == 'unit_type_hidden':\n",
    "                    self.unit_type_hidden = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_hidden += chr(c[0])\n",
    "\n",
    "                elif k == 'unit_type_output':\n",
    "                    self.unit_type_output = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_output += chr(c[0])\n",
    "\n",
    "        # Training and Dev Data \n",
    "        with h5py.File(FILE_DATA, 'r') as f:\n",
    "            \n",
    "            print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_DATA,len(f.keys())) )\n",
    "\n",
    "            for k, v in f.items():\n",
    "                print(k, end=', ')\n",
    "                \n",
    "                # Train Data\n",
    "                if k == 'trData':\n",
    "                    self.trData = np.transpose(np.array(v))\n",
    "                    print(\"trData.shape: \", self.trData.shape)\n",
    "                    print(\"trData-> mean:\", np.mean(self.trData), \", var:\", np.var(self.trData), \", std:\",\n",
    "                          np.std(self.trData), \", range:\", (np.amin(self.trData),np.amax(self.trData)))\n",
    "                elif k == 'trLabel_i':\n",
    "                    self.trLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_i.shape: \", self.trLabel_i.shape)\n",
    "                    print(\"trLabel_i-> mean:\", np.mean(self.trLabel_i), \", var:\", np.var(self.trLabel_i), \", std:\",\n",
    "                          np.std(self.trLabel_i), \", range:\", (np.amin(self.trLabel_i),np.amax(self.trLabel_i)))\n",
    "                elif k == 'trLabel_r':\n",
    "                    self.trLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"trLabel_r.shape: \", self.trLabel_r.shape)\n",
    "                    print(\"trLabel_r-> mean:\", np.mean(self.trLabel_r), \", var:\", np.var(self.trLabel_r), \", std:\",\n",
    "                          np.std(self.trLabel_r), \", range:\", (np.amin(self.trLabel_r),np.amax(self.trLabel_r)))\n",
    "                elif k == 'trNumframes':\n",
    "                    self.trNumframes = np.transpose(np.array(v))\n",
    "                    print(\"trNumframes.shape: \", self.trNumframes.shape)\n",
    "                    \n",
    "                    \n",
    "                # Dev Data\n",
    "                elif k == 'cvData':\n",
    "                    self.cvData = np.transpose(np.array(v))\n",
    "                    print(\"cvData.shape: \", self.cvData.shape)\n",
    "                    print(\"cvData-> mean:\", np.mean(self.cvData), \", var:\", np.var(self.cvData), \", std:\",\n",
    "                          np.std(self.cvData), \", range:\", (np.amin(self.cvData),np.amax(self.cvData)))\n",
    "                elif k == 'cvLabel_i':\n",
    "                    self.cvLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_i.shape: \", self.cvLabel_i.shape)\n",
    "                    print(\"cvLabel_i-> mean:\", np.mean(self.cvLabel_i), \", var:\", np.var(self.cvLabel_i), \", std:\",\n",
    "                          np.std(self.cvLabel_i), \", range:\", (np.amin(self.cvLabel_i),np.amax(self.cvLabel_i)))\n",
    "                elif k == 'cvLabel_r':\n",
    "                    self.cvLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"cvLabel_r.shape: \", self.cvLabel_r.shape)\n",
    "                    print(\"cvLabel_r-> mean:\", np.mean(self.cvLabel_r), \", var:\", np.var(self.cvLabel_r), \", std:\",\n",
    "                          np.std(self.cvLabel_r), \", range:\", (np.amin(self.cvLabel_r),np.amax(self.cvLabel_r)))\n",
    "                elif k == 'cvNumframes':\n",
    "                    self.cvNumframes = np.transpose(np.array(v))\n",
    "                    print(\"cvNumframes.shape: \", self.cvNumframes.shape)\n",
    "                    \n",
    "\n",
    "            self.trLabel = np.concatenate((self.trLabel_r, self.trLabel_i), axis=1)\n",
    "            self.cvLabel = np.concatenate((self.cvLabel_r, self.cvLabel_i), axis=1)\n",
    " \n",
    "            \n",
    "        # Test Data \n",
    "        print(FILE_TEST, os.path.isfile(FILE_TEST))\n",
    "        \n",
    "        with h5py.File(FILE_TEST, 'r') as f:\n",
    "            print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_TEST,len(f.keys())) )\n",
    "\n",
    "            \n",
    "            \n",
    "            for k, v in f.items():\n",
    "                print(k, end=', ')\n",
    "                \n",
    "                #Test Data\n",
    "                if k == 'teData':\n",
    "                    self.teData = np.transpose(np.array(v))\n",
    "                    print(\"teData.shape: \", self.teData.shape)\n",
    "                    print(\"teData-> mean:\", np.mean(self.teData), \", var:\", np.var(self.teData), \", std:\",\n",
    "                          np.std(self.teData), \", range:\", (np.amin(self.teData),np.amax(self.teData)))\n",
    "                    \n",
    "                elif k == 'teLabel_i':\n",
    "                    self.teLabel_i = np.transpose(np.array(v))\n",
    "                    print(\"teLabel_i.shape: \", self.teLabel_i.shape)\n",
    "                    print(\"teLabel_i-> mean:\", np.mean(self.teLabel_i), \", var:\", np.var(self.teLabel_i), \", std:\",\n",
    "                          np.std(self.teLabel_i), \", range:\", (np.amin(self.teLabel_i),np.amax(self.teLabel_i)))\n",
    "                    \n",
    "                elif k == 'teLabel_r':\n",
    "                    self.teLabel_r = np.transpose(np.array(v))\n",
    "                    print(\"teLabel_r.shape: \", self.teLabel_r.shape)\n",
    "                    print(\"teLabel_r-> mean:\", np.mean(self.teLabel_r), \", var:\", np.var(self.teLabel_r), \", std:\",\n",
    "                          np.std(self.teLabel_r), \", range:\", (np.amin(self.teLabel_r),np.amax(self.teLabel_r)))\n",
    "                \n",
    "                elif k == 'teNumframes':\n",
    "                    self.teNumframes = np.transpose(np.array(v))\n",
    "                    print(\"teNumframes.shape: \", self.teNumframes.shape)\n",
    "                    \n",
    "            \n",
    "            self.teLabel = np.concatenate((self.teLabel_r, self.teLabel_i), axis=1)\n",
    "\n",
    "\n",
    "    def ready_batchID(self, total_num_samples, batch_size):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        batchID = []\n",
    "        num_batch = math.ceil(total_num_samples/batch_size)\n",
    "\n",
    "        for b in range( int(num_batch) ):\n",
    "            s = b*batch_size\n",
    "            e = (b+1)*batch_size -1\n",
    "\n",
    "            if e >= total_num_samples:\n",
    "                e = total_num_samples - 1\n",
    "\n",
    "            batchID.append((s,e))\n",
    "\n",
    "        return np.array(batchID,ndmin=2)\n",
    "\n",
    "\n",
    "    def suffle_data(self, total_num_samples):\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        return  np.random.permutation(total_num_samples)\n",
    "\n",
    "\n",
    "    def next_batch(self, total_num_samples, batch_size, cycle):\n",
    "        '''\n",
    "        Currently we use this one. \n",
    "        \n",
    "        parameters: \n",
    "            total_num_samples <int> = Shape[1] of data\n",
    "            batch_size <int>        \n",
    "            cycle <string>  = train, dev, test\n",
    "        '''\n",
    "        # TRAIN: total_num_samples = self.trData.shape[0] = 195192\n",
    "        # DEV: total_num_samples = self.trData.shape[0] = 44961\n",
    "\n",
    "        while True:\n",
    "            batchID = self.ready_batchID(total_num_samples, batch_size) \n",
    "            seq = self.suffle_data(total_num_samples)\n",
    "\n",
    "            for batch in range(batchID.shape[0]):\n",
    "                if cycle.lower()=='train':\n",
    "                    x = opts.trData[ seq[batchID[batch][0]:batchID[batch][1] ] ]\n",
    "                    y = opts.trLabel[ seq[batchID[batch][0]:batchID[batch][1] ] ]\n",
    "\n",
    "                elif cycle.lower()=='dev':\n",
    "                    x = opts.cvData[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "                    y = opts.cvLabel[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "                    \n",
    "                elif cycle.lower()=='test':\n",
    "                    x = opts.teData[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "                    y = opts.teLabel[seq[batchID[batch][0]:batchID[batch][1]]]\n",
    "\n",
    "                # print('Next Batch', x.shape, y.shape)\n",
    "                yield [x, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name <./dnn_models/DNN_params_e10v5.mat>\n",
      "Opt keys (Total 34):\n",
      "ARMA_order, ada_grad_eps, ada_sgd_scale, change_momentum_point, cost_function, cv_interval, dim_input, dim_output, drop_ratio, eval_on_gpu, final_momentum, hid_struct, initial_momentum, isDropout, isDropoutInput, isGPU, isNormalize, isPretrain, learner, net_struct, rbm_batch_size, rbm_learn_rate_binary, rbm_learn_rate_real, rbm_max_epoch, save_on_fly, sgd_batch_size, self.sgd_batch_size: 1024\n",
      "sgd_learn_rate, sgd_max_epoch, split_tanh1_c1, split_tanh1_c2, tr_mu, tr_std, unit_type_hidden, unit_type_output, \n",
      "\n",
      "File name <./dnn_models/DNN_datas_e10v5.mat>\n",
      "Opt h5py keys (Total 8):\n",
      "cvData, cvData.shape:  (449610, 1230)\n",
      "cvData-> mean: -3.12831e-06 , var: 0.607645 , std: 0.779516 , range: (-9.6298428, 10.49406)\n",
      "cvLabel_i, cvLabel_i.shape:  (449610, 963)\n",
      "cvLabel_i-> mean: -1.2493e-05 , var: 0.0129563 , std: 0.113826 , range: (-10.0, 10.0)\n",
      "cvLabel_r, cvLabel_r.shape:  (449610, 963)\n",
      "cvLabel_r-> mean: 0.028573 , var: 0.0212577 , std: 0.1458 , range: (-10.0, 10.0)\n",
      "cvNumframes, cvNumframes.shape:  (3300, 1)\n",
      "trData, trData.shape:  (1951920, 1230)\n",
      "trData-> mean: -0.000807387 , var: 0.612503 , std: 0.782626 , range: (-11.557238, 12.486189)\n",
      "trLabel_i, trLabel_i.shape:  (1951920, 963)\n",
      "trLabel_i-> mean: 1.29036e-06 , var: 0.0133129 , std: 0.115382 , range: (-10.0, 10.0)\n",
      "trLabel_r, trLabel_r.shape:  (1951920, 963)\n",
      "trLabel_r-> mean: 0.0295324 , var: 0.021696 , std: 0.147296 , range: (-10.0, 10.0)\n",
      "trNumframes, trNumframes.shape:  (15000, 1)\n",
      "./dnn_models/Test_datas_e10v5.mat True\n",
      "\n",
      "\n",
      "File name <./dnn_models/Test_datas_e10v5.mat>\n",
      "Opt h5py keys (Total 7):\n",
      "#refs#, #subsystem#, teData, teData.shape:  (72440, 1230)\n",
      "teData-> mean: -0.00110349 , var: 0.556445 , std: 0.745952 , range: (-4.8041673, 5.2418036)\n",
      "teFilename_mix, teLabel_i, teLabel_i.shape:  (72440, 963)\n",
      "teLabel_i-> mean: -3.01466e-06 , var: 1.05955e-05 , std: 0.00325507 , range: (-0.03745788, 0.035189673)\n",
      "teLabel_r, teLabel_r.shape:  (72440, 963)\n",
      "teLabel_r-> mean: 0.0328164 , var: 0.00246607 , std: 0.0496596 , range: (-0.15398552, 0.43288523)\n",
      "teNumframes, teNumframes.shape:  (545, 1)\n"
     ]
    }
   ],
   "source": [
    "# %%pixie_debugger\n",
    "opts = Opts(DNN_PARAMS_FILE, DNN_DATA_FILE, DNN_TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data/Label Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "group = 2 # clean and noisy, clean is not ready yet\n",
    "data_partitions = 3 # train, dev and test\n",
    "\n",
    "index = np.arange(data_partitions)\n",
    "bar_width = .35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# There will be clean_means and clean_std too.\n",
    "\n",
    "noisy_means = [np.mean(opts.trData), np.mean(opts.cvData), np.mean(opts.teData)]\n",
    "noisy_std = [np.std(opts.trData), np.std(opts.cvData), np.std(opts.teData)]\n",
    "print(noisy_means, noisy_std)\n",
    "\n",
    "# print(\"teData-> mean:\", np.mean(self.teData), \", var:\", np.var(self.teData), \", std:\",\n",
    "#                           np.std(self.teData), \", range:\", (np.amin(self.teData),np.amax(self.teData)))\n",
    "\n",
    "\n",
    "rects_clean = ax.bar(index, noisy_means, bar_width,\n",
    "                    alpha=opacity, color='g',\n",
    "                    yerr=noisy_std, error_kw=error_config,\n",
    "                    label='Clean Features')\n",
    "\n",
    "rects_noisy = ax.bar(index + bar_width, noisy_means, bar_width,\n",
    "                    alpha=opacity, color='r',\n",
    "                    yerr=noisy_std, error_kw=error_config,\n",
    "                    label='Noisy Features')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Clean and Noisy Features')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_yticks(np.arange(-1, 1, step=0.05))\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(('Train\\n({0:.2f},{1:.2f})'.format(np.amin(opts.trData),np.amax(opts.trData)), \n",
    "                    'Dev\\n({0:.2f},{1:.2f})'.format(np.amin(opts.cvData),np.amax(opts.cvData)),\n",
    "                    'Test\\n({0:.2f},{1:.2f})'.format(np.amin(opts.teData),np.amax(opts.teData))))\n",
    "ax.set_title('Sharing Y axis')\n",
    "ax.legend()\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old cost functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def calc(x, y):\n",
    "    # Returns predictions and error\n",
    "    predictions = multilayer_NN(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    r, c = y.get_shape().as_list()\n",
    "    r = opts.sgd_batch_size\n",
    "    print('r:', type(r), r, ',c:', type(c), c)\n",
    "\n",
    "    # TRAIN: matlab\n",
    "    # cost1 = 0.5 * sum(sum((pred_real - label_real). ^ 2)) / num_sample;\n",
    "    # cost2 = 0.5 * sum(sum((pred_imag - label_imag). ^ 2)) / num_sample;\n",
    "    # cost = cost1 + cost2;\n",
    "    cost1 = tf.reduce_sum(tf.squared_difference(y[:, :c//2], predictions[:, :c//2]))\n",
    "    cost2 = tf.reduce_sum(tf.squared_difference(y[:, c//2:], predictions[:, c//2:]))\n",
    "    loss_t = 0.5*(cost1+cost2)/r\n",
    "    # mse = tf.losses.mean_squared_error(labels=y, predictions=predictions,weights=0.5)\n",
    "    # loss_t = tf.divide(tf.reduce_sum(mse), r)\n",
    "\n",
    "\n",
    "    # DEV: matlab\n",
    "    # dev_perfs = -mean(sum((dev_label_real - dev_netout1). ^ 2)) - mean(sum((dev_label_imag - dev_netout2). ^ 2));\n",
    "    mse_r = tf.reduce_sum(tf.squared_difference(y[:, :c // 2], predictions[:, :c // 2]), axis=0)\n",
    "    mse_i = tf.reduce_sum(tf.squared_difference(y[:, c // 2:], predictions[:, c // 2:]), axis=0)\n",
    "\n",
    "    loss_d = -tf.reduce_mean(mse_r)-tf.reduce_mean(mse_i)\n",
    "\n",
    "\n",
    "    return [predictions, loss_t, mse_r, mse_i, loss_d ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "    r= K.cast(r,dtype='float32') # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    cost_r = K.sum(K.square(yTrue[:, :(c//2)]- yPred[:, :(c//2)]))\n",
    "    cost_i = K.sum(K.square(yTrue[:, (c//2):]- yPred[:, (c//2):]))\n",
    "    \n",
    "    cost = 0.5*(cost_r+cost_i)/r\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf_session = K.get_session()\n",
    "val1 = np.array([[1, 2], [3, 4], [6, 7]])\n",
    "val2 = np.array([[2, 3], [4, 5], [8, 9]])\n",
    "\n",
    "kvar1 = K.variable(value=val1)\n",
    "kvar2 = K.variable(value=val2)\n",
    "                   \n",
    "r = K.variable(value=customLoss(kvar1,kvar2))\n",
    "                   \n",
    "K.eval(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256 #opts.sgd_batch_size\n",
    "epochs = 80\n",
    "\n",
    "'''\n",
    "n_train, n_train_dim = <number of samples for training, input dimentions>, final will be (1951920, 1230)\n",
    "n_dev, n_dev_dim = <number of samples for development, input dimentions>, final will be (449610, 1230)\n",
    "n_test, n_test_dim =<number of samples for testing, input dimentions>, final will be (72440, 1230)\n",
    "\n",
    "'''\n",
    "n_train, n_train_dim = (1951920, 1230) # opts.trData.shape = (1951920, 1230)\n",
    "n_dev, n_dev_dim = (449610, 1230) # opts.cvData.shape =(449610, 1230)\n",
    "n_test, n_test_dim = (72440, 1230) # opts.teData.shape =(72440, 1230)\n",
    "\n",
    "n_hidden_1 = 1024  # 1st layer number of neurons\n",
    "n_hidden_2 = 1024  # 2nd layer number of neurons\n",
    "n_hidden_3 = 1024  # 3rd layer number of neurons\n",
    "\n",
    "n_classes = (963 + 963)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_e4v1 (Dense)         (None, 1024)              1260544   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1926)              1974150   \n",
      "=================================================================\n",
      "Total params: 3,234,694\n",
      "Trainable params: 3,234,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7624/7624 [==============================] - 377s - loss: 16.0784 - acc: 0.0276 - mean_squared_error: 0.0167 - mean_absolute_error: 0.0455 - mean_absolute_percentage_error: 24548.6627 - cosine_proximity: -1.1815e-04 - val_loss: 15.5544 - val_acc: 0.0274 - val_mean_squared_error: 0.0162 - val_mean_absolute_error: 0.0444 - val_mean_absolute_percentage_error: 7840.6323 - val_cosine_proximity: -1.1629e-04\n",
      "Epoch 2/80\n",
      "7624/7624 [==============================] - 357s - loss: 15.7809 - acc: 0.0295 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0437 - mean_absolute_percentage_error: 5469.5407 - cosine_proximity: -1.2286e-04 - val_loss: 15.5164 - val_acc: 0.0289 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0437 - val_mean_absolute_percentage_error: 5333.5210 - val_cosine_proximity: -1.1780e-04\n",
      "Epoch 3/80\n",
      "7624/7624 [==============================] - 391s - loss: 15.7667 - acc: 0.0297 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0434 - mean_absolute_percentage_error: 5499.9322 - cosine_proximity: -1.2393e-04 - val_loss: 15.5089 - val_acc: 0.0357 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0434 - val_mean_absolute_percentage_error: 5524.6781 - val_cosine_proximity: -1.1867e-04\n",
      "Epoch 4/80\n",
      "7624/7624 [==============================] - 392s - loss: 15.7618 - acc: 0.0299 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0433 - mean_absolute_percentage_error: 4915.7748 - cosine_proximity: -1.2437e-04 - val_loss: 15.4847 - val_acc: 0.0312 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0431 - val_mean_absolute_percentage_error: 4935.0018 - val_cosine_proximity: -1.1897e-04\n",
      "Epoch 5/80\n",
      "7624/7624 [==============================] - 374s - loss: 15.7583 - acc: 0.0295 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0433 - mean_absolute_percentage_error: 4536.9989 - cosine_proximity: -1.2463e-04 - val_loss: 15.4646 - val_acc: 0.0303 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0427 - val_mean_absolute_percentage_error: 4212.3179 - val_cosine_proximity: -1.2078e-04\n",
      "Epoch 6/80\n",
      "7624/7624 [==============================] - 355s - loss: 15.7599 - acc: 0.0297 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0433 - mean_absolute_percentage_error: 4404.3469 - cosine_proximity: -1.2475e-04 - val_loss: 15.4809 - val_acc: 0.0292 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0429 - val_mean_absolute_percentage_error: 3045.3488 - val_cosine_proximity: -1.1997e-04\n",
      "Epoch 7/80\n",
      "7624/7624 [==============================] - 385s - loss: 15.7574 - acc: 0.0302 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0432 - mean_absolute_percentage_error: 4294.0672 - cosine_proximity: -1.2473e-04 - val_loss: 15.4750 - val_acc: 0.0302 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0429 - val_mean_absolute_percentage_error: 3247.0389 - val_cosine_proximity: -1.2032e-04\n",
      "Epoch 8/80\n",
      "7624/7624 [==============================] - 417s - loss: 15.7594 - acc: 0.0298 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0432 - mean_absolute_percentage_error: 4277.9777 - cosine_proximity: -1.2471e-04 - val_loss: 15.4689 - val_acc: 0.0288 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0427 - val_mean_absolute_percentage_error: 3063.2754 - val_cosine_proximity: -1.2041e-04\n",
      "Epoch 9/80\n",
      "7624/7624 [==============================] - 393s - loss: 15.7582 - acc: 0.0298 - mean_squared_error: 0.0164 - mean_absolute_error: 0.0432 - mean_absolute_percentage_error: 4169.6498 - cosine_proximity: -1.2468e-04 - val_loss: 15.4702 - val_acc: 0.0312 - val_mean_squared_error: 0.0161 - val_mean_absolute_error: 0.0428 - val_mean_absolute_percentage_error: 3853.5026 - val_cosine_proximity: -1.2081e-04\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(n_hidden_1, activation='relu', input_shape=(n_train_dim,),name='dense_1_e4v1'))\n",
    "# we will add 2 more layers\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE_1, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = customLoss, optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(opts.next_batch(n_train, batch_size, cycle='train'),  \n",
    "                    validation_data=opts.next_batch(n_dev, batch_size, cycle='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train//batch_size,\n",
    "                    validation_steps=n_dev//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "# history = model.fit_generator(opts.next_batch(n_train, batch_size, cycle='train'),  \n",
    "#                     validation_data=opts.next_batch(n_dev, batch_size, cycle='dev'),\n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_train/batch_size)),\n",
    "#                     validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Best_weight_1=\"./dnn_models/weights1\"+ Code_VERSION+\"_02.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 2nd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_e4v1 (Dense)         (None, 1024)              1260544   \n",
      "_________________________________________________________________\n",
      "dense_2_e4v1 (Dense)         (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1926)              1974150   \n",
      "=================================================================\n",
      "Total params: 4,284,294\n",
      "Trainable params: 4,284,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7624/7624 [==============================] - 492s - loss: 15.6783 - acc: 0.0259 - mean_squared_error: 0.0163 - mean_absolute_error: 0.0426 - mean_absolute_percentage_error: 4769.8290 - cosine_proximity: -1.2926e-04 - val_loss: 15.3804 - val_acc: 0.0252 - val_mean_squared_error: 0.0160 - val_mean_absolute_error: 0.0420 - val_mean_absolute_percentage_error: 2651.7655 - val_cosine_proximity: -1.2617e-04\n",
      "Epoch 2/80\n",
      "7624/7624 [==============================] - 466s - loss: 15.6394 - acc: 0.0258 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0423 - mean_absolute_percentage_error: 3556.1144 - cosine_proximity: -1.3105e-04 - val_loss: 15.3598 - val_acc: 0.0261 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2289.6063 - val_cosine_proximity: -1.2801e-04\n",
      "Epoch 3/80\n",
      "7624/7624 [==============================] - 478s - loss: 15.6343 - acc: 0.0258 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0423 - mean_absolute_percentage_error: 3366.6714 - cosine_proximity: -1.3143e-04 - val_loss: 15.3587 - val_acc: 0.0268 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2444.2676 - val_cosine_proximity: -1.2701e-04\n",
      "Epoch 4/80\n",
      "7624/7624 [==============================] - 466s - loss: 15.6308 - acc: 0.0259 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3277.2850 - cosine_proximity: -1.3160e-04 - val_loss: 15.3555 - val_acc: 0.0269 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0419 - val_mean_absolute_percentage_error: 2549.2858 - val_cosine_proximity: -1.2799e-04\n",
      "Epoch 5/80\n",
      "7624/7624 [==============================] - 483s - loss: 15.6296 - acc: 0.0258 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3205.2080 - cosine_proximity: -1.3185e-04 - val_loss: 15.3538 - val_acc: 0.0301 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2177.4226 - val_cosine_proximity: -1.2823e-04\n",
      "Epoch 6/80\n",
      "7624/7624 [==============================] - 481s - loss: 15.6280 - acc: 0.0261 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3199.4612 - cosine_proximity: -1.3195e-04 - val_loss: 15.3466 - val_acc: 0.0294 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2157.9829 - val_cosine_proximity: -1.2803e-04\n",
      "Epoch 7/80\n",
      "7624/7624 [==============================] - 473s - loss: 15.6278 - acc: 0.0261 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3191.7513 - cosine_proximity: -1.3199e-04 - val_loss: 15.3509 - val_acc: 0.0265 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0416 - val_mean_absolute_percentage_error: 2209.4150 - val_cosine_proximity: -1.2788e-04\n",
      "Epoch 8/80\n",
      "7624/7624 [==============================] - 495s - loss: 15.6266 - acc: 0.0260 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3209.9447 - cosine_proximity: -1.3201e-04 - val_loss: 15.3520 - val_acc: 0.0277 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2468.4918 - val_cosine_proximity: -1.2772e-04\n",
      "Epoch 9/80\n",
      "7624/7624 [==============================] - 473s - loss: 15.6274 - acc: 0.0263 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3187.7746 - cosine_proximity: -1.3202e-04 - val_loss: 15.3471 - val_acc: 0.0249 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0417 - val_mean_absolute_percentage_error: 2216.9558 - val_cosine_proximity: -1.2815e-04\n",
      "Epoch 10/80\n",
      "7624/7624 [==============================] - 483s - loss: 15.6250 - acc: 0.0263 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3152.1277 - cosine_proximity: -1.3202e-04 - val_loss: 15.3436 - val_acc: 0.0241 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0416 - val_mean_absolute_percentage_error: 2053.6447 - val_cosine_proximity: -1.2836e-04\n",
      "Epoch 11/80\n",
      "7624/7624 [==============================] - 511s - loss: 15.6271 - acc: 0.0266 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0422 - mean_absolute_percentage_error: 3103.8021 - cosine_proximity: -1.3207e-04 - val_loss: 15.3590 - val_acc: 0.0263 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0418 - val_mean_absolute_percentage_error: 2351.8926 - val_cosine_proximity: -1.2867e-04\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "# SAVE_MODEL_FILE_2 = \"./dnn_models/py_model_2_\"+ Code_VERSION+\".h5\"\n",
    "model = load_model(Best_weight_1, custom_objects={'customLoss':customLoss})\n",
    "\n",
    "model.pop()\n",
    "\n",
    "model.add(Dense(n_hidden_2, activation='relu',name='dense_2_e4v1'))\n",
    "# we will add 1 more layers\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE_2, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = customLoss, optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(opts.next_batch(n_train, batch_size, cycle='train'),  \n",
    "                    validation_data=opts.next_batch(n_dev, batch_size, cycle='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train//batch_size,\n",
    "                    validation_steps=n_dev//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "# history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "#                     validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "#                     validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Best_weight_2=\"./dnn_models/weights2\"+ Code_VERSION+\"_04.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 3rd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_e4v1 (Dense)         (None, 1024)              1260544   \n",
      "_________________________________________________________________\n",
      "dense_2_e4v1 (Dense)         (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3_e4v1 (Dense)         (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1926)              1974150   \n",
      "=================================================================\n",
      "Total params: 5,333,894\n",
      "Trainable params: 5,333,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7624/7624 [==============================] - 428s - loss: 15.6238 - acc: 0.0260 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0421 - mean_absolute_percentage_error: 3296.6255 - cosine_proximity: -1.3270e-04 - val_loss: 15.3296 - val_acc: 0.0253 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0416 - val_mean_absolute_percentage_error: 1833.8549 - val_cosine_proximity: -1.3074e-04\n",
      "Epoch 2/80\n",
      "7624/7624 [==============================] - 463s - loss: 15.5871 - acc: 0.0255 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0419 - mean_absolute_percentage_error: 2840.4432 - cosine_proximity: -1.3444e-04 - val_loss: 15.3239 - val_acc: 0.0235 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1965.4404 - val_cosine_proximity: -1.2924e-04\n",
      "Epoch 3/80\n",
      "7624/7624 [==============================] - 477s - loss: 15.5766 - acc: 0.0254 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0418 - mean_absolute_percentage_error: 2809.5829 - cosine_proximity: -1.3474e-04 - val_loss: 15.3266 - val_acc: 0.0257 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0415 - val_mean_absolute_percentage_error: 2008.4191 - val_cosine_proximity: -1.3100e-04\n",
      "Epoch 4/80\n",
      "7624/7624 [==============================] - 473s - loss: 15.5715 - acc: 0.0256 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0418 - mean_absolute_percentage_error: 2807.2079 - cosine_proximity: -1.3499e-04 - val_loss: 15.3242 - val_acc: 0.0250 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1953.9709 - val_cosine_proximity: -1.3061e-04\n",
      "Epoch 5/80\n",
      "7624/7624 [==============================] - 485s - loss: 15.5675 - acc: 0.0255 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0418 - mean_absolute_percentage_error: 2790.8585 - cosine_proximity: -1.3505e-04 - val_loss: 15.3261 - val_acc: 0.0261 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1940.0346 - val_cosine_proximity: -1.3161e-04\n",
      "Epoch 6/80\n",
      "7624/7624 [==============================] - 476s - loss: 15.5649 - acc: 0.0251 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0418 - mean_absolute_percentage_error: 2793.5040 - cosine_proximity: -1.3528e-04 - val_loss: 15.3256 - val_acc: 0.0245 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0413 - val_mean_absolute_percentage_error: 1912.1647 - val_cosine_proximity: -1.3104e-04\n",
      "Epoch 7/80\n",
      "7624/7624 [==============================] - 468s - loss: 15.5646 - acc: 0.0246 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0418 - mean_absolute_percentage_error: 2775.3158 - cosine_proximity: -1.3536e-04 - val_loss: 15.3275 - val_acc: 0.0240 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 2019.7535 - val_cosine_proximity: -1.3142e-04\n",
      "Epoch 8/80\n",
      "7624/7624 [==============================] - 466s - loss: 15.5630 - acc: 0.0245 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0417 - mean_absolute_percentage_error: 2757.6974 - cosine_proximity: -1.3547e-04 - val_loss: 15.3251 - val_acc: 0.0240 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1907.2287 - val_cosine_proximity: -1.3060e-04\n",
      "Epoch 9/80\n",
      "7624/7624 [==============================] - 433s - loss: 15.5605 - acc: 0.0250 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0417 - mean_absolute_percentage_error: 2740.3063 - cosine_proximity: -1.3556e-04 - val_loss: 15.3228 - val_acc: 0.0252 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1982.0133 - val_cosine_proximity: -1.3099e-04\n",
      "Epoch 10/80\n",
      "7624/7624 [==============================] - 472s - loss: 15.5612 - acc: 0.0250 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0417 - mean_absolute_percentage_error: 2739.0662 - cosine_proximity: -1.3558e-04 - val_loss: 15.3294 - val_acc: 0.0232 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1854.6203 - val_cosine_proximity: -1.3038e-04\n",
      "Epoch 11/80\n",
      "7624/7624 [==============================] - 473s - loss: 15.5602 - acc: 0.0252 - mean_squared_error: 0.0162 - mean_absolute_error: 0.0417 - mean_absolute_percentage_error: 2761.8042 - cosine_proximity: -1.3566e-04 - val_loss: 15.3254 - val_acc: 0.0245 - val_mean_squared_error: 0.0159 - val_mean_absolute_error: 0.0414 - val_mean_absolute_percentage_error: 1829.5136 - val_cosine_proximity: -1.3114e-04\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "# SAVE_MODEL_FILE_3 = \"./dnn_models/py_model_3_\"+ Code_VERSION+\".h5\"\n",
    "model = load_model(Best_weight_2, custom_objects={'customLoss':customLoss})\n",
    "\n",
    "model.pop()\n",
    "\n",
    "model.add(Dense(n_hidden_2, activation='relu',name='dense_3_e4v1'))\n",
    "model.add(Dense(n_classes, activation='linear'))\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE_3, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = customLoss, optimizer = 'adam', metrics = ['accuracy','mse', 'mae', 'mape', 'cosine'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(opts.next_batch(n_train, batch_size, cycle='train'),  \n",
    "                    validation_data=opts.next_batch(n_dev, batch_size, cycle='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train//batch_size,\n",
    "                    validation_steps=n_dev//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "# history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "#                     validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "#                     validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense_1_e4v1', 'trainable': True, 'batch_input_shape': (None, 1230), 'dtype': 'float32', 'units': 1024, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "\n",
      "{'name': 'dense_2_e4v1', 'trainable': True, 'units': 1024, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "\n",
      "{'name': 'dense_3_e4v1', 'trainable': True, 'units': 1024, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "\n",
      "{'name': 'dense_4', 'trainable': True, 'units': 1926, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    print (g,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Best_weight_3=\"./dnn_models/weights3\"+ Code_VERSION+\"_04.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72160/72440 [============================>.] - ETA: 0s(72440, 1926)\n"
     ]
    }
   ],
   "source": [
    "model = load_model(Best_weight_3, custom_objects={'customLoss':customLoss})\n",
    "\n",
    "\n",
    "y_hat = model.predict(opts.teData, verbose=1)\n",
    "\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sio.savemat(OUTPUT_FILE, {'y_hat':y_hat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unwrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unwrapAugmentedTF_wAvg(impt_mag,num_per_side,unwrap_mag=None):\n",
    "    \n",
    "#     Description: Unwrap the augmented time-frequency (T-F) representation and compute the average.\n",
    "\n",
    "#     Input:\n",
    "#         impt_mag: wrapped T-F representation with dimensions (2*T+1)*d x m\n",
    "#         T: number of frames to left and right of each frame used to augmented T-F representation\n",
    "#     Output:\n",
    "#         unwrap_avgmag: unwrapped and averaged T-F representation with dimensions d x m\n",
    "#         unwrap_mag: unwrapped T-F representation with dimensions d x (2*T+1) x m\n",
    "#\n",
    "        \n",
    "        sliding_window_len       = 2*num_per_side + 1\n",
    "        (numWrapFreqs,numFrames) = impt_mag.shape\n",
    "        numFreqs                 = numWrapFreqs//sliding_window_len\n",
    "        unwrap_avgmag            = np.zeros((numFreqs,numFrames))\n",
    "        \n",
    "        if num_per_side > 0:\n",
    "            if unwrap_mag is None:\n",
    "            \n",
    "                unwrap_mag        = np.zeros((numFreqs,sliding_window_len+1,numFrames))\n",
    "                curr_ind_location = np.ones((numFrames,1),dtype=int)\n",
    "                \n",
    "\n",
    "                for frameNum in range(numFrames):\n",
    "                    \n",
    "                    # Get the indices for the frames used in this augmented matrix\n",
    "                    frame_inds=[]\n",
    "                    for inds in range(frameNum-num_per_side,frameNum+num_per_side+1):\n",
    "                        if inds<0:\n",
    "                            frame_inds.append(0)\n",
    "                        elif inds>=numFrames:\n",
    "                            frame_inds.append(numFrames-1)\n",
    "                        else:\n",
    "                            frame_inds.append(inds)\n",
    "                    \n",
    "                    # Unwrap the data for this frame\n",
    "                    slid_win_data = np.reshape( impt_mag[:,frameNum], (numFreqs,sliding_window_len)) #Size d x (2*T + 1)\n",
    "                    \n",
    "                    for ind_num in range(len(frame_inds)):\n",
    "\n",
    "                        slid = np.array(slid_win_data[:,ind_num], ndmin=2).T\n",
    "                        unwrap_mag[:,curr_ind_location[frame_inds[ind_num]], frame_inds[ind_num]] = slid \n",
    "                        \n",
    "                        # Update counters\n",
    "                        curr_ind_location[frame_inds[ind_num]] = curr_ind_location[frame_inds[ind_num]] + 1\n",
    "\n",
    "\n",
    "            temp = np.zeros((numFreqs,sliding_window_len+1))\n",
    "            \n",
    "            for frameNum in range(numFrames):\n",
    "#                 print(unwrap_mag.shape)\n",
    "                \n",
    "                unwrap_avgmag[:,frameNum] = np.mean(unwrap_mag[:,:,frameNum],axis=1)\n",
    "#                 print(unwrap_mag.shape)\n",
    "#                 return\n",
    "\n",
    "            \n",
    "        else:\n",
    "            unwrap_avgmag = impt_mag;\n",
    "            unwrap_mag = 0;\n",
    "            \n",
    "        return unwrap_avgmag,unwrap_mag"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "reals=[]\n",
    "imags=None\n",
    "\n",
    "labwin = 1\n",
    "for e, y_ in enumerate(y_hat):\n",
    "    \n",
    "    numFrames = int(opts.teNumframes[e])\n",
    "    \n",
    "    real_output = y_[:numFrames,:out_vec_len]\n",
    "    imag_output = y_[:numFrames,out_vec_len:]\n",
    "#     print(e, numFrames, real_output.shape, imag_output.shape)\n",
    "    \n",
    "    real_output_unwrap, _ = unwrapAugmentedTF_wAvg(real_output.T,labwin)\n",
    "    imag_output_unwrap, _ = unwrapAugmentedTF_wAvg(imag_output.T,labwin)\n",
    "    \n",
    "    print('unwrap:',real_output_unwrap.shape, end=' , ')\n",
    "    \n",
    "#     reals= np.array([list(real_output_unwrap)], dtype=object) if reals is None else np.append(reals,[list(real_output_unwrap)], axis=0)\n",
    "    reals.append(real_output_unwrap)\n",
    "    print('reals:',len(reals))\n",
    "#     imags.append(imag_output_unwrap)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.savetxt('./dnn_models/test.out', reals[0], delimiter=',')   # X is an array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
