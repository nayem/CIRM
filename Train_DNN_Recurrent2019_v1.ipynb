{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DNN(working)\n",
    "\n",
    "** - Generalize DNN structure  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import pixiedust** is for debugging jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras Version: 2.2.4 \n",
      "Tensorflow Version: 1.12.0 \n",
      "Python Version: 3.6.8\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
    "\n",
    "import keras\n",
    "print(\"keras Version:\", keras.__version__ , \"\\nTensorflow Version:\", tf.VERSION, \"\\nPython Version:\", platform.python_version()) \n",
    "from keras.utils.data_utils import Sequence\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import pixiedust #jupyter notebook debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To control GPU usage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read parameters from .mat files\n",
    "some read-only version names for ease use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'_e12v1' -> 2 lstm layers, batch size 25, 256 nodes\n",
    "'_e12v2' -> 2 lstm layers with a dropout layer, batch size 25, 256 nodes\n",
    "\n",
    "'_e12v3' -> 2 BLSTM layers, batch size 25, 256 nodes\n",
    "'_e12v4' -> 2 lstm layers with a dropout layer, batch size 25, 256 nodes\n",
    "\n",
    "'_e13v1' -> 2 Blstm layers, batch size 25, 256 nodes, signal approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Constants and File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code_VERSION** is the version of this notebook. This is used for naming weights files\n",
    "\n",
    "***Note: This will be different in each notebook.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code_VERSION = \"_e19v41\"\n",
    "File_path = \"/data/knayem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary constants/file names that are used through out the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_FILE = \"./dnn_models/DNN_params.mat\"\n",
    "\n",
    "TrainData_FILE = File_path +\"/dnn_models/Train_datas_spec.mat\"\n",
    "DevData_FILE = File_path+\"/dnn_models/CrossValidation_datas_spec.mat\"\n",
    "TestData_FILE = File_path+\"/dnn_models/Test_datas_spec.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following filenames are used to save weights, \n",
    "\n",
    "**MODEL_FILE: ** to save best model/weights after an epoch cycle (if weights need to be updated)\n",
    "\n",
    "**SAVE_MODEL_FILE: ** to save final/best model/weights after a training.\n",
    "\n",
    "**LOG_FILE: ** to print log\n",
    "\n",
    "\n",
    "***Note: This will be different in each notebook.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model, after a training cycle\n",
    "MODEL_FILE = File_path +\"/dnn_models/dnn_weights\"+ Code_VERSION+\"_{epoch:02d}.h5\"\n",
    "\n",
    "# Final saved file\n",
    "SAVE_MODEL_FILE = File_path +\"/dnn_models/dnn_model_final\"+ Code_VERSION+\".h5\"\n",
    "\n",
    "LOG_FILE = File_path +\"/dnn_models/dnn_log\"+Code_VERSION+\".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following filenames are used as final output, \n",
    "\n",
    "**OUTPUT_FILE: ** Output of test. After testing on all test files, we save the output spectogram for each testing file.\n",
    "\n",
    "***Note: This will be different in each notebook.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated Spectogram for test dataset\n",
    "OUTPUT_FILE = File_path +\"/dnn_models/dnn_Spec\"+_+\".mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Class defination to read Matlab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "#     opts_dict = dict()\n",
    "\n",
    "    def __init__(self, FILE_PARA, FILE_DATA=\"\", FILE_TEST=\"\"):\n",
    "        \n",
    "        # Basic parameters\n",
    "        with h5py.File(FILE_PARA, 'r') as f:\n",
    "            key_list = list(f.keys())\n",
    "            print('Opt keys:')\n",
    "\n",
    "            for e,(k, v) in enumerate(f['opts'].items()):\n",
    "\n",
    "                print(\"{0}->{1},\".format(e,k), end=\"\")\n",
    "\n",
    "                if k == 'ARMA_order':\n",
    "                    self.ARMA_order = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.ARMA_order\n",
    "                elif k == 'ada_grad_eps':\n",
    "                    self.ada_grad_eps = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.ada_grad_eps\n",
    "                elif k == 'ada_sgd_scale':\n",
    "                    self.ada_sgd_scale = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.ada_sgd_scale\n",
    "                elif k == 'amra_order':\n",
    "                    self.amra_order = int(np.array(v)[0][0])\n",
    "                elif k == 'change_momentum_point':\n",
    "                    self.change_momentum_point = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.change_momentum_point\n",
    "                elif k == 'clip_level':\n",
    "                    self.clip_level = int(np.array(v)[0][0])\n",
    "                elif k == 'cost_function':\n",
    "                    self.cost_function = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.cost_function += chr(c[0])\n",
    "\n",
    "#                     self.opts_dict[k] = self.cost_function\n",
    "\n",
    "                elif k == 'cv_interval':\n",
    "                    self.cv_interval = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.cv_interval\n",
    "                elif k == 'dim_input':\n",
    "                    self.dim_input = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.dim_input\n",
    "                    print(\"(\",k,\"=\",self.dim_input,\")\",end=\" \")\n",
    "                elif k == 'dim_output':\n",
    "                    self.dim_output = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.dim_output\n",
    "                    print(\"(\",k,\"=\",self.dim_output,\")\",end=\" \")\n",
    "                elif k == 'drop_ratio':\n",
    "                    self.drop_ratio = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.drop_ratio\n",
    "                elif k == 'eval_on_gpu':\n",
    "                    self.eval_on_gpu = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.\n",
    "                elif k == 'feawin':\n",
    "                    self.feawin = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.feawin\n",
    "                elif k == 'final_momentum':\n",
    "                    self.final_momentum = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.final_momentum\n",
    "                elif k == 'Fs':\n",
    "                    self.Fs = int(np.array(v)[0][0])\n",
    "                elif k == 'fRange':\n",
    "                    self.fRange = int(np.array(v)[0][0])\n",
    "                elif k == 'hid_struct':\n",
    "                    self.hid_struct = np.array(v)\n",
    "#                     self.opts_dict[k] = self.hid_struct\n",
    "                elif k == 'hopsize':\n",
    "                    self.hopsize = int(np.array(v)[0][0])\n",
    "                elif k == 'initial_momentum':\n",
    "                    self.initial_momentum = np.array(v)[0][0]\n",
    "#                     self.opts_dict[k] = self.initial_momentum\n",
    "                elif k == 'isDropout':\n",
    "                    self.isDropout = 0\n",
    "#                     self.opts_dict[k] = self.isDropout\n",
    "                elif k == 'isDropoutInput':\n",
    "                    self.isDropoutInput = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isDropoutInput\n",
    "                elif k == 'isGPU':\n",
    "                    self.isGPU = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isGPU\n",
    "                elif k == 'isNormalize':\n",
    "                    self.isNormalize = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isNormalize\n",
    "                elif k == 'isPretrain':\n",
    "                    self.isPretrain = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.isPretrain\n",
    "                elif k == 'labwin':\n",
    "                    self.labwin = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.labwin\n",
    "                elif k == 'labeltype':\n",
    "                    self.labeltype = int(np.array(v)[0][0])\n",
    "                elif k == 'labcompress':\n",
    "                    self.labcompress = int(np.array(v)[0][0])\n",
    "                elif k == 'learner':\n",
    "                    self.learner = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.learner += chr(c[0])\n",
    "#                     self.opts_dict[k] = self.learner\n",
    "                elif k == 'logistic_max':\n",
    "                    self.logistic_max = int(np.array(v)[0][0])\n",
    "                elif k == 'logistic_steep':\n",
    "                    self.logistic_max = int(np.array(v)[0][0])\n",
    "                elif k == 'net_struct':\n",
    "                    self.net_struct = np.array(v)\n",
    "#                     for n_s in np.array(v):\n",
    "#                         print('Opts Net Stuct:',n_s[0])\n",
    "#                     self.opts_dict[k] = self.net_struct\n",
    "                elif k == 'noise':\n",
    "                    self.noise = int(np.array(v)[0][0])\n",
    "                elif k == 'nfft':\n",
    "                    self.nfft = int(np.array(v)[0][0])\n",
    "                elif k == 'numGammatoneChans':\n",
    "                    self.numGammatoneChans = int(np.array(v)[0][0])\n",
    "                elif k == 'overlap':\n",
    "                    self.overlap = int(np.array(v)[0][0])\n",
    "                elif k == 'overlap_len':\n",
    "                    self.overlap_len = int(np.array(v)[0][0])\n",
    "                elif k == 'rbm_batch_size':\n",
    "                    self.rbm_batch_size = int(np.array(v)[0][0])\n",
    "                    print(\"(self.rbm_batch_size=\",self.rbm_batch_size,\")\",end=\" \")\n",
    "#                     self.opts_dict[k] = self.rbm_batch_size\n",
    "                elif k == 'rbm_learn_rate_binary':\n",
    "                    self.rbm_learn_rate_binary = np.array(v)\n",
    "#                     self.opts_dict[k] = self.rbm_learn_rate_binary\n",
    "                elif k == 'rbm_learn_rate_real':\n",
    "                    self.rbm_learn_rate_real = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.rbm_learn_rate_real\n",
    "                elif k == 'rbm_max_epoch':\n",
    "                    self.rbm_max_epoch = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.rbm_max_epoch\n",
    "                elif k == 'save_on_fly':\n",
    "                    self.save_on_fly = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.save_on_fly\n",
    "                elif k == 'sgd_batch_size':\n",
    "                    self.sgd_batch_size = int(np.array(v)[0][0]) # BATCH_SIZE for training net\n",
    "                    print(\"(self.sgd_batch_size:\",self.sgd_batch_size,\")\",end=\" \")\n",
    "#                     self.opts_dict[k] = self.sgd_batch_size\n",
    "                elif k == 'sgd_learn_rate':\n",
    "                    self.sgd_learn_rate = np.array(v)\n",
    "#                     self.opts_dict[k] = self.sgd_learn_rate\n",
    "                elif k == 'sgd_max_epoch':\n",
    "                    self.sgd_max_epoch = int(np.array(v)[0][0])\n",
    "                    # self.opts_dict[k] = self.sgd_max_epoch\n",
    "                elif k == 'split_tanh1_c1':\n",
    "                    self.split_tanh1_c1 = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.split_tanh1_c1\n",
    "                elif k == 'split_tanh1_c2':\n",
    "                    self.split_tanh1_c2 = int(np.array(v)[0][0])\n",
    "#                     self.opts_dict[k] = self.split_tanh1_c2\n",
    "                elif k == 'unit_type_hidden':\n",
    "                    self.unit_type_hidden = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_hidden += chr(c[0])\n",
    "\n",
    "                elif k == 'unit_type_output':\n",
    "                    self.unit_type_output = \"\"\n",
    "                    for c in np.array(v):\n",
    "                        self.unit_type_output += chr(c[0])\n",
    "                elif k == 'winlen':\n",
    "                    self.winlen = int(np.array(v)[0][0])\n",
    "                elif k == 'feawin':\n",
    "                    self.feawin = int(np.array(v)[0][0])\n",
    "\n",
    "                        \n",
    "\n",
    "    # Read different data files (Train, Dev, Test)\n",
    "    def read_data(self, FILE_NAME, DATA_TYPE):\n",
    "        print(FILE_NAME, os.path.isfile(FILE_NAME))\n",
    "\n",
    "        with h5py.File(FILE_NAME, 'r') as f:\n",
    "            # print('\\n\\nFile name <{0}>\\nOpt h5py keys (Total {1}):'.format(FILE_TEST,len(f.keys())) )\n",
    "\n",
    "            for k, v in f.items():\n",
    "                if DATA_TYPE.lower() == 'train':\n",
    "                    # Features (input)\n",
    "                    if k == 'trData':\n",
    "                        self.trData = np.transpose(np.array(v))\n",
    "                    elif k == 'trLabel_r':\n",
    "                        self.trLabel_r = np.transpose(np.array(v))\n",
    "                    elif k == 'trLabel_i':\n",
    "                        self.trLabel_i = np.transpose(np.array(v))\n",
    "                    elif k == 'numframes':\n",
    "                        self.trNumframes = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_r':\n",
    "                        self.trCleanSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_i':\n",
    "                        self.trCleanSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_r':\n",
    "                        self.trMixtureSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_i':\n",
    "                        self.trMixtureSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'cleanFilename':\n",
    "                        self.trCleanFilename = np.transpose(np.array(v))\n",
    "                    elif k == 'trFilename':\n",
    "                        self.trFilename = np.transpose(np.array(v))\n",
    "                \n",
    "                elif DATA_TYPE.lower() == 'dev':\n",
    "                    # Features (input)\n",
    "                    if k == 'cvData':\n",
    "                        self.cvData = np.transpose(np.array(v))\n",
    "                    elif k == 'cvLabel_r':\n",
    "                        self.cvLabel_r = np.transpose(np.array(v))\n",
    "                    elif k == 'cvLabel_i':\n",
    "                        self.cvLabel_i = np.transpose(np.array(v))\n",
    "                    elif k == 'numframes':\n",
    "                        self.cvNumframes = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_r':\n",
    "                        self.cvCleanSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_i':\n",
    "                        self.cvCleanSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_r':\n",
    "                        self.cvMixtureSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_i':\n",
    "                        self.cvMixtureSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'cleanFilename':\n",
    "                        self.cvCleanFilename = np.transpose(np.array(v))\n",
    "                    elif k == 'cvFilename':\n",
    "                        self.cvFilename = np.transpose(np.array(v))\n",
    "            \n",
    "                elif DATA_TYPE.lower() == 'test':\n",
    "                    # Features (input)\n",
    "                    if k == 'teData':\n",
    "                        self.teData = np.transpose(np.array(v))\n",
    "                    elif k == 'teLabel_r':\n",
    "                        self.teLabel_r = np.transpose(np.array(v))\n",
    "                    elif k == 'teLabel_i':\n",
    "                        self.teLabel_i = np.transpose(np.array(v))\n",
    "                    elif k == 'numframes':\n",
    "                        self.teNumframes = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_r':\n",
    "                        self.teCleanSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_clean_i':\n",
    "                        self.teCleanSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_r':\n",
    "                        self.teMixtureSpec_r = np.transpose(np.array(v))\n",
    "                    elif k == 'spec_mixture_i':\n",
    "                        self.teMixtureSpec_i = np.transpose(np.array(v))\n",
    "                    elif k == 'cleanFilename':\n",
    "                        self.teCleanFilename = np.transpose(np.array(v))\n",
    "                    elif k == 'teFilename':\n",
    "                        for e,x in enumerate(np.array(v)):\n",
    "                            print(e,x)\n",
    "                        self.teFilename = np.transpose(np.array(v))\n",
    "            \n",
    "            # Display statistics\n",
    "            if DATA_TYPE.lower() == 'train':\n",
    "#                 self.trLabel = np.concatenate((self.trLabel_r, self.trLabel_i), axis=1)\n",
    "                self.display_stat(DATA_TYPE=\"train\")\n",
    "                \n",
    "            elif DATA_TYPE.lower() == 'dev':\n",
    "#                 self.cvLabel = np.concatenate((self.cvLabel_r, self.cvLabel_i), axis=1)\n",
    "                self.display_stat(DATA_TYPE=\"dev\")\n",
    "                \n",
    "            elif DATA_TYPE.lower() == 'test':\n",
    "#                 self.teLabel = np.concatenate((self.teLabel_r, self.teLabel_i), axis=1)\n",
    "                self.display_stat(DATA_TYPE=\"test\")\n",
    "                \n",
    "                \n",
    "                \n",
    "    # Display the simple statistics of Data (train, dev, test)\n",
    "    def display_stat(self, DATA_TYPE):\n",
    "        if DATA_TYPE.lower() == 'train':\n",
    "            print(\"\\nSummary->[TRAIN DATA]\")\n",
    "            print(\"trNumframes.shape=\", self.trNumframes.shape)\n",
    "            \n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.trData)\n",
    "#             print(\"trData.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.trLabel_r)\n",
    "#             print(\"trLabel_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.trLabel_i)\n",
    "#             print(\"trLabel_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.trLabel)\n",
    "#             print(\"trLabel.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trCleanSpec_r)\n",
    "            print(\"trCleanSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trCleanSpec_i)\n",
    "            print(\"trCleanSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trMixtureSpec_r)\n",
    "            print(\"trMixtureSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.trMixtureSpec_i)\n",
    "            print(\"trMixtureSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            print(\"trCleanFilename.shape={0}, trFilename.shape={0}\".\n",
    "                              format(self.trCleanFilename.shape, self.trFilename.shape) )\n",
    "            \n",
    "        elif DATA_TYPE.lower() == 'dev':\n",
    "            print(\"\\nSummary->[DEV DATA]\")\n",
    "            print(\"cvNumframes.shape=\", self.cvNumframes.shape)\n",
    "            \n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.cvData)\n",
    "#             print(\"cvData.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.cvLabel_r)\n",
    "#             print(\"cvLabel_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.cvLabel_i)\n",
    "#             print(\"cvLabel_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.cvLabel)\n",
    "#             print(\"cvLabel.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvCleanSpec_r)\n",
    "            print(\"cvCleanSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvCleanSpec_i)\n",
    "            print(\"cvCleanSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvMixtureSpec_r)\n",
    "            print(\"cvMixtureSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.cvMixtureSpec_i)\n",
    "            print(\"cvMixtureSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            print(\"cvCleanFilename.shape={0}, cvFilename.shape={0}\".\n",
    "                              format(self.cvCleanFilename.shape, self.cvFilename.shape) )\n",
    "\n",
    "            \n",
    "        elif DATA_TYPE.lower() == 'test':\n",
    "            print(\"\\nSummary->[TEST DATA]\")\n",
    "            print(\"teNumframes.shape=\", self.teNumframes.shape)\n",
    "            \n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.teData)\n",
    "#             print(\"teData.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.teLabel_r)\n",
    "#             print(\"teLabel_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.teLabel_i)\n",
    "#             print(\"teLabel_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "#             shp, m, v, s, mn, mx = self.data_stat(self.teLabel)\n",
    "#             print(\"teLabel.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "#                               format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teCleanSpec_r)\n",
    "            print(\"teCleanSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teCleanSpec_i)\n",
    "            print(\"teCleanSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teMixtureSpec_r)\n",
    "            print(\"teMixtureSpec_r.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            shp, m, v, s, mn, mx = self.data_stat(self.teMixtureSpec_i)\n",
    "            print(\"teMixtureSpec_i.shape={0}, mean={1:.4f}, variance={2:.4f}, std={3:.4f}, range=[{4:.4f},{5:.4f}]\".\n",
    "                              format(shp, m, v, s, mn, mx) )\n",
    "            print(\"teCleanFilename.shape={0}, trFilename.shape={0}\".\n",
    "                              format(self.teCleanFilename.shape, self.teFilename.shape) )\n",
    "                        \n",
    "    # Helper function for display, returns actual values\n",
    "    def data_stat(self, data):\n",
    "        return data.shape,np.mean(data),np.var(data),np.std(data),np.amin(data),np.amax(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an object which does the parameter (Matlab *Opts* object) reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt keys:\n",
      "0->ARMA_order,1->Fs,2->ada_grad_eps,3->ada_sgd_scale,4->arma_order,5->change_momentum_point,6->clip_level,7->cost_function,8->cv_interval,9->dim_input,( dim_input = 1230 ) 10->dim_output,( dim_output = 963 ) 11->drop_ratio,12->eval_on_gpu,13->fRange,14->feawin,15->final_momentum,16->hid_struct,17->hop_size,18->hopsize,19->initial_momentum,20->isDropout,21->isDropoutInput,22->isGPU,23->isNormalize,24->isPretrain,25->labcompress,26->labeltype,27->labwin,28->learner,29->logistic_max,30->logistic_steep,31->net_struct,32->nfft,33->noise,34->numGammatoneChans,35->overlap,36->overlap_len,37->rbm_batch_size,(self.rbm_batch_size= 1024 ) 38->rbm_learn_rate_binary,39->rbm_learn_rate_real,40->rbm_max_epoch,41->save_on_fly,42->sgd_batch_size,(self.sgd_batch_size: 1024 ) 43->sgd_learn_rate,44->sgd_max_epoch,45->split_tanh1_c1,46->split_tanh1_c2,47->tr_mu,48->tr_std,49->unit_type_hidden,50->unit_type_output,51->win_len,52->winlen,"
     ]
    }
   ],
   "source": [
    "opts = Opts(PARAM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/knayem/dnn_models/Train_datas_spec.mat True\n",
      "\n",
      "Summary->[TRAIN DATA]\n",
      "trNumframes.shape= (60000, 1)\n",
      "trCleanSpec_r.shape=(7807680, 321), mean=0.0021, variance=0.2995, std=0.5473, range=[-46.6837,39.0942]\n",
      "trCleanSpec_i.shape=(7807680, 321), mean=-0.0000, variance=0.2909, std=0.5394, range=[-50.2884,37.3594]\n",
      "trMixtureSpec_r.shape=(7807680, 321), mean=0.0060, variance=3.2893, std=1.8136, range=[-285.0552,274.8278]\n",
      "trMixtureSpec_i.shape=(7807680, 321), mean=-0.0000, variance=3.0763, std=1.7539, range=[-134.2137,120.0675]\n",
      "trCleanFilename.shape=(6, 1), trFilename.shape=(6, 1)\n"
     ]
    }
   ],
   "source": [
    "opts.read_data(TrainData_FILE, DATA_TYPE=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dev (validation) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dnn_models/CrossValidation_datas.mat True\n",
      "\n",
      "Summary->[DEV DATA]\n",
      "cvNumframes.shape= (3300, 1)\n",
      "cvData.shape=(449610, 1230), mean=-0.0000, variance=0.6076, std=0.7795, range=[-9.6298,10.4941]\n",
      "cvLabel_r.shape=(449610, 963), mean=0.0286, variance=0.0213, std=0.1458, range=[-10.0000,10.0000]\n",
      "cvLabel_i.shape=(449610, 963), mean=-0.0000, variance=0.0130, std=0.1138, range=[-10.0000,10.0000]\n",
      "cvLabel.shape=(449610, 1926), mean=0.0143, variance=0.0173, std=0.1316, range=[-10.0000,10.0000]\n",
      "cvCleanSpec_r.shape=(449610, 321), mean=0.0020, variance=0.3002, std=0.5479, range=[-40.8223,41.5895]\n",
      "cvCleanSpec_i.shape=(449610, 321), mean=-0.0000, variance=0.2895, std=0.5381, range=[-39.2169,43.0322]\n",
      "cvMixtureSpec_r.shape=(449610, 321), mean=0.0044, variance=3.2486, std=1.8024, range=[-121.7945,125.0182]\n",
      "cvMixtureSpec_i.shape=(449610, 321), mean=-0.0001, variance=3.1470, std=1.7740, range=[-101.6771,93.7446]\n"
     ]
    }
   ],
   "source": [
    "opts.read_data(DevData_FILE, DATA_TYPE=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Test data\n",
    "\n",
    "You can run this portion later, when you need this data. It will help with ram storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/knayem/dnn_models/Test_datas_spec.mat True\n",
      "0 [3707764736          2          1          1          1          1]\n",
      "\n",
      "Summary->[TEST DATA]\n",
      "teNumframes.shape= (21800, 1)\n",
      "teCleanSpec_r.shape=(2897600, 321), mean=0.0020, variance=0.2996, std=0.5473, range=[-42.8964,38.0338]\n",
      "teCleanSpec_i.shape=(2897600, 321), mean=-0.0000, variance=0.2918, std=0.5402, range=[-37.0904,45.6445]\n",
      "teMixtureSpec_r.shape=(2897600, 321), mean=0.0057, variance=3.3611, std=1.8333, range=[-126.1488,136.7698]\n",
      "teMixtureSpec_i.shape=(2897600, 321), mean=-0.0001, variance=3.1499, std=1.7748, range=[-129.6114,133.3650]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Opts' object has no attribute 'trCleanFilename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-066f00be77e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestData_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_TYPE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-d85ab1220177>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(self, FILE_NAME, DATA_TYPE)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mDATA_TYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;31m#                 self.teLabel = np.concatenate((self.teLabel_r, self.teLabel_i), axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TYPE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d85ab1220177>\u001b[0m in \u001b[0;36mdisplay_stat\u001b[0;34m(self, DATA_TYPE)\u001b[0m\n\u001b[1;32m    353\u001b[0m                               format(shp, m, v, s, mn, mx) )\n\u001b[1;32m    354\u001b[0m             print(\"trCleanFilename.shape={0}, trFilename.shape={0}\".\n\u001b[0;32m--> 355\u001b[0;31m                               format(self.trCleanFilename.shape, self.trFilename.shape) )\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;31m# Helper function for display, returns actual values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Opts' object has no attribute 'trCleanFilename'"
     ]
    }
   ],
   "source": [
    "opts.read_data(TestData_FILE, DATA_TYPE=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Next Batch Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. DNN Batch Generator\n",
    "\n",
    "Every time it returns a batch `(x, y)`. **x** is the features and **y** is the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_DataGenerator_SPEC(keras.utils.Sequence):\n",
    "    '''\n",
    "    Generates Spectogram data for Keras\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, CYCLE, opts, batch_size=32, shuffle=True):\n",
    "        '''\n",
    "        Initialization\n",
    "        '''\n",
    "        self.opts = opts\n",
    "        self.batch_size = batch_size\n",
    "        self.CYCLE = CYCLE.lower()\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.BATCH_TYPE = 'floor' # {actual,floor}\n",
    "        # 'actual' uses all frames, so last batch can be smaller than batch_size.\n",
    "        # 'floor' may not use all the frames, but all the batches will be of equal size.\n",
    "        \n",
    "        self.__len__()    \n",
    "        self.batchIDs = self.ready_batchID() \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Denotes the number of batches per epoch.\n",
    "\n",
    "        returns:\n",
    "            num_batch <int> = total number of batches\n",
    "            \n",
    "        variable:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test}\n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            self.total_num_samples = self.opts.trMixtureSpec_r.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            self.total_num_samples = self.opts.cvMixtureSpec_r.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            self.total_num_samples = self.opts.teMixtureSpec_r.shape[0]\n",
    "            self.BATCH_TYPE = 'actual'\n",
    "\n",
    "\n",
    "        if self.BATCH_TYPE.lower() == 'actual':\n",
    "            self.num_batch = math.ceil(self.total_num_samples/self.batch_size) \n",
    "            \n",
    "        elif self.BATCH_TYPE.lower() == 'floor':\n",
    "            self.num_batch = self.total_num_samples//self.batch_size \n",
    "            \n",
    "        \n",
    "        return int(self.num_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ready_batchID(self):\n",
    "        '''\n",
    "        Create a 2D array with (start,end) frame number of each batch.\n",
    "\n",
    "        returns:\n",
    "            batchIDs <nparray.2D> = shape(number_of_batches, 2); \n",
    "                                    each row (s,e) is the inclusive start(s) and end(e) frame number\n",
    "        variables:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test} \n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        batchIDs = np.zeros((self.num_batch,2))\n",
    "        # (s,e); s = the begining frame number and e = the ending frame number of a batch\n",
    "\n",
    "        s = np.where( np.arange(self.total_num_samples)%self.batch_size == 0 )\n",
    "        e = np.where( np.arange(self.total_num_samples)%self.batch_size == self.batch_size-1 )\n",
    "\n",
    "        batchIDs[:, 0] = s[0][0:self.num_batch]\n",
    "\n",
    "        # when all batches are equal of batch_size   \n",
    "        if e[0].shape[0] >= self.num_batch:\n",
    "            batchIDs[:, 1] = e[0][0:self.num_batch]\n",
    "\n",
    "        # when last batch is smaller than batch_size\n",
    "        elif e[0].shape[0] < self.num_batch:\n",
    "            batchIDs[:self.num_batch-1, 1] = e[0][0:self.num_batch]\n",
    "            batchIDs[-1,1] = self.total_num_samples-1\n",
    "            \n",
    "\n",
    "        return batchIDs\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Generate one batch of data\n",
    "        '''\n",
    "        # Get the correcponding batch\n",
    "        batch = self.batchIDs[index]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = self.indexes[ int( batch[0]): int( batch[1])+1 ]\n",
    "        \n",
    "        # Generate data\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Updates indexes after each epoch\n",
    "        '''\n",
    "        self.indexes = np.arange(self.total_num_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        '''\n",
    "        Generates data containing batch_size samples.\n",
    "        '''\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            x = ( self.opts.trMixtureSpec_r[ list_IDs_temp ]**2 + self.opts.trMixtureSpec_i[ list_IDs_temp ]**2 )**0.5\n",
    "            y = ( self.opts.trCleanSpec_r[ list_IDs_temp ]**2 + self.opts.trCleanSpec_i[ list_IDs_temp ]**2 )**0.5\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            x = ( self.opts.cvMixtureSpec_r[ list_IDs_temp ]**2 + self.opts.cvMixtureSpec_i[ list_IDs_temp ]**2 )**0.5\n",
    "            y = ( self.opts.cvCleanSpec_r[ list_IDs_temp ]**2 + self.opts.cvCleanSpec_i[ list_IDs_temp ]**2 )**0.5\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            x = ( self.opts.teMixtureSpec_r[ list_IDs_temp ]**2 + self.opts.teMixtureSpec_i[ list_IDs_temp ]**2 )**0.5\n",
    "            y = ( self.opts.teCleanSpec_r[ list_IDs_temp ]**2 + self.opts.teCleanSpec_i[ list_IDs_temp ]**2 )**0.5\n",
    "\n",
    "        # do normalizeation\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_DataGenerator_SPEC2(keras.utils.Sequence):\n",
    "    '''\n",
    "    Generates Spectogram data for Keras in sequence\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, CYCLE, opts, batch_size=32, shuffle=True):\n",
    "        '''\n",
    "        Initialization\n",
    "        '''\n",
    "        self.opts = opts\n",
    "        self.batch_size = batch_size\n",
    "        self.CYCLE = CYCLE.lower()\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.BATCH_TYPE = 'floor' # {actual,floor}\n",
    "        # 'actual' uses all frames, so last batch can be smaller than batch_size.\n",
    "        # 'floor' may not use all the frames, but all the batches will be of equal size.\n",
    "        \n",
    "        self.__len__()    \n",
    "        self.batchIDs = self.ready_batchID() \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Denotes the number of batches per epoch.\n",
    "\n",
    "        returns:\n",
    "            num_batch <int> = total number of batches\n",
    "            \n",
    "        variable:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test}\n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            sampleFrames = self.opts.trNumframes\n",
    "            sampleSource_r = self.opts.trMixtureSpec_r\n",
    "            sampleSource_i = self.opts.trMixtureSpec_i\n",
    "            \n",
    "            cleanSource_r = self.opts.trCleanSpec_r\n",
    "            cleanSource_i = self.opts.trCleanSpec_i\n",
    "            \n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            sampleFrames = self.opts.cvNumframes\n",
    "            sampleSource_r = self.opts.cvMixtureSpec_r\n",
    "            sampleSource_i = self.opts.cvMixtureSpec_i\n",
    "            \n",
    "            cleanSource_r = self.opts.cvCleanSpec_r\n",
    "            cleanSource_i = self.opts.cvCleanSpec_i\n",
    "            \n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            sampleFrames = self.opts.teNumframes\n",
    "            sampleSource_r = self.opts.teMixtureSpec_r\n",
    "            sampleSource_i = self.opts.teMixtureSpec_i\n",
    "            \n",
    "            cleanSource_r = self.opts.teCleanSpec_r\n",
    "            cleanSource_i = self.opts.teCleanSpec_i\n",
    "\n",
    "            \n",
    "        sampleFrames = sampleFrames.flatten()\n",
    "        cumulativeFrames = np.cumsum(sampleFrames)\n",
    "        \n",
    "        self.allSamples_r = None\n",
    "        self.allSamples_i = None\n",
    "        self.cleanSamples_r = None\n",
    "        self.cleanSamples_i = None\n",
    "        \n",
    "        for indx in range(len(cumulativeFrames)):\n",
    "\n",
    "            prev = sampleSource_r[indx-1] if indx !=0 else 0\n",
    "            now = sampleSource_r[indx]\n",
    "            self.allSamples_r = sampleSource_r[prev:now] if self.allSamples_r is None else np.concatenate( (self.allSamples_r, sampleSource_r[prev:now]), axis=1) \n",
    "            self.allSamples_i = sampleSource_i[prev:now] if self.allSamples_i is None else np.concatenate( (self.allSamples_i, sampleSource_i[prev:now]), axis=1) \n",
    "            \n",
    "            self.cleanSamples_r = cleanSource_r[prev:now] if self.cleanSamples_r is None else np.concatenate( (self.allSamples_r, cleanSource_r[prev:now]), axis=1) \n",
    "            self.cleanSamples_i = cleanSource_i[prev:now] if self.cleanSamples_i is None else np.concatenate( (self.allSamples_i, cleanSource_i[prev:now]), axis=1) \n",
    "            \n",
    "            n_emptyFrame = sampleFrames%self.batch_size\n",
    "            if n_emptyFrame:\n",
    "                self.allSamples_r = np.concatenate( (self.allSamples_r, np.zeros(n_emptyFrame,321)), axis=1)\n",
    "                self.allSamples_i = np.concatenate( (self.allSamples_i, np.zeros(n_emptyFrame,321)), axis=1)\n",
    "                \n",
    "                self.cleanSamples_r = np.concatenate( (self.cleanSamples_r, np.zeros(n_emptyFrame,321)), axis=1)\n",
    "                self.cleanSamples_i = np.concatenate( (self.cleanSamples_i, np.zeros(n_emptyFrame,321)), axis=1)\n",
    "                \n",
    "                \n",
    "        self.total_num_samples = self.allSamples_r.shape[0]\n",
    "        \n",
    "        if self.BATCH_TYPE.lower() == 'actual':\n",
    "            self.num_batch = math.ceil(self.total_num_samples/self.batch_size) \n",
    "        elif self.BATCH_TYPE.lower() == 'floor':\n",
    "            self.num_batch = self.total_num_samples//self.batch_size\n",
    "\n",
    "\n",
    "        return int(self.num_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ready_batchID(self):\n",
    "        '''\n",
    "        Create a 2D array with (start,end) frame number of each batch.\n",
    "\n",
    "        returns:\n",
    "            batchIDs <nparray.2D> = shape(number_of_batches, 2); \n",
    "                                    each row (s,e) is the inclusive start(s) and end(e) frame number\n",
    "        variables:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test} \n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        batchIDs = np.zeros((self.num_batch,2))\n",
    "        # (s,e); s = the begining frame number and e = the ending frame number of a batch\n",
    "\n",
    "        s = np.where( np.arange(self.total_num_samples)%self.batch_size == 0 )\n",
    "        e = np.where( np.arange(self.total_num_samples)%self.batch_size == self.batch_size-1 )\n",
    "\n",
    "        batchIDs[:, 0] = s[0][0:self.num_batch]\n",
    "\n",
    "        # when all batches are equal of batch_size   \n",
    "        if e[0].shape[0] >= self.num_batch:\n",
    "            batchIDs[:, 1] = e[0][0:self.num_batch]\n",
    "\n",
    "        # when last batch is smaller than batch_size\n",
    "        elif e[0].shape[0] < self.num_batch:\n",
    "            batchIDs[:self.num_batch-1, 1] = e[0][0:self.num_batch]\n",
    "            batchIDs[-1,1] = self.total_num_samples-1\n",
    "\n",
    "        return batchIDs\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Generate one batch of data\n",
    "        '''\n",
    "        # Get the correcponding batch\n",
    "        batch = self.batchIDs[index]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = self.indexes[ int( batch[0]): int( batch[1])+1 ]\n",
    "        \n",
    "        # Generate data\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Updates indexes after each epoch\n",
    "        '''\n",
    "        self.indexes = np.arange(self.total_num_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        '''\n",
    "        Generates data containing batch_size samples.\n",
    "        '''\n",
    "        x = ( self.allSamples_r[ list_IDs_temp ]**2 + self.allSamples_i[ list_IDs_temp ]**2 )**0.5\n",
    "        y = ( self.cleanSamples_r[ list_IDs_temp ]**2 + self.cleanSamples_i[ list_IDs_temp ]**2 )**0.5\n",
    "\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class DNN_DataGenerator_SA(keras.utils.Sequence):\n",
    "    '''\n",
    "    Generates data for Keras\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, CYCLE='Train', opts=None, input_list=['features','mix_spec_r','mix_spec_i'], \n",
    "                 batch_size=32, shuffle=True):\n",
    "        '''\n",
    "        Initialization\n",
    "        '''\n",
    "        self.opts = opts\n",
    "        # key of the dictionary used as input\n",
    "        self.input_list = input_list\n",
    "        self.batch_size = batch_size\n",
    "        self.CYCLE = CYCLE\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.BATCH_TYPE = 'floor' # {actual,floor}\n",
    "        # 'actual' uses all frames, so last batch can be smaller than batch_size.\n",
    "        # 'floor' may not use all the frames, but all the batches will be of equal size.\n",
    "        \n",
    "        self.__len__()    \n",
    "        self.batchIDs = self.ready_batchID() \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Denotes the number of batches per epoch.\n",
    "\n",
    "        returns:\n",
    "            num_batch <int> = total number of batches\n",
    "            \n",
    "        variable:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test}\n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            self.total_num_samples = self.opts.trData.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            self.total_num_samples = self.opts.cvData.shape[0]\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            self.total_num_samples = self.opts.teData.shape[0]\n",
    "\n",
    "\n",
    "        if self.BATCH_TYPE.lower() == 'actual':\n",
    "            self.num_batch = math.ceil(self.total_num_samples/self.batch_size) \n",
    "            \n",
    "        elif self.BATCH_TYPE.lower() == 'floor':\n",
    "            self.num_batch = self.total_num_samples//self.batch_size \n",
    "\n",
    "\n",
    "        return int(self.num_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ready_batchID(self):\n",
    "        '''\n",
    "        Create a 2D array with (start,end) frame number of each batch.\n",
    "\n",
    "        returns:\n",
    "            batchIDs <nparray.2D> = shape(number_of_batches, 2); \n",
    "                                    each row (s,e) is the inclusive start(s) and end(e) frame number\n",
    "        variables:\n",
    "            opts <object> = all the parametric data\n",
    "            batch_size <int>        \n",
    "            CYCLE <string>  = {train, dev, test} \n",
    "            total_num_samples <int> = total number of frame, shape[0] of data\n",
    "        '''\n",
    "\n",
    "        batchIDs = np.zeros((self.num_batch,2))\n",
    "        # (s,e); s = the begining frame number and e = the ending frame number of a batch\n",
    "\n",
    "        s = np.where( np.arange(self.total_num_samples)%self.batch_size == 0 )\n",
    "        e = np.where( np.arange(self.total_num_samples)%self.batch_size == self.batch_size-1 )\n",
    "\n",
    "        batchIDs[:, 0] = s[0]\n",
    "\n",
    "        # when all batches are equal of batch_size   \n",
    "        if e[0].shape[0] >= self.num_batch:\n",
    "            batchIDs[:, 1] = e[0][0:self.num_batch]\n",
    "\n",
    "        # when last batch is smaller than batch_size\n",
    "        elif e[0].shape[0] < self.num_batch:\n",
    "            batchIDs[:self.num_batch-1, 1] = e[0][0:self.num_batch]\n",
    "            batchIDs[-1,1] = self.total_num_samples-1\n",
    "\n",
    "        return batchIDs\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Generate one batch of data\n",
    "        '''\n",
    "        # Get the correcponding batch\n",
    "        batch = self.batchIDs[index]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = self.indexes[ int( batch[0]): int( batch[1]) ]\n",
    "        \n",
    "        # Generate data\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.total_num_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        '''\n",
    "        Generates data containing batch_size samples.\n",
    "        '''\n",
    "        if self.CYCLE.lower()=='train':\n",
    "            x = self.opts.trData[ list_IDs_temp ]\n",
    "            y = self.opts.trLabel[ list_IDs_temp ]\n",
    "            mix_stft_r = self.opts.trMixtureSpec_r[list_IDs_temp]\n",
    "            mix_stft_i = self.opts.trMixtureSpec_i[list_IDs_temp]\n",
    "\n",
    "        elif self.CYCLE.lower()=='dev':\n",
    "            x = self.opts.cvData[ list_IDs_temp ]\n",
    "            y = self.opts.cvLabel[ list_IDs_temp ]\n",
    "            mix_stft_r = self.opts.cvMixtureSpec_r[list_IDs_temp]\n",
    "            mix_stft_i = self.opts.cvMixtureSpec_i[list_IDs_temp]\n",
    "\n",
    "        elif self.CYCLE.lower()=='test':\n",
    "            x = self.opts.teData[ list_IDs_temp ]\n",
    "            y = self.opts.teLabel[list_IDs_temp ]\n",
    "            mix_stft_r = self.opts.teMixtureSpec_r[list_IDs_temp]\n",
    "            mix_stft_i = self.opts.teMixtureSpec_i[list_IDs_temp]\n",
    "            \n",
    "\n",
    "        return {self.input_list[0]:x, self.input_list[1]:mix_stft_r, self.input_list[2]:mix_stft_i}, y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cost Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wrap_freqs = 963\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Unwrapping\n",
    "\n",
    "Need to signal approximation to unwrap."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def unwrapAugmentedTF_wAvg(impt_mag, num_per_side, wrap_freqs, unwrap_mag=None):\n",
    "    '''\n",
    "    Description: Unwrap the augmented time-frequency (T-F) representation and compute the average.\n",
    "\n",
    "    Input:\n",
    "        impt_mag: wrapped T-F representation with dimensions (2*T+1)*d x m\n",
    "        num_per_side(T): number of frames to left and right of each frame used to augmented T-F representation\n",
    "    \n",
    "    Output:\n",
    "        unwrap_avgmag: unwrapped and averaged T-F representation with dimensions d x m\n",
    "        unwrap_mag: unwrapped T-F representation with dimensions d x (2*T+1) x m\n",
    "    '''\n",
    "\n",
    "    sliding_window_len       = 2*num_per_side + 1\n",
    "\n",
    "    numWrapFreqs = wrap_freqs\n",
    "    numFrames = K.shape(impt_mag)[1] #tensor\n",
    "    numFreqs = numWrapFreqs//sliding_window_len\n",
    "    \n",
    "    print(\"<numFrames>\",numFrames)\n",
    "    print(\"<numFreqs>\",numFreqs)\n",
    "    \n",
    "    unwrap_avgmag = K.zeros( (numFreqs,numFrames) ) #tensor\n",
    "    print(\"<unwrap_avgmag>\",unwrap_avgmag)\n",
    "    unwrap_avgmagTemp = []\n",
    "    \n",
    "\n",
    "    if num_per_side > 0:\n",
    "        if unwrap_mag is None:\n",
    "\n",
    "            unwrap_mag = K.zeros( (numFreqs,sliding_window_len,numFrames) ) #tensor\n",
    "            unwrap_magTemp = []\n",
    "            \n",
    "            curr_ind_location = K.zeros ( (numFrames), dtype='int32') #tensor\n",
    "            \n",
    "    \n",
    "            #for frameNum = 1:numFrames\n",
    "            def f_numFrames(frameNum):\n",
    "\n",
    "                # Get the indices for the frames used in this augmented matrix\n",
    "                frame_inds=K.arange(frameNum-num_per_side, frameNum+num_per_side+1) #tensor\n",
    "                print('1.<frame_inds>',frame_inds ) \n",
    "                frame_inds = tf.clip_by_value(frame_inds, K.variable(0,dtype='int32'), (numFrames-1))\n",
    "                print('2.<frame_inds>',frame_inds)\n",
    "              \n",
    "                # Unwrap the data for this frame, Size d x (2*T + 1)\n",
    "                slid_win_data = K.reshape( impt_mag[...,frameNum], (numFreqs,sliding_window_len) ) \n",
    "                print(\"<slid_win_data>\",slid_win_data)\n",
    "              \n",
    "                freqSlide_Temp = []\n",
    "                \n",
    "                #for ind_num = 1:length(frame_inds)\n",
    "                def f_frame_inds(ind_num):\n",
    "                    \n",
    "                    slid = slid_win_data[:,ind_num] ##### tf.gather_nd()\n",
    "                    print(\"<slid>\", slid)\n",
    "                    fi = frame_inds[ind_num]\n",
    "                    print(\"<1.fi>\", fi)\n",
    "                    \n",
    "                    low = K.arange(fi, dtype='int32')\n",
    "                    high = K.arange(fi+1, stop=numFrames, dtype='int32')\n",
    "                    if curr_ind_location is None:\n",
    "                        print(\"<curr_ind_location> is None\")\n",
    "                    if curr_ind_location is not None:\n",
    "                        print(\"<curr_ind_location> is Not None\")\n",
    "#                     print(\"1.<curr_ind_location>\",curr_ind_location)\n",
    "                    curr_ind_location = K.concatenate( [tf.gather(curr_ind_location, low), K.expand_dims(tf.gather(curr_ind_location, fi)+1),tf.gather(curr_ind_location, high)] )\n",
    "                    \n",
    "                    print(\"2.<curr_ind_location>\",curr_ind_location)\n",
    "                    \n",
    "                    fi = K.expand_dims(fi,axis=-1)\n",
    "                    print(\"<2.fi>\", fi)\n",
    "                    \n",
    "#                     unwrap_mag(:,curr_ind_location(fi), fi) = slid_win_data(:,ind_num);\n",
    "\n",
    "                    return K.flatten(tf.gather(slid_win_data, fi, axis=1))\n",
    "#                     curr_ind_locationTemp[fi] = curr_ind_locationTemp[fi] + 1;\n",
    "                    \n",
    "                    '''\n",
    "                    -Do stack() kind of think here, \n",
    "                    -    unwrap_magTemp = []\n",
    "                    check dimensons with matlab code\n",
    "                    correct the loss_SA() \n",
    "                    \n",
    "                    unwrap_mag(:,curr_ind_location(fi), fi) = slid;\n",
    "                    '''\n",
    "                \n",
    "                freqSlide_Temp.append( K.map_fn(f_frame_inds, K.arange(K.shape(frame_inds)[0]) , dtype='float32') )\n",
    "                freqSlide_Temp = tf.stack(freqSlide_Temp, axis=1)\n",
    "                print(\"<freqSlide_Temp>\",freqSlide_Temp)\n",
    "        \n",
    "                return freqSlide_Temp\n",
    "            \n",
    "            unwrap_magTemp.append( K.map_fn(f_numFrames, K.arange(numFrames), dtype='float32') )\n",
    "            unwrap_magTemp = tf.stack(unwrap_magTemp, axis=1)\n",
    "            print(\"<unwrap_magTemp>\", unwrap_magTemp)\n",
    "            unwrap_mag = unwrap_magTemp\n",
    "    \n",
    "    \n",
    "#         for frameNum in range(numFrames):\n",
    "#             tf.assign( unwrap_avgmag[:,frameNum] , K.mean(unwrap_mag[:,:,frameNum],axis=1) )\n",
    "        \n",
    "        def f_frameNum(frameNum):\n",
    "            frameNum = K.expand_dims(K.expand_dims(frameNum,axis=-1), axis=-1)\n",
    "            print(\"<[[frameNum]]>\", frameNum )\n",
    "            uw_mag = tf.gather_nd(unwrap_mag, frameNum)\n",
    "            print(\"1.<uw_mag>\",uw_mag)\n",
    "            uw_mag = K.mean(uw_mag,axis=1)\n",
    "            print(\"2.<uw_mag>\",uw_mag)  \n",
    "#             tf.assign( unwrap_avgmag[:,frameNum] , K.mean(uw_mag,axis=1) )\n",
    "#             return unwrap_avgmag\n",
    "            return uw_mag\n",
    "        \n",
    "        unwrap_avgmagTemp.append( K.map_fn(f_frameNum, K.arange(numFrames) , dtype='float32') )\n",
    "        unwrap_avgmag = tf.stack(unwrap_avgmagTemp, axis=1)\n",
    "        print(\"<unwrap_avgmag>\",unwrap_avgmag)\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        unwrap_avgmag = impt_mag;\n",
    "        unwrap_mag = 0;\n",
    "\n",
    "    return unwrap_avgmag,unwrap_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Mask Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction_SPEC(yTrue,yPred):\n",
    "    \n",
    "    r, c = K.shape(yTrue)[0], K.shape(yTrue)[1]\n",
    "    \n",
    "    samples = K.cast(r,dtype='float32') \n",
    "    # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "    cost_real = K.sum(K.square(yTrue[:, :c//2]- yPred[:, :c//2]))\n",
    "    cost_imag = K.sum(K.square(yTrue[:, c//2:]- yPred[:, c//2:]))\n",
    "    \n",
    "    return 0.5*(cost_real+cost_imag)/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Signal Approximation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def lossFunction_SA(opts):\n",
    "    '''\n",
    "    variables:\n",
    "        opts.batch_ids <list> = id of randomly selected frames\n",
    "    '''\n",
    "    \n",
    "    def customLoss_train_SA(yTrue,yPred):\n",
    "\n",
    "        r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "    \n",
    "        samples = K.cast(r,dtype='float32') \n",
    "        # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "\n",
    "        pred_real = yPred[:, :(c//2)]\n",
    "        pred_imag = yPred[:, (c//2):]\n",
    "\n",
    "        true_real = yTrue[:, :(c//2)]\n",
    "        true_imag = yTrue[:, (c//2):]\n",
    "\n",
    "\n",
    "        mix_stft = K.cast(K.transpose(opts.mix_spec[opts.batch_ids, :321]),dtype='complex64') + \\\n",
    "                1j*K.cast(K.transpose(opts.mix_spec[opts.batch_ids, 321:]),dtype='complex64')\n",
    "\n",
    "        complex_irmmask = K.cast(K.transpose(pred_real),dtype='complex64') + \\\n",
    "                        1j*K.cast(K.transpose(pred_imag),dtype='complex64') \n",
    "        # estimated mask\n",
    "\n",
    "\n",
    "        if opts.labwin==0:\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        else:\n",
    "            real_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_real), opts.labwin)\n",
    "            imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_imag), opts.labwin)\n",
    "            print(real_output_unwrap.shape)\n",
    "\n",
    "            clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_real), opts.labwin)\n",
    "            clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_imag), opts.labwin)\n",
    "            print(clean_real_unwrap.shape)\n",
    "\n",
    "            complex_irmmask    = K.cast(K.transpose(real_output_unwrap),dtype='complex64') + \\\n",
    "                                1j*K.cast(K.transpose(imag_output_unwrap),dtype='complex64')\n",
    "            estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "            true_irmmask       = K.cast(K.transpose(clean_real_unwrap),dtype='complex64') + \\\n",
    "                                1j*K.cast(K.transpose(clean_imag_unwrap),dtype='complex64')\n",
    "            y                  = true_irmmask*mix_stft\n",
    "\n",
    "            \n",
    "        diff = y-estimate\n",
    "        cost_real = K.sum(K.square(np.real(diff)))\n",
    "        cost_imag = K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "        return  0.5*(cost_real+cost_imag)/samples\n",
    "    \n",
    "    \n",
    "    return customLoss_train_SA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def lossFunction_SA(yTrue,yPred):\n",
    "\n",
    "    X_r, X_i = mix_spec_r, mix_spec_i\n",
    "\n",
    "    labwin = opts.labwin\n",
    "    r,c = K.shape(yTrue)[0], K.shape(yTrue)[1] # opts.sgd_batch_size\n",
    "\n",
    "    samples = K.cast(r,dtype='float32') \n",
    "    # because by default, K.shape tensors are dtype int32\n",
    "\n",
    "\n",
    "    pred_real = yPred[:, :(c//2)] \n",
    "    pred_imag = yPred[:, (c//2):]\n",
    "\n",
    "    true_real = yTrue[:, :(c//2)]\n",
    "    true_imag = yTrue[:, (c//2):]\n",
    "\n",
    "    mix_stft = K.cast(K.transpose(X_r),dtype='complex64') + \\\n",
    "            1j*K.cast(K.transpose(X_i),dtype='complex64')\n",
    "    print(\"<mix_stft>\",mix_stft)\n",
    "\n",
    "    complex_irmmask = K.cast(K.transpose(pred_real),dtype='complex64') + \\\n",
    "                    1j*K.cast(K.transpose(pred_imag),dtype='complex64') \n",
    "    # estimated mask\n",
    "    \n",
    "    print(\"<complex_irmmask>\",complex_irmmask)\n",
    "    print(\"<28.complex_irmmask*mix_stft>\",complex_irmmask*mix_stft)\n",
    "\n",
    "\n",
    "    if labwin==0:\n",
    "        estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "    else:\n",
    "        real_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_real), labwin, wrap_freqs)\n",
    "        imag_output_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(pred_imag), labwin, wrap_freqs)\n",
    "        print(\"<real_output_unwrap>\",real_output_unwrap.shape)\n",
    "        print(\"<imag_output_unwrap>\",imag_output_unwrap.shape)\n",
    "\n",
    "        clean_real_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_real), labwin, wrap_freqs)\n",
    "        clean_imag_unwrap,_ = unwrapAugmentedTF_wAvg(K.transpose(true_imag), labwin, wrap_freqs)\n",
    "        print(\"<clean_real_unwrap>\",clean_real_unwrap.shape)\n",
    "        print(\"<clean_imag_unwrap>\",clean_imag_unwrap.shape)\n",
    "\n",
    "        complex_irmmask    = K.cast(K.transpose(real_output_unwrap),dtype='complex64') + \\\n",
    "                            1j*K.cast(K.transpose(imag_output_unwrap),dtype='complex64')\n",
    "        print(\"<complex_irmmask>\",complex_irmmask)\n",
    "        estimate           = complex_irmmask*mix_stft\n",
    "\n",
    "        true_irmmask       = K.cast(K.transpose(clean_real_unwrap),dtype='complex64') + \\\n",
    "                            1j*K.cast(K.transpose(clean_imag_unwrap),dtype='complex64')\n",
    "        y                  = true_irmmask*mix_stft\n",
    "\n",
    "\n",
    "    diff = y-estimate\n",
    "    cost_real = K.sum(K.square(np.real(diff)))\n",
    "    cost_imag = K.sum(K.square(np.imag(diff)))\n",
    "\n",
    "    return  0.5*(cost_real+cost_imag)/samples\n",
    "\n",
    "\n",
    "# return customLoss_train_SA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opts.sgd_batch_size\n",
    "# batch_size = 5\n",
    "\n",
    "epochs = 80\n",
    "batch_size = 32\n",
    "\n",
    "n_input = 321  # data input\n",
    "n_hidden_1 = 321  # 1st layer number of neurons\n",
    "n_hidden_2 = 321  # 2nd layer number of neurons\n",
    "n_hidden_3 = 321  # 3rd layer number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_train**, **n_train_dim** = ```<number of samples for training, input dimentions>```, final will be (1951920, 1230)\n",
    "\n",
    "**n_dev**, **n_dev_dim** = ```<number of samples for development, input dimentions>```, final will be (449610, 1230)\n",
    "\n",
    "**n_test**, **n_test_dim** =```<number of samples for testing, input dimentions>```, final will be (72440, 1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_train_dim = (195192, 321) # opts.trData.shape = (1951920, 1230)\n",
    "n_dev, n_dev_dim = (44961, 321) # opts.cvData.shape =(449610, 1230)\n",
    "n_test, n_test_dim = (72440, 321) # opts.teData.shape =(72440, 1230)\n",
    "\n",
    "n_train_files = 15000 #15000\n",
    "n_dev_files = 3300 # 3300\n",
    "n_test_files = 545\n",
    "\n",
    "n_classes = (321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PRETRAIN_VERSION = \"_e14v1\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BEST_TRAINMODEL_FILE = \"./dnn_models/lstm_weights\"+ PRETRAIN_VERSION+\"_05.h5\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model = load_model(BEST_TRAINMODEL_FILE, custom_objects={'customLoss_train':customLoss_train,\n",
    "                                                          'customLoss_val':customLoss_val,\n",
    "                                                          'customLoss_test':customLoss_test,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 321)               103362    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 321)               103362    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 321)               103362    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 321)               103362    \n",
      "=================================================================\n",
      "Total params: 413,448\n",
      "Trainable params: 413,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "BatchNormalization()\n",
    "model.add( Dense(n_hidden_1, activation='relu', input_shape=(n_input,)) )\n",
    "BatchNormalization()\n",
    "model.add( Dense(n_hidden_2, activation='relu') )\n",
    "BatchNormalization()\n",
    "model.add( Dense(n_hidden_3, activation='relu') )\n",
    "BatchNormalization()\n",
    "model.add(Dense(n_classes, activation='relu'))\n",
    "\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "# callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "#              ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]\n",
    "          \n",
    "model.compile(loss = lossFunction_SPEC, optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'opts': opts,\n",
    "          'batch_size': batch_size, \n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "# Generators\n",
    "training_generator = DNN_DataGenerator_SPEC('Train', **params)\n",
    "validation_generator = DNN_DataGenerator_SPEC('Dev', **params)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-6, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "60997/60997 [==============================] - 369s 6ms/step - loss: 11.5451 - acc: 0.5896 - val_loss: 11.1764 - val_acc: 0.6115\n",
      "Epoch 1/80\n",
      "Epoch 2/80\n",
      "60997/60997 [==============================] - 333s 5ms/step - loss: 10.1950 - acc: 0.6044 - val_loss: 11.0825 - val_acc: 0.6206\n",
      "Epoch 3/80\n",
      "60997/60997 [==============================] - 359s 6ms/step - loss: 9.9682 - acc: 0.6079 - val_loss: 10.8754 - val_acc: 0.6288\n",
      "Epoch 4/80\n",
      "60997/60997 [==============================] - 338s 6ms/step - loss: 9.8513 - acc: 0.6101 - val_loss: 10.9853 - val_acc: 0.6214\n",
      "Epoch 5/80\n",
      "60997/60997 [==============================] - 335s 6ms/step - loss: 9.7740 - acc: 0.6112 - val_loss: 10.7263 - val_acc: 0.6245\n",
      "Epoch 6/80\n",
      "60997/60997 [==============================] - 336s 6ms/step - loss: 9.7315 - acc: 0.6124 - val_loss: 10.9792 - val_acc: 0.6302\n",
      "Epoch 7/80\n",
      "60997/60997 [==============================] - 330s 5ms/step - loss: 9.6976 - acc: 0.6130 - val_loss: 10.6784 - val_acc: 0.6248\n",
      "Epoch 8/80\n",
      "60997/60997 [==============================] - 341s 6ms/step - loss: 9.6710 - acc: 0.6137 - val_loss: 10.8349 - val_acc: 0.6172\n",
      "Epoch 9/80\n",
      "60997/60997 [==============================] - 335s 5ms/step - loss: 9.6370 - acc: 0.6137 - val_loss: 10.9143 - val_acc: 0.6343\n",
      "Epoch 10/80\n",
      "60997/60997 [==============================] - 338s 6ms/step - loss: 9.6058 - acc: 0.6144 - val_loss: 10.7963 - val_acc: 0.6143\n",
      "Epoch 11/80\n",
      "60997/60997 [==============================] - 335s 5ms/step - loss: 9.5867 - acc: 0.6149 - val_loss: 10.7259 - val_acc: 0.6230\n",
      "Epoch 12/80\n",
      "60997/60997 [==============================] - 331s 5ms/step - loss: 9.5687 - acc: 0.6158 - val_loss: 10.7915 - val_acc: 0.6303\n",
      "Epoch 13/80\n",
      "60997/60997 [==============================] - 340s 6ms/step - loss: 9.5606 - acc: 0.6158 - val_loss: 10.6866 - val_acc: 0.6278\n",
      "Epoch 14/80\n",
      "60997/60997 [==============================] - 335s 5ms/step - loss: 9.5426 - acc: 0.6171 - val_loss: 10.9305 - val_acc: 0.6111\n",
      "\bEpoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs = epochs,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6, \n",
    "                    callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "\n",
    "class FreqRecurrLayer(Dense):\n",
    "\n",
    "    def __init__(self, units,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \n",
    "        self.output_dim = units\n",
    "        super().__init__(units,\n",
    "                 activation,\n",
    "                 use_bias,\n",
    "                 kernel_initializer,\n",
    "                 bias_initializer,\n",
    "                 kernel_regularizer,\n",
    "                 bias_regularizer,\n",
    "                 activity_regularizer,\n",
    "                 kernel_constraint,\n",
    "                 bias_constraint,\n",
    "                 **kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.input_batch = input_shape[-2]\n",
    "        self.input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint,\n",
    "                                      trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint,\n",
    "                                        trainable=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "        self.recurr = self.add_weight(name='recurr', \n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        super().build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output_temp = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output_temp = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "            \n",
    "        for b in self.input_batch:\n",
    "            \n",
    "            \n",
    "            \n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "        res = K.dot(x, self.kernel)\n",
    "        prev = 0\n",
    "        temp_w = None\n",
    "        for i in range(self.input_shape1):\n",
    "            if i == 0:\n",
    "                temp_w = res[0]\n",
    "            else:\n",
    "                temp_w = K.concatenate([temp_w, K.dot(res[i],self.recurr[i-1]) ], axis=0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# history = model.fit_generator(opts.next_batch(opts.trData.shape[0], batch_size),  \n",
    "#                     validation_data=opts.next_batch(opts.cvData.shape[0], batch_size, isTrainCycle=False),\n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_input_sz/batch_size)),\n",
    "#                     validation_steps=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1, callbacks=callbacks)\n",
    "\n",
    "# history = model.fit_generator(opts.next_batch(opts.cvData.shape[0], batch_size),  \n",
    "#                     epochs=epochs, steps_per_epoch=int(math.ceil(n_out_sz/batch_size)), \n",
    "#                     verbose=1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    print('EPOCHS*****************')\n",
    "    for x,y,ids in dnn_batch_id_gen(opts, batch_size, 'Train'):\n",
    "        opts.batch_ids = ids\n",
    "#         training_loss = model.train_on_batch(x, y)\n",
    "        history = model.fit(x, y, batch_size, callbacks=None, shuffle=False)\n",
    "        break\n",
    "        \n",
    "    for x,y,ids in dnn_batch_id_gen(opts, batch_size, 'Dev'):\n",
    "        opts.batch_ids = ids\n",
    "        scores = model.test_on_batch(x, y)\n",
    "        print(scores)\n",
    "        break\n",
    "\n",
    "\n",
    "model.save(SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Mag weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PRETRAIN_VERSION = \"_e14v1\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BEST_TRAINMODEL_FILE = \"./dnn_models/lstm_weights\"+ PRETRAIN_VERSION+\"_05.h5\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model = load_model(BEST_TRAINMODEL_FILE, custom_objects={'customLoss_train':customLoss_train,\n",
    "                                                          'customLoss_val':customLoss_val,\n",
    "                                                          'customLoss_test':customLoss_test,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify and retrain"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.pop()\n",
    "model.add(TimeDistributed(Dense(units=n_classes, activation='linear')))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, min_delta=1e-4, verbose=1, mode='auto'), \n",
    "             ModelCheckpoint(filepath= MODEL_FILE, monitor='val_acc', save_best_only=True),\n",
    "             LossFunctionCallback_SA(model,opts)]\n",
    "\n",
    "adam = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = adam, metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit_generator(next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='train'), \n",
    "                    validation_data=next_batch_rnn(opts,batch_size,maxlen=Max_Frame,CYCLE='dev'),\n",
    "                    epochs=epochs, steps_per_epoch=n_train_files//batch_size, \n",
    "                    validation_steps=n_dev_files//batch_size, \n",
    "                    verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.save(SAVE_MODEL_FILE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 1\n",
    "single bidirectional GRU layer\n",
    "\n",
    "real+img (963+963)=1926-d output vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FILE = \"./dnn_models/dnn_weights\"+ Code_VERSION+\"_09.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(BEST_MODEL_FILE, custom_objects={'lossFunction_SPEC':lossFunction_SPEC})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2264/2264 [==============================] - 3s 1ms/step\n",
      "y_hat shape (72440, 321)\n"
     ]
    }
   ],
   "source": [
    "params = {'opts': opts,\n",
    "          'batch_size': batch_size, \n",
    "          'shuffle': False}\n",
    "\n",
    "testing_generator = DNN_DataGenerator_SPEC('Test', **params)\n",
    "y_hat = model.predict_generator(testing_generator, verbose=1)\n",
    "print('y_hat shape', y_hat.shape)\n",
    "sio.savemat(OUTPUT_FILE, {'y_hat':y_hat})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
